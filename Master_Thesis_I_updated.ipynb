{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjin2020/essay/blob/main/Master_Thesis_I_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqi09g5FpBNR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ],
      "id": "aqi09g5FpBNR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1YIe7VzsUrS",
        "outputId": "63425acc-b1d1-491b-e8da-6b9476830a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  /content/drive/My Drive/Colab Notebooks/master essay 2.zip\n",
            "   creating: train_local/master essay/\n",
            "  inflating: train_local/master essay/Day-ahead_prices_201801010000_201912312359.xlsx  \n",
            "  inflating: train_local/__MACOSX/master essay/._Day-ahead_prices_201801010000_201912312359.xlsx  \n",
            "  inflating: train_local/master essay/Actual_generation_201801010000_201912312359.xlsx  \n",
            "  inflating: train_local/__MACOSX/master essay/._Actual_generation_201801010000_201912312359.xlsx  \n"
          ]
        }
      ],
      "source": [
        "!mkdir train_local\n",
        "drive.mount('/content/drive')\n",
        "!unzip \"/content/drive/My Drive/Colab Notebooks/master essay 2.zip\" -d train_local"
      ],
      "id": "-1YIe7VzsUrS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c02c1d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc7788c1-5bd8-4a01-ae6e-4388ccd9ac0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm"
      ],
      "id": "8c02c1d8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17cfc5c5"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel('train_local/master essay/Actual_generation_201801010000_201912312359.xlsx',header=None)\n",
        "y=pd.read_excel('train_local/master essay/Day-ahead_prices_201801010000_201912312359.xlsx',header=None)"
      ],
      "id": "17cfc5c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "013cf258"
      },
      "source": [
        "## Data Preprocessing"
      ],
      "id": "013cf258"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd10e8d3"
      },
      "outputs": [],
      "source": [
        "df.columns,y.columns=df.iloc[0],y.iloc[1]\n",
        "df,y=df.drop(0),y.drop([0,1])"
      ],
      "id": "fd10e8d3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bea127e4"
      },
      "outputs": [],
      "source": [
        "df=df.fillna(0)"
      ],
      "id": "bea127e4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc420266"
      },
      "outputs": [],
      "source": [
        "def to_datetime(d):\n",
        "    return pd.to_datetime(d,format='%b %d, %Y %I:%M %p')\n",
        "df['dayTime']=df['Date']+' '+df['Time of day']\n",
        "df['dayTime']=df['dayTime'].apply(to_datetime)\n",
        "df=df.iloc[:,2:].groupby([pd.Grouper(key='dayTime',freq='1H')]).sum()"
      ],
      "id": "dc420266"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "700cdf88"
      },
      "outputs": [],
      "source": [
        "y['dayTime']=y['Date']+' '+y['Time of day']\n",
        "y['dayTime']=y['dayTime'].apply(to_datetime)\n",
        "y=y.fillna(0)"
      ],
      "id": "700cdf88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5db48a9b"
      },
      "outputs": [],
      "source": [
        "a=y.iloc[:,[2,15,-1]].groupby([pd.Grouper(key='dayTime',freq='1H')]).sum()\n",
        "a=a.replace(list(set([i for i in a.iloc[:,1].values if not isinstance(i, float) and not isinstance(i, int)])),0)\n",
        "a['Price,Germany']=a.iloc[:,0]+a.iloc[:,1]"
      ],
      "id": "5db48a9b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "69cadc81",
        "outputId": "5af13758-cb3c-47c0-f5d4-cbd841ac62c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     Biomass[MWh]  Hydropower[MWh]  Wind offshore[MWh]  \\\n",
              "dayTime                                                                  \n",
              "2018-01-01 00:00:00       4779.25          1783.75                2917   \n",
              "2018-01-01 01:00:00       4764.50          1676.25                3000   \n",
              "2018-01-01 02:00:00       4766.75          1672.00                3096   \n",
              "2018-01-01 03:00:00       4771.00          1658.25                3137   \n",
              "2018-01-01 04:00:00       4776.50          1654.00                3212   \n",
              "\n",
              "                     Wind onshore[MWh]  Photovoltaics[MWh]  \\\n",
              "dayTime                                                      \n",
              "2018-01-01 00:00:00              28661                   0   \n",
              "2018-01-01 01:00:00              30106                   0   \n",
              "2018-01-01 02:00:00              30773                   0   \n",
              "2018-01-01 03:00:00              31642                   0   \n",
              "2018-01-01 04:00:00              31529                   0   \n",
              "\n",
              "                     Other renewable[MWh]  Nuclear[MWh]  \\\n",
              "dayTime                                                   \n",
              "2018-01-01 00:00:00                   168       5853.75   \n",
              "2018-01-01 01:00:00                   168       4754.00   \n",
              "2018-01-01 02:00:00                   168       4591.00   \n",
              "2018-01-01 03:00:00                   168       4888.75   \n",
              "2018-01-01 04:00:00                   168       4696.50   \n",
              "\n",
              "                     Fossil brown coal[MWh]  Fossil hard coal[MWh]  \\\n",
              "dayTime                                                              \n",
              "2018-01-01 00:00:00                  6724.0                   1712   \n",
              "2018-01-01 01:00:00                  6810.0                   1609   \n",
              "2018-01-01 02:00:00                  6766.0                   1684   \n",
              "2018-01-01 03:00:00                  6728.0                   1681   \n",
              "2018-01-01 04:00:00                  6710.0                   1677   \n",
              "\n",
              "                     Fossil gas[MWh]  Hydro pumped storage[MWh]  \\\n",
              "dayTime                                                           \n",
              "2018-01-01 00:00:00             2407                        196   \n",
              "2018-01-01 01:00:00             2407                        632   \n",
              "2018-01-01 02:00:00             2398                        527   \n",
              "2018-01-01 03:00:00             2416                         55   \n",
              "2018-01-01 04:00:00             2411                         49   \n",
              "\n",
              "                     Other conventional[MWh]  Price,Germany  \n",
              "dayTime                                                      \n",
              "2018-01-01 00:00:00                     1119          -5.27  \n",
              "2018-01-01 01:00:00                     1102         -29.99  \n",
              "2018-01-01 02:00:00                     1057         -56.65  \n",
              "2018-01-01 03:00:00                     1049         -63.14  \n",
              "2018-01-01 04:00:00                     1079         -64.62  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3ae3a6f-e449-4635-ae3a-50dd43525b4a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Biomass[MWh]</th>\n",
              "      <th>Hydropower[MWh]</th>\n",
              "      <th>Wind offshore[MWh]</th>\n",
              "      <th>Wind onshore[MWh]</th>\n",
              "      <th>Photovoltaics[MWh]</th>\n",
              "      <th>Other renewable[MWh]</th>\n",
              "      <th>Nuclear[MWh]</th>\n",
              "      <th>Fossil brown coal[MWh]</th>\n",
              "      <th>Fossil hard coal[MWh]</th>\n",
              "      <th>Fossil gas[MWh]</th>\n",
              "      <th>Hydro pumped storage[MWh]</th>\n",
              "      <th>Other conventional[MWh]</th>\n",
              "      <th>Price,Germany</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dayTime</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2018-01-01 00:00:00</th>\n",
              "      <td>4779.25</td>\n",
              "      <td>1783.75</td>\n",
              "      <td>2917</td>\n",
              "      <td>28661</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>5853.75</td>\n",
              "      <td>6724.0</td>\n",
              "      <td>1712</td>\n",
              "      <td>2407</td>\n",
              "      <td>196</td>\n",
              "      <td>1119</td>\n",
              "      <td>-5.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 01:00:00</th>\n",
              "      <td>4764.50</td>\n",
              "      <td>1676.25</td>\n",
              "      <td>3000</td>\n",
              "      <td>30106</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>4754.00</td>\n",
              "      <td>6810.0</td>\n",
              "      <td>1609</td>\n",
              "      <td>2407</td>\n",
              "      <td>632</td>\n",
              "      <td>1102</td>\n",
              "      <td>-29.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 02:00:00</th>\n",
              "      <td>4766.75</td>\n",
              "      <td>1672.00</td>\n",
              "      <td>3096</td>\n",
              "      <td>30773</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>4591.00</td>\n",
              "      <td>6766.0</td>\n",
              "      <td>1684</td>\n",
              "      <td>2398</td>\n",
              "      <td>527</td>\n",
              "      <td>1057</td>\n",
              "      <td>-56.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 03:00:00</th>\n",
              "      <td>4771.00</td>\n",
              "      <td>1658.25</td>\n",
              "      <td>3137</td>\n",
              "      <td>31642</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>4888.75</td>\n",
              "      <td>6728.0</td>\n",
              "      <td>1681</td>\n",
              "      <td>2416</td>\n",
              "      <td>55</td>\n",
              "      <td>1049</td>\n",
              "      <td>-63.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 04:00:00</th>\n",
              "      <td>4776.50</td>\n",
              "      <td>1654.00</td>\n",
              "      <td>3212</td>\n",
              "      <td>31529</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>4696.50</td>\n",
              "      <td>6710.0</td>\n",
              "      <td>1677</td>\n",
              "      <td>2411</td>\n",
              "      <td>49</td>\n",
              "      <td>1079</td>\n",
              "      <td>-64.62</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3ae3a6f-e449-4635-ae3a-50dd43525b4a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e3ae3a6f-e449-4635-ae3a-50dd43525b4a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e3ae3a6f-e449-4635-ae3a-50dd43525b4a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df=pd.merge(df,a.iloc[:,2],on='dayTime',how='left')\n",
        "df.head()"
      ],
      "id": "69cadc81"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "df430554",
        "outputId": "81c56bb1-954e-42a9-e27b-b2289412941f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     Biomass[MWh]  Hydropower[MWh]  Wind offshore[MWh]  \\\n",
              "dayTime                                                                  \n",
              "2018-01-01 00:00:00      0.094784         0.035376            0.057851   \n",
              "2018-01-01 01:00:00      0.093125         0.032763            0.058637   \n",
              "2018-01-01 02:00:00      0.092245         0.032356            0.059913   \n",
              "2018-01-01 03:00:00      0.091095         0.031662            0.059896   \n",
              "2018-01-01 04:00:00      0.091668         0.031743            0.061643   \n",
              "\n",
              "                     Wind onshore[MWh]  Photovoltaics[MWh]  \\\n",
              "dayTime                                                      \n",
              "2018-01-01 00:00:00           0.568417                 0.0   \n",
              "2018-01-01 01:00:00           0.588442                 0.0   \n",
              "2018-01-01 02:00:00           0.595510                 0.0   \n",
              "2018-01-01 03:00:00           0.604155                 0.0   \n",
              "2018-01-01 04:00:00           0.605088                 0.0   \n",
              "\n",
              "                     Other renewable[MWh]  Nuclear[MWh]  \\\n",
              "dayTime                                                   \n",
              "2018-01-01 00:00:00              0.003332      0.116094   \n",
              "2018-01-01 01:00:00              0.003284      0.092920   \n",
              "2018-01-01 02:00:00              0.003251      0.088844   \n",
              "2018-01-01 03:00:00              0.003208      0.093343   \n",
              "2018-01-01 04:00:00              0.003224      0.090133   \n",
              "\n",
              "                     Fossil brown coal[MWh]  Fossil hard coal[MWh]  \\\n",
              "dayTime                                                              \n",
              "2018-01-01 00:00:00                0.133353               0.033953   \n",
              "2018-01-01 01:00:00                0.133106               0.031449   \n",
              "2018-01-01 02:00:00                0.130934               0.032588   \n",
              "2018-01-01 03:00:00                0.128461               0.032096   \n",
              "2018-01-01 04:00:00                0.128775               0.032184   \n",
              "\n",
              "                     Fossil gas[MWh]  Hydro pumped storage[MWh]  \\\n",
              "dayTime                                                           \n",
              "2018-01-01 00:00:00         0.047737                   0.003887   \n",
              "2018-01-01 01:00:00         0.047046                   0.012353   \n",
              "2018-01-01 02:00:00         0.046405                   0.010198   \n",
              "2018-01-01 03:00:00         0.046130                   0.001050   \n",
              "2018-01-01 04:00:00         0.046271                   0.000940   \n",
              "\n",
              "                     Other conventional[MWh]  Price,Germany  Total Production  \n",
              "dayTime                                                                        \n",
              "2018-01-01 00:00:00                 0.022192      -0.000105               1.0  \n",
              "2018-01-01 01:00:00                 0.021539      -0.000586               1.0  \n",
              "2018-01-01 02:00:00                 0.020455      -0.001096               1.0  \n",
              "2018-01-01 03:00:00                 0.020029      -0.001206               1.0  \n",
              "2018-01-01 04:00:00                 0.020708      -0.001240               1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7aee57c6-4ff5-4b90-a099-6ab692c45943\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Biomass[MWh]</th>\n",
              "      <th>Hydropower[MWh]</th>\n",
              "      <th>Wind offshore[MWh]</th>\n",
              "      <th>Wind onshore[MWh]</th>\n",
              "      <th>Photovoltaics[MWh]</th>\n",
              "      <th>Other renewable[MWh]</th>\n",
              "      <th>Nuclear[MWh]</th>\n",
              "      <th>Fossil brown coal[MWh]</th>\n",
              "      <th>Fossil hard coal[MWh]</th>\n",
              "      <th>Fossil gas[MWh]</th>\n",
              "      <th>Hydro pumped storage[MWh]</th>\n",
              "      <th>Other conventional[MWh]</th>\n",
              "      <th>Price,Germany</th>\n",
              "      <th>Total Production</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dayTime</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2018-01-01 00:00:00</th>\n",
              "      <td>0.094784</td>\n",
              "      <td>0.035376</td>\n",
              "      <td>0.057851</td>\n",
              "      <td>0.568417</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003332</td>\n",
              "      <td>0.116094</td>\n",
              "      <td>0.133353</td>\n",
              "      <td>0.033953</td>\n",
              "      <td>0.047737</td>\n",
              "      <td>0.003887</td>\n",
              "      <td>0.022192</td>\n",
              "      <td>-0.000105</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 01:00:00</th>\n",
              "      <td>0.093125</td>\n",
              "      <td>0.032763</td>\n",
              "      <td>0.058637</td>\n",
              "      <td>0.588442</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.092920</td>\n",
              "      <td>0.133106</td>\n",
              "      <td>0.031449</td>\n",
              "      <td>0.047046</td>\n",
              "      <td>0.012353</td>\n",
              "      <td>0.021539</td>\n",
              "      <td>-0.000586</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 02:00:00</th>\n",
              "      <td>0.092245</td>\n",
              "      <td>0.032356</td>\n",
              "      <td>0.059913</td>\n",
              "      <td>0.595510</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003251</td>\n",
              "      <td>0.088844</td>\n",
              "      <td>0.130934</td>\n",
              "      <td>0.032588</td>\n",
              "      <td>0.046405</td>\n",
              "      <td>0.010198</td>\n",
              "      <td>0.020455</td>\n",
              "      <td>-0.001096</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 03:00:00</th>\n",
              "      <td>0.091095</td>\n",
              "      <td>0.031662</td>\n",
              "      <td>0.059896</td>\n",
              "      <td>0.604155</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003208</td>\n",
              "      <td>0.093343</td>\n",
              "      <td>0.128461</td>\n",
              "      <td>0.032096</td>\n",
              "      <td>0.046130</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.020029</td>\n",
              "      <td>-0.001206</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 04:00:00</th>\n",
              "      <td>0.091668</td>\n",
              "      <td>0.031743</td>\n",
              "      <td>0.061643</td>\n",
              "      <td>0.605088</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003224</td>\n",
              "      <td>0.090133</td>\n",
              "      <td>0.128775</td>\n",
              "      <td>0.032184</td>\n",
              "      <td>0.046271</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>0.020708</td>\n",
              "      <td>-0.001240</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7aee57c6-4ff5-4b90-a099-6ab692c45943')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7aee57c6-4ff5-4b90-a099-6ab692c45943 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7aee57c6-4ff5-4b90-a099-6ab692c45943');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "#df['Total Production']=df.iloc[:,1:-2].sum(axis=1)\n",
        "#data_pro=df.div(df['Total Production'].values,axis=0)\n",
        "#data_pro.head()"
      ],
      "id": "df430554"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "343d16c0"
      },
      "outputs": [],
      "source": [
        "df['year']=df.index.year.astype(object)\n",
        "df['month']=df.index.month.astype(object)\n",
        "df['day']=df.index.day.astype(object)\n",
        "df['hour']=df.index.hour.astype(object)"
      ],
      "id": "343d16c0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6e9703d9",
        "outputId": "1ff612e9-a1ea-466d-a53c-9860bf2eeed0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     Biomass[MWh]  Hydropower[MWh]  Wind offshore[MWh]  \\\n",
              "dayTime                                                                  \n",
              "2018-01-01 00:00:00       4779.25          1783.75                2917   \n",
              "2018-01-01 01:00:00       4764.50          1676.25                3000   \n",
              "2018-01-01 02:00:00       4766.75          1672.00                3096   \n",
              "2018-01-01 03:00:00       4771.00          1658.25                3137   \n",
              "2018-01-01 04:00:00       4776.50          1654.00                3212   \n",
              "2018-01-01 05:00:00       4768.75          1648.25                3282   \n",
              "2018-01-01 06:00:00       4781.00          1638.00                3291   \n",
              "2018-01-01 07:00:00       4793.75          1623.00                3282   \n",
              "2018-01-01 08:00:00       4810.00          1613.25                3285   \n",
              "2018-01-01 09:00:00       4848.75          1610.50                3278   \n",
              "2018-01-01 10:00:00       4866.50          1623.25                3196   \n",
              "2018-01-01 11:00:00       4863.50          1630.00                3253   \n",
              "2018-01-01 12:00:00       4850.25          1729.75                3289   \n",
              "2018-01-01 13:00:00       4850.50          1742.75                3286   \n",
              "2018-01-01 14:00:00       4860.75          1738.25                3281   \n",
              "2018-01-01 15:00:00       4866.75          1733.00                4232   \n",
              "2018-01-01 16:00:00       4867.25          1737.25                4007   \n",
              "2018-01-01 17:00:00       4876.75          1710.50                3688   \n",
              "2018-01-01 18:00:00       4881.25          1702.50                3256   \n",
              "2018-01-01 19:00:00       4878.25          1702.25                3105   \n",
              "2018-01-01 20:00:00       4855.50          1704.00                2859   \n",
              "2018-01-01 21:00:00       4846.75          1703.25                2556   \n",
              "2018-01-01 22:00:00       4826.50          1701.75                2346   \n",
              "2018-01-01 23:00:00       4802.00          1698.00                1946   \n",
              "\n",
              "                     Wind onshore[MWh]  Photovoltaics[MWh]  \\\n",
              "dayTime                                                      \n",
              "2018-01-01 00:00:00              28661                   0   \n",
              "2018-01-01 01:00:00              30106                   0   \n",
              "2018-01-01 02:00:00              30773                   0   \n",
              "2018-01-01 03:00:00              31642                   0   \n",
              "2018-01-01 04:00:00              31529                   0   \n",
              "2018-01-01 05:00:00              32063                   0   \n",
              "2018-01-01 06:00:00              31448                   0   \n",
              "2018-01-01 07:00:00              30919                   0   \n",
              "2018-01-01 08:00:00              30312                 174   \n",
              "2018-01-01 09:00:00              30812                1565   \n",
              "2018-01-01 10:00:00              31739                4038   \n",
              "2018-01-01 11:00:00              32300                6001   \n",
              "2018-01-01 12:00:00              31597                6409   \n",
              "2018-01-01 13:00:00              29291                5685   \n",
              "2018-01-01 14:00:00              25579                4029   \n",
              "2018-01-01 15:00:00              22183                1548   \n",
              "2018-01-01 16:00:00              19523                  41   \n",
              "2018-01-01 17:00:00              18211                   0   \n",
              "2018-01-01 18:00:00              17723                   0   \n",
              "2018-01-01 19:00:00              17339                   0   \n",
              "2018-01-01 20:00:00              16499                   0   \n",
              "2018-01-01 21:00:00              16489                   0   \n",
              "2018-01-01 22:00:00              16059                   0   \n",
              "2018-01-01 23:00:00              15814                   0   \n",
              "\n",
              "                     Other renewable[MWh]  Nuclear[MWh]  \\\n",
              "dayTime                                                   \n",
              "2018-01-01 00:00:00                   168       5853.75   \n",
              "2018-01-01 01:00:00                   168       4754.00   \n",
              "2018-01-01 02:00:00                   168       4591.00   \n",
              "2018-01-01 03:00:00                   168       4888.75   \n",
              "2018-01-01 04:00:00                   168       4696.50   \n",
              "2018-01-01 05:00:00                   168       4830.75   \n",
              "2018-01-01 06:00:00                   168       4762.50   \n",
              "2018-01-01 07:00:00                   168       4639.00   \n",
              "2018-01-01 08:00:00                   168       4616.25   \n",
              "2018-01-01 09:00:00                   168       4657.25   \n",
              "2018-01-01 10:00:00                   168       4857.00   \n",
              "2018-01-01 11:00:00                   168       4826.25   \n",
              "2018-01-01 12:00:00                   168       5138.50   \n",
              "2018-01-01 13:00:00                   168       6297.75   \n",
              "2018-01-01 14:00:00                   168       7285.50   \n",
              "2018-01-01 15:00:00                   168       7966.00   \n",
              "2018-01-01 16:00:00                   174       8594.50   \n",
              "2018-01-01 17:00:00                   182       8905.25   \n",
              "2018-01-01 18:00:00                   188       9019.25   \n",
              "2018-01-01 19:00:00                   213       9089.00   \n",
              "2018-01-01 20:00:00                   216       9104.50   \n",
              "2018-01-01 21:00:00                   215       9112.00   \n",
              "2018-01-01 22:00:00                   208       9110.50   \n",
              "2018-01-01 23:00:00                   209       9128.50   \n",
              "\n",
              "                     Fossil brown coal[MWh]  Fossil hard coal[MWh]  \\\n",
              "dayTime                                                              \n",
              "2018-01-01 00:00:00                  6724.0                   1712   \n",
              "2018-01-01 01:00:00                  6810.0                   1609   \n",
              "2018-01-01 02:00:00                  6766.0                   1684   \n",
              "2018-01-01 03:00:00                  6728.0                   1681   \n",
              "2018-01-01 04:00:00                  6710.0                   1677   \n",
              "2018-01-01 05:00:00                  6728.0                   1682   \n",
              "2018-01-01 06:00:00                  6666.0                   1690   \n",
              "2018-01-01 07:00:00                  6580.0                   1679   \n",
              "2018-01-01 08:00:00                  6507.0                   1509   \n",
              "2018-01-01 09:00:00                  6538.0                   1516   \n",
              "2018-01-01 10:00:00                  6570.0                   1544   \n",
              "2018-01-01 11:00:00                  6596.0                   1565   \n",
              "2018-01-01 12:00:00                  6359.0                   1595   \n",
              "2018-01-01 13:00:00                  6348.0                   1633   \n",
              "2018-01-01 14:00:00                  6366.0                   1632   \n",
              "2018-01-01 15:00:00                  6897.0                   1806   \n",
              "2018-01-01 16:00:00                  9121.0                   1786   \n",
              "2018-01-01 17:00:00                 10710.0                   1851   \n",
              "2018-01-01 18:00:00                 11266.0                   1914   \n",
              "2018-01-01 19:00:00                 11755.0                   1753   \n",
              "2018-01-01 20:00:00                 12142.0                   1985   \n",
              "2018-01-01 21:00:00                 12551.0                   2040   \n",
              "2018-01-01 22:00:00                 13019.0                   2009   \n",
              "2018-01-01 23:00:00                 12914.0                   1939   \n",
              "\n",
              "                     Fossil gas[MWh]  Hydro pumped storage[MWh]  \\\n",
              "dayTime                                                           \n",
              "2018-01-01 00:00:00             2407                        196   \n",
              "2018-01-01 01:00:00             2407                        632   \n",
              "2018-01-01 02:00:00             2398                        527   \n",
              "2018-01-01 03:00:00             2416                         55   \n",
              "2018-01-01 04:00:00             2411                         49   \n",
              "2018-01-01 05:00:00             2393                         52   \n",
              "2018-01-01 06:00:00             2305                        459   \n",
              "2018-01-01 07:00:00             2268                        457   \n",
              "2018-01-01 08:00:00             2251                        458   \n",
              "2018-01-01 09:00:00             2417                        463   \n",
              "2018-01-01 10:00:00             2494                        525   \n",
              "2018-01-01 11:00:00             2506                        464   \n",
              "2018-01-01 12:00:00             2521                        903   \n",
              "2018-01-01 13:00:00             2537                        315   \n",
              "2018-01-01 14:00:00             2547                        421   \n",
              "2018-01-01 15:00:00             2559                        365   \n",
              "2018-01-01 16:00:00             2831                       1455   \n",
              "2018-01-01 17:00:00             2944                       3014   \n",
              "2018-01-01 18:00:00             3005                       5423   \n",
              "2018-01-01 19:00:00             3205                       4836   \n",
              "2018-01-01 20:00:00             3293                       4134   \n",
              "2018-01-01 21:00:00             3311                       3005   \n",
              "2018-01-01 22:00:00             3289                       1217   \n",
              "2018-01-01 23:00:00             3291                        647   \n",
              "\n",
              "                     Other conventional[MWh]  Price,Germany  year month day  \\\n",
              "dayTime                                                                       \n",
              "2018-01-01 00:00:00                     1119          -5.27  2018     1   1   \n",
              "2018-01-01 01:00:00                     1102         -29.99  2018     1   1   \n",
              "2018-01-01 02:00:00                     1057         -56.65  2018     1   1   \n",
              "2018-01-01 03:00:00                     1049         -63.14  2018     1   1   \n",
              "2018-01-01 04:00:00                     1079         -64.62  2018     1   1   \n",
              "2018-01-01 05:00:00                     1100         -67.00  2018     1   1   \n",
              "2018-01-01 06:00:00                     1091         -72.54  2018     1   1   \n",
              "2018-01-01 07:00:00                     1092         -76.01  2018     1   1   \n",
              "2018-01-01 08:00:00                     1088         -71.45  2018     1   1   \n",
              "2018-01-01 09:00:00                     1084         -66.88  2018     1   1   \n",
              "2018-01-01 10:00:00                     1083         -62.00  2018     1   1   \n",
              "2018-01-01 11:00:00                     1092         -56.08  2018     1   1   \n",
              "2018-01-01 12:00:00                     1086         -49.96  2018     1   1   \n",
              "2018-01-01 13:00:00                     1092         -24.29  2018     1   1   \n",
              "2018-01-01 14:00:00                     1091          -5.59  2018     1   1   \n",
              "2018-01-01 15:00:00                     1088           0.23  2018     1   1   \n",
              "2018-01-01 16:00:00                     1121          11.02  2018     1   1   \n",
              "2018-01-01 17:00:00                     1133          23.50  2018     1   1   \n",
              "2018-01-01 18:00:00                     1153          22.91  2018     1   1   \n",
              "2018-01-01 19:00:00                     1193          21.02  2018     1   1   \n",
              "2018-01-01 20:00:00                     1193          21.05  2018     1   1   \n",
              "2018-01-01 21:00:00                     1200          21.95  2018     1   1   \n",
              "2018-01-01 22:00:00                     1203          23.52  2018     1   1   \n",
              "2018-01-01 23:00:00                     1201          18.96  2018     1   1   \n",
              "\n",
              "                    hour  \n",
              "dayTime                   \n",
              "2018-01-01 00:00:00    0  \n",
              "2018-01-01 01:00:00    1  \n",
              "2018-01-01 02:00:00    2  \n",
              "2018-01-01 03:00:00    3  \n",
              "2018-01-01 04:00:00    4  \n",
              "2018-01-01 05:00:00    5  \n",
              "2018-01-01 06:00:00    6  \n",
              "2018-01-01 07:00:00    7  \n",
              "2018-01-01 08:00:00    8  \n",
              "2018-01-01 09:00:00    9  \n",
              "2018-01-01 10:00:00   10  \n",
              "2018-01-01 11:00:00   11  \n",
              "2018-01-01 12:00:00   12  \n",
              "2018-01-01 13:00:00   13  \n",
              "2018-01-01 14:00:00   14  \n",
              "2018-01-01 15:00:00   15  \n",
              "2018-01-01 16:00:00   16  \n",
              "2018-01-01 17:00:00   17  \n",
              "2018-01-01 18:00:00   18  \n",
              "2018-01-01 19:00:00   19  \n",
              "2018-01-01 20:00:00   20  \n",
              "2018-01-01 21:00:00   21  \n",
              "2018-01-01 22:00:00   22  \n",
              "2018-01-01 23:00:00   23  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ab1fee7-e715-495d-96a2-254ff4c3f75d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Biomass[MWh]</th>\n",
              "      <th>Hydropower[MWh]</th>\n",
              "      <th>Wind offshore[MWh]</th>\n",
              "      <th>Wind onshore[MWh]</th>\n",
              "      <th>Photovoltaics[MWh]</th>\n",
              "      <th>Other renewable[MWh]</th>\n",
              "      <th>Nuclear[MWh]</th>\n",
              "      <th>Fossil brown coal[MWh]</th>\n",
              "      <th>Fossil hard coal[MWh]</th>\n",
              "      <th>Fossil gas[MWh]</th>\n",
              "      <th>Hydro pumped storage[MWh]</th>\n",
              "      <th>Other conventional[MWh]</th>\n",
              "      <th>Price,Germany</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dayTime</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2018-01-01 00:00:00</th>\n",
              "      <td>4779.25</td>\n",
              "      <td>1783.75</td>\n",
              "      <td>2917</td>\n",
              "      <td>28661</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>5853.75</td>\n",
              "      <td>6724.0</td>\n",
              "      <td>1712</td>\n",
              "      <td>2407</td>\n",
              "      <td>196</td>\n",
              "      <td>1119</td>\n",
              "      <td>-5.27</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 01:00:00</th>\n",
              "      <td>4764.50</td>\n",
              "      <td>1676.25</td>\n",
              "      <td>3000</td>\n",
              "      <td>30106</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>4754.00</td>\n",
              "      <td>6810.0</td>\n",
              "      <td>1609</td>\n",
              "      <td>2407</td>\n",
              "      <td>632</td>\n",
              "      <td>1102</td>\n",
              "      <td>-29.99</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 02:00:00</th>\n",
              "      <td>4766.75</td>\n",
              "      <td>1672.00</td>\n",
              "      <td>3096</td>\n",
              "      <td>30773</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>4591.00</td>\n",
              "      <td>6766.0</td>\n",
              "      <td>1684</td>\n",
              "      <td>2398</td>\n",
              "      <td>527</td>\n",
              "      <td>1057</td>\n",
              "      <td>-56.65</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 03:00:00</th>\n",
              "      <td>4771.00</td>\n",
              "      <td>1658.25</td>\n",
              "      <td>3137</td>\n",
              "      <td>31642</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>4888.75</td>\n",
              "      <td>6728.0</td>\n",
              "      <td>1681</td>\n",
              "      <td>2416</td>\n",
              "      <td>55</td>\n",
              "      <td>1049</td>\n",
              "      <td>-63.14</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 04:00:00</th>\n",
              "      <td>4776.50</td>\n",
              "      <td>1654.00</td>\n",
              "      <td>3212</td>\n",
              "      <td>31529</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>4696.50</td>\n",
              "      <td>6710.0</td>\n",
              "      <td>1677</td>\n",
              "      <td>2411</td>\n",
              "      <td>49</td>\n",
              "      <td>1079</td>\n",
              "      <td>-64.62</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 05:00:00</th>\n",
              "      <td>4768.75</td>\n",
              "      <td>1648.25</td>\n",
              "      <td>3282</td>\n",
              "      <td>32063</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>4830.75</td>\n",
              "      <td>6728.0</td>\n",
              "      <td>1682</td>\n",
              "      <td>2393</td>\n",
              "      <td>52</td>\n",
              "      <td>1100</td>\n",
              "      <td>-67.00</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 06:00:00</th>\n",
              "      <td>4781.00</td>\n",
              "      <td>1638.00</td>\n",
              "      <td>3291</td>\n",
              "      <td>31448</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>4762.50</td>\n",
              "      <td>6666.0</td>\n",
              "      <td>1690</td>\n",
              "      <td>2305</td>\n",
              "      <td>459</td>\n",
              "      <td>1091</td>\n",
              "      <td>-72.54</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 07:00:00</th>\n",
              "      <td>4793.75</td>\n",
              "      <td>1623.00</td>\n",
              "      <td>3282</td>\n",
              "      <td>30919</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>4639.00</td>\n",
              "      <td>6580.0</td>\n",
              "      <td>1679</td>\n",
              "      <td>2268</td>\n",
              "      <td>457</td>\n",
              "      <td>1092</td>\n",
              "      <td>-76.01</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 08:00:00</th>\n",
              "      <td>4810.00</td>\n",
              "      <td>1613.25</td>\n",
              "      <td>3285</td>\n",
              "      <td>30312</td>\n",
              "      <td>174</td>\n",
              "      <td>168</td>\n",
              "      <td>4616.25</td>\n",
              "      <td>6507.0</td>\n",
              "      <td>1509</td>\n",
              "      <td>2251</td>\n",
              "      <td>458</td>\n",
              "      <td>1088</td>\n",
              "      <td>-71.45</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 09:00:00</th>\n",
              "      <td>4848.75</td>\n",
              "      <td>1610.50</td>\n",
              "      <td>3278</td>\n",
              "      <td>30812</td>\n",
              "      <td>1565</td>\n",
              "      <td>168</td>\n",
              "      <td>4657.25</td>\n",
              "      <td>6538.0</td>\n",
              "      <td>1516</td>\n",
              "      <td>2417</td>\n",
              "      <td>463</td>\n",
              "      <td>1084</td>\n",
              "      <td>-66.88</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 10:00:00</th>\n",
              "      <td>4866.50</td>\n",
              "      <td>1623.25</td>\n",
              "      <td>3196</td>\n",
              "      <td>31739</td>\n",
              "      <td>4038</td>\n",
              "      <td>168</td>\n",
              "      <td>4857.00</td>\n",
              "      <td>6570.0</td>\n",
              "      <td>1544</td>\n",
              "      <td>2494</td>\n",
              "      <td>525</td>\n",
              "      <td>1083</td>\n",
              "      <td>-62.00</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 11:00:00</th>\n",
              "      <td>4863.50</td>\n",
              "      <td>1630.00</td>\n",
              "      <td>3253</td>\n",
              "      <td>32300</td>\n",
              "      <td>6001</td>\n",
              "      <td>168</td>\n",
              "      <td>4826.25</td>\n",
              "      <td>6596.0</td>\n",
              "      <td>1565</td>\n",
              "      <td>2506</td>\n",
              "      <td>464</td>\n",
              "      <td>1092</td>\n",
              "      <td>-56.08</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 12:00:00</th>\n",
              "      <td>4850.25</td>\n",
              "      <td>1729.75</td>\n",
              "      <td>3289</td>\n",
              "      <td>31597</td>\n",
              "      <td>6409</td>\n",
              "      <td>168</td>\n",
              "      <td>5138.50</td>\n",
              "      <td>6359.0</td>\n",
              "      <td>1595</td>\n",
              "      <td>2521</td>\n",
              "      <td>903</td>\n",
              "      <td>1086</td>\n",
              "      <td>-49.96</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 13:00:00</th>\n",
              "      <td>4850.50</td>\n",
              "      <td>1742.75</td>\n",
              "      <td>3286</td>\n",
              "      <td>29291</td>\n",
              "      <td>5685</td>\n",
              "      <td>168</td>\n",
              "      <td>6297.75</td>\n",
              "      <td>6348.0</td>\n",
              "      <td>1633</td>\n",
              "      <td>2537</td>\n",
              "      <td>315</td>\n",
              "      <td>1092</td>\n",
              "      <td>-24.29</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 14:00:00</th>\n",
              "      <td>4860.75</td>\n",
              "      <td>1738.25</td>\n",
              "      <td>3281</td>\n",
              "      <td>25579</td>\n",
              "      <td>4029</td>\n",
              "      <td>168</td>\n",
              "      <td>7285.50</td>\n",
              "      <td>6366.0</td>\n",
              "      <td>1632</td>\n",
              "      <td>2547</td>\n",
              "      <td>421</td>\n",
              "      <td>1091</td>\n",
              "      <td>-5.59</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 15:00:00</th>\n",
              "      <td>4866.75</td>\n",
              "      <td>1733.00</td>\n",
              "      <td>4232</td>\n",
              "      <td>22183</td>\n",
              "      <td>1548</td>\n",
              "      <td>168</td>\n",
              "      <td>7966.00</td>\n",
              "      <td>6897.0</td>\n",
              "      <td>1806</td>\n",
              "      <td>2559</td>\n",
              "      <td>365</td>\n",
              "      <td>1088</td>\n",
              "      <td>0.23</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 16:00:00</th>\n",
              "      <td>4867.25</td>\n",
              "      <td>1737.25</td>\n",
              "      <td>4007</td>\n",
              "      <td>19523</td>\n",
              "      <td>41</td>\n",
              "      <td>174</td>\n",
              "      <td>8594.50</td>\n",
              "      <td>9121.0</td>\n",
              "      <td>1786</td>\n",
              "      <td>2831</td>\n",
              "      <td>1455</td>\n",
              "      <td>1121</td>\n",
              "      <td>11.02</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 17:00:00</th>\n",
              "      <td>4876.75</td>\n",
              "      <td>1710.50</td>\n",
              "      <td>3688</td>\n",
              "      <td>18211</td>\n",
              "      <td>0</td>\n",
              "      <td>182</td>\n",
              "      <td>8905.25</td>\n",
              "      <td>10710.0</td>\n",
              "      <td>1851</td>\n",
              "      <td>2944</td>\n",
              "      <td>3014</td>\n",
              "      <td>1133</td>\n",
              "      <td>23.50</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 18:00:00</th>\n",
              "      <td>4881.25</td>\n",
              "      <td>1702.50</td>\n",
              "      <td>3256</td>\n",
              "      <td>17723</td>\n",
              "      <td>0</td>\n",
              "      <td>188</td>\n",
              "      <td>9019.25</td>\n",
              "      <td>11266.0</td>\n",
              "      <td>1914</td>\n",
              "      <td>3005</td>\n",
              "      <td>5423</td>\n",
              "      <td>1153</td>\n",
              "      <td>22.91</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 19:00:00</th>\n",
              "      <td>4878.25</td>\n",
              "      <td>1702.25</td>\n",
              "      <td>3105</td>\n",
              "      <td>17339</td>\n",
              "      <td>0</td>\n",
              "      <td>213</td>\n",
              "      <td>9089.00</td>\n",
              "      <td>11755.0</td>\n",
              "      <td>1753</td>\n",
              "      <td>3205</td>\n",
              "      <td>4836</td>\n",
              "      <td>1193</td>\n",
              "      <td>21.02</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 20:00:00</th>\n",
              "      <td>4855.50</td>\n",
              "      <td>1704.00</td>\n",
              "      <td>2859</td>\n",
              "      <td>16499</td>\n",
              "      <td>0</td>\n",
              "      <td>216</td>\n",
              "      <td>9104.50</td>\n",
              "      <td>12142.0</td>\n",
              "      <td>1985</td>\n",
              "      <td>3293</td>\n",
              "      <td>4134</td>\n",
              "      <td>1193</td>\n",
              "      <td>21.05</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 21:00:00</th>\n",
              "      <td>4846.75</td>\n",
              "      <td>1703.25</td>\n",
              "      <td>2556</td>\n",
              "      <td>16489</td>\n",
              "      <td>0</td>\n",
              "      <td>215</td>\n",
              "      <td>9112.00</td>\n",
              "      <td>12551.0</td>\n",
              "      <td>2040</td>\n",
              "      <td>3311</td>\n",
              "      <td>3005</td>\n",
              "      <td>1200</td>\n",
              "      <td>21.95</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 22:00:00</th>\n",
              "      <td>4826.50</td>\n",
              "      <td>1701.75</td>\n",
              "      <td>2346</td>\n",
              "      <td>16059</td>\n",
              "      <td>0</td>\n",
              "      <td>208</td>\n",
              "      <td>9110.50</td>\n",
              "      <td>13019.0</td>\n",
              "      <td>2009</td>\n",
              "      <td>3289</td>\n",
              "      <td>1217</td>\n",
              "      <td>1203</td>\n",
              "      <td>23.52</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 23:00:00</th>\n",
              "      <td>4802.00</td>\n",
              "      <td>1698.00</td>\n",
              "      <td>1946</td>\n",
              "      <td>15814</td>\n",
              "      <td>0</td>\n",
              "      <td>209</td>\n",
              "      <td>9128.50</td>\n",
              "      <td>12914.0</td>\n",
              "      <td>1939</td>\n",
              "      <td>3291</td>\n",
              "      <td>647</td>\n",
              "      <td>1201</td>\n",
              "      <td>18.96</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ab1fee7-e715-495d-96a2-254ff4c3f75d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7ab1fee7-e715-495d-96a2-254ff4c3f75d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7ab1fee7-e715-495d-96a2-254ff4c3f75d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "df.head(24)"
      ],
      "id": "6e9703d9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e3c0984",
        "outputId": "ce12439a-8b9f-4529-a124-8d9596e84d00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(df.shape[0]==365*24*2)"
      ],
      "id": "7e3c0984"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13a5c7be"
      },
      "source": [
        "## Causal inference"
      ],
      "id": "13a5c7be"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1529038"
      },
      "source": [
        "#### A simple double machine learning example (Partial Least Squares)"
      ],
      "id": "b1529038"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW37Tp_tp4xR",
        "outputId": "8e261625-ea63-424d-e0d2-e59db757480f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting DoubleML\n",
            "  Downloading DoubleML-0.4.1-py3-none-any.whl (116 kB)\n",
            "\u001b[K     || 116 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from DoubleML) (0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from DoubleML) (1.4.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from DoubleML) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from DoubleML) (1.21.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from DoubleML) (1.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from DoubleML) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->DoubleML) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->DoubleML) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->DoubleML) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->DoubleML) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->DoubleML) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->DoubleML) (0.5.2)\n",
            "Installing collected packages: DoubleML\n",
            "Successfully installed DoubleML-0.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U DoubleML"
      ],
      "id": "AW37Tp_tp4xR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87334a7c",
        "outputId": "181a6189-9856-4004-8589-d96d827a2217"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "import doubleml as dml\n",
        "from doubleml import DoubleMLData\n",
        "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier"
      ],
      "id": "87334a7c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45c84b22"
      },
      "outputs": [],
      "source": [
        "cols=np.array(df.columns)"
      ],
      "id": "45c84b22"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce99c2a5",
        "outputId": "87c08492-f132-4201-ebd9-17bd62057672"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<doubleml.double_ml_data.DoubleMLData at 0x7f07098ff2d0>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features = [col for col in cols if col!='Hydropower[MWh]' and col!='Price,Germany' and col!='Total Production']\n",
        "\n",
        "data = dml.DoubleMLData(df,\n",
        "                                 y_col=\"Price,Germany\",\n",
        "                                 d_cols=\"Hydropower[MWh]\",\n",
        "                                 x_cols=features)\n",
        "data"
      ],
      "id": "ce99c2a5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc6cda77"
      },
      "outputs": [],
      "source": [
        "forest = RandomForestRegressor(max_depth=5, min_samples_leaf=2)\n",
        "dml = dml.DoubleMLPLR(data,\n",
        "                                ml_g = forest,\n",
        "                                ml_m = forest)"
      ],
      "id": "fc6cda77"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdfc43ca",
        "outputId": "98ac6958-d01f-428e-8fc2-d3932a36af02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<doubleml.double_ml_plr.DoubleMLPLR at 0x7f07097ad8d0>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dml.fit()"
      ],
      "id": "cdfc43ca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "c99985e6",
        "outputId": "27c9a209-0e7f-4b14-e844-aab2f36cf346"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-dea1265e-49db-4a99-8bd3-cee74a8f06a3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "      <th>std err</th>\n",
              "      <th>t</th>\n",
              "      <th>P&gt;|t|</th>\n",
              "      <th>2.5 %</th>\n",
              "      <th>97.5 %</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Hydropower[MWh]</th>\n",
              "      <td>-0.000134</td>\n",
              "      <td>0.000398</td>\n",
              "      <td>-0.335755</td>\n",
              "      <td>0.737056</td>\n",
              "      <td>-0.000914</td>\n",
              "      <td>0.000647</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dea1265e-49db-4a99-8bd3-cee74a8f06a3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dea1265e-49db-4a99-8bd3-cee74a8f06a3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dea1265e-49db-4a99-8bd3-cee74a8f06a3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                     coef   std err         t     P>|t|     2.5 %    97.5 %\n",
              "Hydropower[MWh] -0.000134  0.000398 -0.335755  0.737056 -0.000914  0.000647"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dml.summary"
      ],
      "id": "c99985e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87f9e3ab"
      },
      "source": [
        "#### Hourly-based treatment effects"
      ],
      "id": "87f9e3ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a503cb08"
      },
      "outputs": [],
      "source": [
        "features = ['Biomass[MWh]',\n",
        "            'Hydropower[MWh]',\n",
        "            'Wind offshore[MWh]',\n",
        "            'Wind onshore[MWh]',\n",
        "            'Photovoltaics[MWh]',\n",
        "            'Other renewable[MWh]',\n",
        "            'Nuclear[MWh]',\n",
        "            'Fossil brown coal[MWh]',\n",
        "            'Fossil hard coal[MWh]',\n",
        "            'Fossil gas[MWh]',\n",
        "            'Hydro pumped storage[MWh]',\n",
        "            'Other conventional[MWh]']"
      ],
      "id": "a503cb08"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ed39480"
      },
      "outputs": [],
      "source": [
        "import doubleml as dml\n",
        "from doubleml import DoubleMLData\n",
        "#To avoid the bug, one must import the packages twice."
      ],
      "id": "4ed39480"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53258d5e"
      },
      "outputs": [],
      "source": [
        "class TreatmentEffects:\n",
        "    def __init__(self,data,features,y):\n",
        "        self.features = features\n",
        "        self.data     = data\n",
        "        self.y        = y\n",
        "   \n",
        "    def DoubleML(self):\n",
        "        n = len(self.features)\n",
        "        d = {}\n",
        "        \n",
        "        forest1 = RandomForestRegressor(max_depth = 5, min_samples_leaf = 2)\n",
        "        forest2 = RandomForestClassifier(max_depth = 5, min_samples_leaf = 2)\n",
        "        for i in np.arange(24):\n",
        "            print('round'+' '+str(i+1))\n",
        "            dtf = self.data[self.data['hour'] == i].drop('hour', 1)\n",
        "            d[str(i)+':00'] = {}\n",
        "            for j in np.arange(n):\n",
        "                f = [col for col in self.features if col != self.features[j]]\n",
        "                data = dml.DoubleMLData(dtf,\n",
        "                                 y_col  = self.y,\n",
        "                                 d_cols = self.features[j],\n",
        "                                 x_cols = f)\n",
        "                \n",
        "                if dtf[self.features[j]].dtype == 'uint8':\n",
        "                    forest = forest2\n",
        "                else:\n",
        "                    forest = forest1\n",
        "                    \n",
        "                dbl = dml.DoubleMLPLR(data,\n",
        "                                ml_g = forest1,\n",
        "                                ml_m = forest)\n",
        "                dbl.fit()\n",
        "                d[str(i)+':00'][self.features[j]] = float(dbl.coef)\n",
        "        self.coefs = d\n",
        "    \n",
        "    def fit(self):\n",
        "        res = np.transpose(pd.DataFrame.from_dict(self.coefs))\n",
        "        mse = {}\n",
        "        self.data['predict'] = 0\n",
        "        for i in np.arange(24):\n",
        "            dtf = self.data[self.data['hour'] == i].drop(['hour',self.y,'predict'], 1)\n",
        "            coef = res.iloc[i,:]\n",
        "            self.data.loc[self.data['hour']==i,'predict'] = dtf@coef\n",
        "            n = len(self.data.loc[self.data['hour']==i,'predict'])\n",
        "            mse[str(i)+':00'] = (1/n) * sum((self.data.loc[self.data['hour']==i,'predict'] - self.data.loc[self.data['hour']==i,'Price,Germany'])**2)\n",
        "        self.prediction = pd.DataFrame(self.data['predict'])\n",
        "        self.mse_train = pd.DataFrame([mse])\n",
        "        \n",
        "##    def predict(self,test_data):\n",
        "            \n",
        "            \n",
        "            \n",
        "            "
      ],
      "id": "53258d5e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be582bb1",
        "outputId": "4af332bd-d032-4574-c71b-c923195e0a54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "result = TreatmentEffects(df.drop('Total Production',1),features,'Price,Germany')"
      ],
      "id": "be582bb1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "06533dc7",
        "outputId": "7a140d78-8273-45e7-da18-820f753a700a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        }
      ],
      "source": [
        "result.DoubleML()"
      ],
      "id": "06533dc7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8a5a469d",
        "outputId": "88aafeeb-a76d-46b8-bf5f-808bfda6104c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9e749cd1-1d18-4742-8d66-f992ccdf4386\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Biomass[MWh]</th>\n",
              "      <th>Hydropower[MWh]</th>\n",
              "      <th>Wind offshore[MWh]</th>\n",
              "      <th>Wind onshore[MWh]</th>\n",
              "      <th>Photovoltaics[MWh]</th>\n",
              "      <th>Other renewable[MWh]</th>\n",
              "      <th>Nuclear[MWh]</th>\n",
              "      <th>Fossil brown coal[MWh]</th>\n",
              "      <th>Fossil hard coal[MWh]</th>\n",
              "      <th>Fossil gas[MWh]</th>\n",
              "      <th>Hydro pumped storage[MWh]</th>\n",
              "      <th>Other conventional[MWh]</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0:00</th>\n",
              "      <td>-0.001144</td>\n",
              "      <td>-0.000952</td>\n",
              "      <td>-0.000119</td>\n",
              "      <td>-0.000596</td>\n",
              "      <td>-0.040684</td>\n",
              "      <td>-0.087821</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.001037</td>\n",
              "      <td>-0.000209</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.000221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1:00</th>\n",
              "      <td>-0.002054</td>\n",
              "      <td>0.001503</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>-0.000625</td>\n",
              "      <td>-0.105862</td>\n",
              "      <td>-0.071543</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.000559</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>-0.000073</td>\n",
              "      <td>-0.001297</td>\n",
              "      <td>0.001333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2:00</th>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.001889</td>\n",
              "      <td>-0.000215</td>\n",
              "      <td>-0.000729</td>\n",
              "      <td>-0.192566</td>\n",
              "      <td>-0.065576</td>\n",
              "      <td>0.001459</td>\n",
              "      <td>0.001330</td>\n",
              "      <td>0.001008</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>-0.007147</td>\n",
              "      <td>0.001687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3:00</th>\n",
              "      <td>-0.000185</td>\n",
              "      <td>0.000828</td>\n",
              "      <td>-0.000039</td>\n",
              "      <td>-0.000624</td>\n",
              "      <td>-0.027398</td>\n",
              "      <td>-0.079546</td>\n",
              "      <td>0.001376</td>\n",
              "      <td>0.000835</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.000331</td>\n",
              "      <td>0.002311</td>\n",
              "      <td>0.001493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4:00</th>\n",
              "      <td>-0.001048</td>\n",
              "      <td>-0.000275</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>-0.000572</td>\n",
              "      <td>0.108268</td>\n",
              "      <td>-0.092946</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.000686</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.001051</td>\n",
              "      <td>0.001012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5:00</th>\n",
              "      <td>-0.001602</td>\n",
              "      <td>0.000573</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>-0.000466</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>-0.091036</td>\n",
              "      <td>0.001180</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>-0.000694</td>\n",
              "      <td>0.001121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6:00</th>\n",
              "      <td>-0.005120</td>\n",
              "      <td>-0.000065</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>-0.000271</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>-0.087230</td>\n",
              "      <td>0.000988</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>0.001312</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.003056</td>\n",
              "      <td>0.001034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7:00</th>\n",
              "      <td>-0.003602</td>\n",
              "      <td>-0.001627</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>-0.000320</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>-0.124323</td>\n",
              "      <td>0.001395</td>\n",
              "      <td>0.000728</td>\n",
              "      <td>0.001652</td>\n",
              "      <td>0.000658</td>\n",
              "      <td>0.002597</td>\n",
              "      <td>0.001285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8:00</th>\n",
              "      <td>-0.001280</td>\n",
              "      <td>-0.000482</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.000313</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>-0.153971</td>\n",
              "      <td>0.000868</td>\n",
              "      <td>0.000855</td>\n",
              "      <td>0.001810</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>0.002629</td>\n",
              "      <td>0.001722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9:00</th>\n",
              "      <td>-0.002199</td>\n",
              "      <td>0.001185</td>\n",
              "      <td>-0.000098</td>\n",
              "      <td>-0.000404</td>\n",
              "      <td>-0.000119</td>\n",
              "      <td>-0.161630</td>\n",
              "      <td>0.001918</td>\n",
              "      <td>0.000693</td>\n",
              "      <td>0.001795</td>\n",
              "      <td>0.000883</td>\n",
              "      <td>0.002138</td>\n",
              "      <td>0.001194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10:00</th>\n",
              "      <td>-0.002023</td>\n",
              "      <td>0.000813</td>\n",
              "      <td>-0.000424</td>\n",
              "      <td>-0.000363</td>\n",
              "      <td>-0.000165</td>\n",
              "      <td>-0.162922</td>\n",
              "      <td>0.001884</td>\n",
              "      <td>0.000927</td>\n",
              "      <td>0.001608</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.001976</td>\n",
              "      <td>0.001291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11:00</th>\n",
              "      <td>-0.003162</td>\n",
              "      <td>0.000760</td>\n",
              "      <td>-0.000110</td>\n",
              "      <td>-0.000398</td>\n",
              "      <td>-0.000281</td>\n",
              "      <td>-0.149895</td>\n",
              "      <td>0.002173</td>\n",
              "      <td>0.000733</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.001094</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.001959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12:00</th>\n",
              "      <td>-0.002165</td>\n",
              "      <td>0.003499</td>\n",
              "      <td>-0.000204</td>\n",
              "      <td>-0.000376</td>\n",
              "      <td>-0.000425</td>\n",
              "      <td>-0.173699</td>\n",
              "      <td>0.002435</td>\n",
              "      <td>0.000869</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>0.002289</td>\n",
              "      <td>0.001776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13:00</th>\n",
              "      <td>-0.003023</td>\n",
              "      <td>0.002841</td>\n",
              "      <td>-0.000028</td>\n",
              "      <td>-0.000335</td>\n",
              "      <td>-0.000458</td>\n",
              "      <td>-0.168921</td>\n",
              "      <td>0.002518</td>\n",
              "      <td>0.001120</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.000994</td>\n",
              "      <td>0.002290</td>\n",
              "      <td>0.002532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14:00</th>\n",
              "      <td>-0.003214</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>-0.000292</td>\n",
              "      <td>-0.000311</td>\n",
              "      <td>-0.153442</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>0.001134</td>\n",
              "      <td>0.001376</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.002406</td>\n",
              "      <td>0.002194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15:00</th>\n",
              "      <td>-0.003096</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>-0.000294</td>\n",
              "      <td>-0.000231</td>\n",
              "      <td>-0.145819</td>\n",
              "      <td>0.001345</td>\n",
              "      <td>0.001243</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.001004</td>\n",
              "      <td>0.001736</td>\n",
              "      <td>0.002766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16:00</th>\n",
              "      <td>-0.003627</td>\n",
              "      <td>-0.001307</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>-0.000254</td>\n",
              "      <td>-0.000274</td>\n",
              "      <td>-0.126676</td>\n",
              "      <td>0.001001</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.001685</td>\n",
              "      <td>0.001052</td>\n",
              "      <td>0.001466</td>\n",
              "      <td>0.002965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17:00</th>\n",
              "      <td>-0.003146</td>\n",
              "      <td>-0.002278</td>\n",
              "      <td>0.000166</td>\n",
              "      <td>-0.000240</td>\n",
              "      <td>-0.000141</td>\n",
              "      <td>-0.144807</td>\n",
              "      <td>0.000445</td>\n",
              "      <td>0.000655</td>\n",
              "      <td>0.001749</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.002530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18:00</th>\n",
              "      <td>-0.005034</td>\n",
              "      <td>-0.002473</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>-0.000364</td>\n",
              "      <td>-0.000225</td>\n",
              "      <td>-0.128945</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.000626</td>\n",
              "      <td>0.001819</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>0.001402</td>\n",
              "      <td>0.001533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19:00</th>\n",
              "      <td>-0.005770</td>\n",
              "      <td>-0.002670</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>-0.000396</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>-0.142559</td>\n",
              "      <td>0.000383</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.001608</td>\n",
              "      <td>0.000680</td>\n",
              "      <td>0.002009</td>\n",
              "      <td>0.001005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20:00</th>\n",
              "      <td>-0.004978</td>\n",
              "      <td>-0.002284</td>\n",
              "      <td>-0.000143</td>\n",
              "      <td>-0.000386</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>-0.136830</td>\n",
              "      <td>-0.000038</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.000560</td>\n",
              "      <td>0.001498</td>\n",
              "      <td>0.001248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21:00</th>\n",
              "      <td>-0.003531</td>\n",
              "      <td>-0.001136</td>\n",
              "      <td>-0.000161</td>\n",
              "      <td>-0.000463</td>\n",
              "      <td>-0.001539</td>\n",
              "      <td>-0.116371</td>\n",
              "      <td>-0.000057</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.001192</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.001413</td>\n",
              "      <td>0.000983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22:00</th>\n",
              "      <td>-0.000827</td>\n",
              "      <td>-0.002597</td>\n",
              "      <td>-0.000148</td>\n",
              "      <td>-0.000473</td>\n",
              "      <td>-0.002914</td>\n",
              "      <td>-0.103730</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>0.000930</td>\n",
              "      <td>-0.000230</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>0.000941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23:00</th>\n",
              "      <td>-0.001896</td>\n",
              "      <td>-0.002419</td>\n",
              "      <td>-0.000065</td>\n",
              "      <td>-0.000593</td>\n",
              "      <td>-0.000576</td>\n",
              "      <td>-0.075375</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>0.000887</td>\n",
              "      <td>-0.000227</td>\n",
              "      <td>0.002275</td>\n",
              "      <td>0.000892</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e749cd1-1d18-4742-8d66-f992ccdf4386')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9e749cd1-1d18-4742-8d66-f992ccdf4386 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9e749cd1-1d18-4742-8d66-f992ccdf4386');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Biomass[MWh]  Hydropower[MWh]  Wind offshore[MWh]  Wind onshore[MWh]  \\\n",
              "0:00      -0.001144        -0.000952           -0.000119          -0.000596   \n",
              "1:00      -0.002054         0.001503            0.000173          -0.000625   \n",
              "2:00       0.000064         0.001889           -0.000215          -0.000729   \n",
              "3:00      -0.000185         0.000828           -0.000039          -0.000624   \n",
              "4:00      -0.001048        -0.000275            0.000157          -0.000572   \n",
              "5:00      -0.001602         0.000573            0.000205          -0.000466   \n",
              "6:00      -0.005120        -0.000065            0.000147          -0.000271   \n",
              "7:00      -0.003602        -0.001627            0.000222          -0.000320   \n",
              "8:00      -0.001280        -0.000482            0.000200          -0.000313   \n",
              "9:00      -0.002199         0.001185           -0.000098          -0.000404   \n",
              "10:00     -0.002023         0.000813           -0.000424          -0.000363   \n",
              "11:00     -0.003162         0.000760           -0.000110          -0.000398   \n",
              "12:00     -0.002165         0.003499           -0.000204          -0.000376   \n",
              "13:00     -0.003023         0.002841           -0.000028          -0.000335   \n",
              "14:00     -0.003214         0.001290            0.000139          -0.000292   \n",
              "15:00     -0.003096         0.000012            0.000168          -0.000294   \n",
              "16:00     -0.003627        -0.001307            0.000168          -0.000254   \n",
              "17:00     -0.003146        -0.002278            0.000166          -0.000240   \n",
              "18:00     -0.005034        -0.002473            0.000174          -0.000364   \n",
              "19:00     -0.005770        -0.002670            0.000006          -0.000396   \n",
              "20:00     -0.004978        -0.002284           -0.000143          -0.000386   \n",
              "21:00     -0.003531        -0.001136           -0.000161          -0.000463   \n",
              "22:00     -0.000827        -0.002597           -0.000148          -0.000473   \n",
              "23:00     -0.001896        -0.002419           -0.000065          -0.000593   \n",
              "\n",
              "       Photovoltaics[MWh]  Other renewable[MWh]  Nuclear[MWh]  \\\n",
              "0:00            -0.040684             -0.087821      0.001007   \n",
              "1:00            -0.105862             -0.071543      0.001308   \n",
              "2:00            -0.192566             -0.065576      0.001459   \n",
              "3:00            -0.027398             -0.079546      0.001376   \n",
              "4:00             0.108268             -0.092946      0.001007   \n",
              "5:00             0.000951             -0.091036      0.001180   \n",
              "6:00             0.000150             -0.087230      0.000988   \n",
              "7:00             0.000615             -0.124323      0.001395   \n",
              "8:00             0.000114             -0.153971      0.000868   \n",
              "9:00            -0.000119             -0.161630      0.001918   \n",
              "10:00           -0.000165             -0.162922      0.001884   \n",
              "11:00           -0.000281             -0.149895      0.002173   \n",
              "12:00           -0.000425             -0.173699      0.002435   \n",
              "13:00           -0.000458             -0.168921      0.002518   \n",
              "14:00           -0.000311             -0.153442      0.001528   \n",
              "15:00           -0.000231             -0.145819      0.001345   \n",
              "16:00           -0.000274             -0.126676      0.001001   \n",
              "17:00           -0.000141             -0.144807      0.000445   \n",
              "18:00           -0.000225             -0.128945      0.000324   \n",
              "19:00            0.000178             -0.142559      0.000383   \n",
              "20:00            0.001308             -0.136830     -0.000038   \n",
              "21:00           -0.001539             -0.116371     -0.000057   \n",
              "22:00           -0.002914             -0.103730      0.000412   \n",
              "23:00           -0.000576             -0.075375      0.000571   \n",
              "\n",
              "       Fossil brown coal[MWh]  Fossil hard coal[MWh]  Fossil gas[MWh]  \\\n",
              "0:00                 0.000221               0.001037        -0.000209   \n",
              "1:00                 0.000559               0.001156        -0.000073   \n",
              "2:00                 0.001330               0.001008         0.000322   \n",
              "3:00                 0.000835               0.001072         0.000331   \n",
              "4:00                 0.000686               0.001073         0.000285   \n",
              "5:00                 0.000903               0.001109         0.000229   \n",
              "6:00                 0.000917               0.001312         0.000265   \n",
              "7:00                 0.000728               0.001652         0.000658   \n",
              "8:00                 0.000855               0.001810         0.001005   \n",
              "9:00                 0.000693               0.001795         0.000883   \n",
              "10:00                0.000927               0.001608         0.000800   \n",
              "11:00                0.000733               0.001326         0.001094   \n",
              "12:00                0.000869               0.001045         0.000831   \n",
              "13:00                0.001120               0.001242         0.000994   \n",
              "14:00                0.001134               0.001376         0.001200   \n",
              "15:00                0.001243               0.001517         0.001004   \n",
              "16:00                0.001100               0.001685         0.001052   \n",
              "17:00                0.000655               0.001749         0.000961   \n",
              "18:00                0.000626               0.001819         0.000618   \n",
              "19:00                0.000306               0.001608         0.000680   \n",
              "20:00                0.000267               0.001250         0.000560   \n",
              "21:00                0.000140               0.001192         0.000118   \n",
              "22:00                0.000193               0.000930        -0.000230   \n",
              "23:00                0.000530               0.000887        -0.000227   \n",
              "\n",
              "       Hydro pumped storage[MWh]  Other conventional[MWh]  \n",
              "0:00                    0.000679                 0.000221  \n",
              "1:00                   -0.001297                 0.001333  \n",
              "2:00                   -0.007147                 0.001687  \n",
              "3:00                    0.002311                 0.001493  \n",
              "4:00                    0.001051                 0.001012  \n",
              "5:00                   -0.000694                 0.001121  \n",
              "6:00                    0.003056                 0.001034  \n",
              "7:00                    0.002597                 0.001285  \n",
              "8:00                    0.002629                 0.001722  \n",
              "9:00                    0.002138                 0.001194  \n",
              "10:00                   0.001976                 0.001291  \n",
              "11:00                   0.001326                 0.001959  \n",
              "12:00                   0.002289                 0.001776  \n",
              "13:00                   0.002290                 0.002532  \n",
              "14:00                   0.002406                 0.002194  \n",
              "15:00                   0.001736                 0.002766  \n",
              "16:00                   0.001466                 0.002965  \n",
              "17:00                   0.001307                 0.002530  \n",
              "18:00                   0.001402                 0.001533  \n",
              "19:00                   0.002009                 0.001005  \n",
              "20:00                   0.001498                 0.001248  \n",
              "21:00                   0.001413                 0.000983  \n",
              "22:00                   0.001671                 0.000941  \n",
              "23:00                   0.002275                 0.000892  "
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coef = np.transpose(pd.DataFrame.from_dict(result.coefs))\n",
        "coef.head(24)"
      ],
      "id": "8a5a469d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7c9ccad"
      },
      "outputs": [],
      "source": [
        "result.fit()"
      ],
      "id": "d7c9ccad"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76165686"
      },
      "source": [
        "#### Predictions"
      ],
      "id": "76165686"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66b63e03"
      },
      "outputs": [],
      "source": [
        "pred = result.prediction\n",
        "pred.head(24)\n",
        "# Daily profile for 2018-01-01"
      ],
      "id": "66b63e03"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d05bc5cc"
      },
      "source": [
        "#### In-sample error"
      ],
      "id": "d05bc5cc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6718112",
        "outputId": "dfa5853d-65d0-437d-9fee-590ef19288b7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0:00</th>\n",
              "      <td>2225.742862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1:00</th>\n",
              "      <td>1388.410389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2:00</th>\n",
              "      <td>310.669951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3:00</th>\n",
              "      <td>681.756566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4:00</th>\n",
              "      <td>991.902099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5:00</th>\n",
              "      <td>834.063903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6:00</th>\n",
              "      <td>2835.129895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7:00</th>\n",
              "      <td>1548.912262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8:00</th>\n",
              "      <td>2244.223198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9:00</th>\n",
              "      <td>1805.206007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10:00</th>\n",
              "      <td>2240.555061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11:00</th>\n",
              "      <td>1652.808771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12:00</th>\n",
              "      <td>1479.017497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13:00</th>\n",
              "      <td>722.633884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14:00</th>\n",
              "      <td>1092.019424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15:00</th>\n",
              "      <td>1509.335709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16:00</th>\n",
              "      <td>1566.588704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17:00</th>\n",
              "      <td>2371.663694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18:00</th>\n",
              "      <td>4901.859690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19:00</th>\n",
              "      <td>7215.730860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20:00</th>\n",
              "      <td>7815.945509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21:00</th>\n",
              "      <td>5297.627835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22:00</th>\n",
              "      <td>3386.919720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23:00</th>\n",
              "      <td>2532.938920</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "0:00   2225.742862\n",
              "1:00   1388.410389\n",
              "2:00    310.669951\n",
              "3:00    681.756566\n",
              "4:00    991.902099\n",
              "5:00    834.063903\n",
              "6:00   2835.129895\n",
              "7:00   1548.912262\n",
              "8:00   2244.223198\n",
              "9:00   1805.206007\n",
              "10:00  2240.555061\n",
              "11:00  1652.808771\n",
              "12:00  1479.017497\n",
              "13:00   722.633884\n",
              "14:00  1092.019424\n",
              "15:00  1509.335709\n",
              "16:00  1566.588704\n",
              "17:00  2371.663694\n",
              "18:00  4901.859690\n",
              "19:00  7215.730860\n",
              "20:00  7815.945509\n",
              "21:00  5297.627835\n",
              "22:00  3386.919720\n",
              "23:00  2532.938920"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.transpose(result.mse_train)"
      ],
      "id": "c6718112"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82779f37"
      },
      "source": [
        "The training error is obviously too high"
      ],
      "id": "82779f37"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97c4a1be"
      },
      "source": [
        "#### Estimate seasonal effects"
      ],
      "id": "97c4a1be"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d38683e6"
      },
      "outputs": [],
      "source": [
        "df_new = df.drop('Total Production',1)\n",
        "df_new['year']=df_new.index.year.astype(object)\n",
        "df_new['month']=df_new.index.month.astype(object)"
      ],
      "id": "d38683e6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83a6c287"
      },
      "outputs": [],
      "source": [
        "df_new = pd.concat([df_new[features+['hour','Price,Germany']],pd.get_dummies(df_new[['year','month']])],1)"
      ],
      "id": "83a6c287"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b9c0e13"
      },
      "outputs": [],
      "source": [
        "features_new = [element for element in list(df_new.columns) if element != 'hour' and element != 'Price,Germany']\n",
        "result2 = TreatmentEffects(df_new,features_new,'Price,Germany')"
      ],
      "id": "2b9c0e13"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "efcd30b3",
        "outputId": "e67cdf8b-8727-415f-c1c1-65780b1df695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        }
      ],
      "source": [
        "result2.DoubleML()"
      ],
      "id": "efcd30b3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b859a214",
        "outputId": "006ab8eb-e846-4c60-d220-69c3449ca538"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
          ]
        }
      ],
      "source": [
        "result2.fit()"
      ],
      "id": "b859a214"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "76678781",
        "outputId": "5262b09b-45a6-47af-faf7-713884cbcc28"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5a72dfae-b924-4219-850a-eb6731e4a5eb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dayTime</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2018-01-01 00:00:00</th>\n",
              "      <td>-26.021466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 01:00:00</th>\n",
              "      <td>-24.761477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 02:00:00</th>\n",
              "      <td>-5.718536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 03:00:00</th>\n",
              "      <td>-15.711965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 04:00:00</th>\n",
              "      <td>-29.442636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 05:00:00</th>\n",
              "      <td>-27.735411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 06:00:00</th>\n",
              "      <td>-31.035784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 07:00:00</th>\n",
              "      <td>-37.384714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 08:00:00</th>\n",
              "      <td>-40.900065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 09:00:00</th>\n",
              "      <td>-39.678406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 10:00:00</th>\n",
              "      <td>-26.418463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 11:00:00</th>\n",
              "      <td>-31.180508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 12:00:00</th>\n",
              "      <td>-21.472705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 13:00:00</th>\n",
              "      <td>-17.651969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 14:00:00</th>\n",
              "      <td>-8.214829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 15:00:00</th>\n",
              "      <td>-0.132927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 16:00:00</th>\n",
              "      <td>-4.031780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 17:00:00</th>\n",
              "      <td>-15.607968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 18:00:00</th>\n",
              "      <td>-35.674471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 19:00:00</th>\n",
              "      <td>-38.646672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 20:00:00</th>\n",
              "      <td>-34.435484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 21:00:00</th>\n",
              "      <td>-28.280077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 22:00:00</th>\n",
              "      <td>-25.666387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01 23:00:00</th>\n",
              "      <td>-15.468856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a72dfae-b924-4219-850a-eb6731e4a5eb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5a72dfae-b924-4219-850a-eb6731e4a5eb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5a72dfae-b924-4219-850a-eb6731e4a5eb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                       predict\n",
              "dayTime                       \n",
              "2018-01-01 00:00:00 -26.021466\n",
              "2018-01-01 01:00:00 -24.761477\n",
              "2018-01-01 02:00:00  -5.718536\n",
              "2018-01-01 03:00:00 -15.711965\n",
              "2018-01-01 04:00:00 -29.442636\n",
              "2018-01-01 05:00:00 -27.735411\n",
              "2018-01-01 06:00:00 -31.035784\n",
              "2018-01-01 07:00:00 -37.384714\n",
              "2018-01-01 08:00:00 -40.900065\n",
              "2018-01-01 09:00:00 -39.678406\n",
              "2018-01-01 10:00:00 -26.418463\n",
              "2018-01-01 11:00:00 -31.180508\n",
              "2018-01-01 12:00:00 -21.472705\n",
              "2018-01-01 13:00:00 -17.651969\n",
              "2018-01-01 14:00:00  -8.214829\n",
              "2018-01-01 15:00:00  -0.132927\n",
              "2018-01-01 16:00:00  -4.031780\n",
              "2018-01-01 17:00:00 -15.607968\n",
              "2018-01-01 18:00:00 -35.674471\n",
              "2018-01-01 19:00:00 -38.646672\n",
              "2018-01-01 20:00:00 -34.435484\n",
              "2018-01-01 21:00:00 -28.280077\n",
              "2018-01-01 22:00:00 -25.666387\n",
              "2018-01-01 23:00:00 -15.468856"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred2 = result2.prediction\n",
        "pred2.head(24)"
      ],
      "id": "76678781"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d860ee5"
      },
      "source": [
        "#### In sample error"
      ],
      "id": "2d860ee5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f5db8e32"
      },
      "outputs": [],
      "source": [
        "np.transpose(result2.mse_train)"
      ],
      "id": "f5db8e32"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction"
      ],
      "metadata": {
        "id": "d2LtzYv21euA"
      },
      "id": "d2LtzYv21euA"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "UeqFUerv1eKQ"
      },
      "id": "UeqFUerv1eKQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45a0484c"
      },
      "outputs": [],
      "source": [
        "y = df['Price,Germany']\n",
        "N = len(y)\n",
        "res = np.fft.fft(y.values)"
      ],
      "id": "45a0484c"
    },
    {
      "cell_type": "code",
      "source": [
        "def fourierExtrapolation(x, n_predict):\n",
        "    n = x.size\n",
        "    n_harm = 5000         \n",
        "    t = np.arange(0, n)\n",
        "    p = np.polyfit(t, x, 1)         \n",
        "    x_notrend = x - p[0] * t       \n",
        "    x_freqdom = np.fft.fft(x_notrend)\n",
        "    f = np.fft.fftfreq(n)              \n",
        "    indexes = list(range(n))\n",
        "    indexes.sort(key=lambda i: np.absolute(f[i]))\n",
        "\n",
        "    t = np.arange(0, n + n_predict)\n",
        "    restored_sig = np.zeros(t.size)\n",
        "    for i in indexes[:1 + n_harm * 2]:\n",
        "        ampli = np.absolute(x_freqdom[i]) / n  \n",
        "        phase = np.angle(x_freqdom[i])         \n",
        "        restored_sig += ampli * np.cos(2 * np.pi * f[i] * t + phase)\n",
        "    return restored_sig + p[0] * t\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    x = np.array(y)\n",
        "    time = 60*24\n",
        "    tm   = 0*24\n",
        "    mid  = 45*24\n",
        "    n_predict = 0\n",
        "    prices = fourierExtrapolation(x, n_predict)\n",
        "    residuals = x - prices\n",
        "    plt.style.use('dark_background')\n",
        "    plt.plot(np.arange(tm, time), x[tm:time], 'C0', label='real price', color='aliceblue')\n",
        "#    plt.plot(np.arange(tm, time), prices[tm:time], 'C1', \n",
        "#            label='price approximation',color='aliceblue')\n",
        "#    plt.plot(np.arange(0, time), residuals[0:time], 'C1', \n",
        "#            label='residuals',color='gold')\n",
        "    plt.plot(np.arange(tm,mid), yhat[tm:mid], 'C2',\n",
        "            label='train',color='salmon',linestyle='--')\n",
        "#    plt.plot(np.arange(tm,mid), pred[:,-1][tm-24:], 'C2',\n",
        "#            label='train',color='salmon',linestyle='--')\n",
        "    plt.plot(np.arange(mid,time), yhat2[:time-mid], 'C2',\n",
        "            label='test',color='deepskyblue',linestyle='--')\n",
        "#    plt.plot(np.arange(mid,time), pred2[:,-1][:time-mid], 'C2',\n",
        "#            label='test',color='deepskyblue',linestyle='--')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "CZ3WuUneqgCl",
        "outputId": "188a1227-c73a-4127-a1a0-83861fd4f604"
      },
      "id": "CZ3WuUneqgCl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-bcde09da2c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#    plt.plot(np.arange(0, time), residuals[0:time], 'C1',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#            label='residuals',color='gold')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     plt.plot(np.arange(tm,mid), yhat[tm:mid], 'C2',\n\u001b[0m\u001b[1;32m     36\u001b[0m             label='train',color='salmon',linestyle='--')\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#    plt.plot(np.arange(tm,mid), pred[:,-1][tm-24:], 'C2',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'yhat' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred2[:-1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anO91v96K9Ts",
        "outputId": "5560eeee-9451-4851-81d5-a71a1c22aa2e"
      },
      "id": "anO91v96K9Ts",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(743, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dYeTFVZJ64xw"
      },
      "id": "dYeTFVZJ64xw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "data = df\n",
        "#data   = df.drop('Price,Germany',1)\n",
        "#data[\"p\"] = list(prices)\n",
        "datat = data.drop(['day','hour','month','year','Price,Germany'],1)\n",
        "scaled = scaler.fit_transform(datat)\n",
        "n_hours = round(17520/24)\n",
        "day     = 30\n",
        "reframed = series_to_supervised(scaled, day*24, 1) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "gQDPSUNYqgJd",
        "outputId": "c7cc5a95-acf6-459a-9616-2769fc6e8b35"
      },
      "id": "gQDPSUNYqgJd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4ac95a28d551>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mn_hours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17520\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mday\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mreframed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseries_to_supervised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'series_to_supervised' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discharge = []\n",
        "for i in range(1,day*24+1):\n",
        "  string = 'var13(t-'+str(i)+')'\n",
        "  discharge.append(string)\n"
      ],
      "metadata": {
        "id": "w06zLmv_TLs7"
      },
      "id": "w06zLmv_TLs7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reframed = reframed.drop(discharge, axis=1)"
      ],
      "metadata": {
        "id": "lQgcR69TMP_f"
      },
      "id": "lQgcR69TMP_f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#values = reframed.values\n",
        "#n_train_hours = 365 * 24\n",
        "#n_test_hours  = 31 * 24\n",
        "#train = values[:n_train_hours, :]\n",
        "#test = values[n_train_hours:(n_test_hours+n_train_hours), :]"
      ],
      "metadata": {
        "id": "wVyBZR0nPqtn"
      },
      "id": "wVyBZR0nPqtn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_X, train_y = train[:, :-1], train[:, -1]\n",
        "#test_X, test_y = test[:, :-1], test[:, -1]\n",
        "#train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "#test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))"
      ],
      "metadata": {
        "id": "3C3G_WAKRjPG"
      },
      "id": "3C3G_WAKRjPG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = keras.Sequential([\n",
        "#    keras.layers.LSTM(128,'relu',return_sequences=True),\n",
        "#    keras.layers.LSTM(1024,'relu',return_sequences=True),\n",
        "#    keras.layers.LSTM(1024,'relu',return_sequences=True),\n",
        "#    keras.layers.LSTM(128,'relu',return_sequences=True),\n",
        "#    keras.layers.Dense(1,'linear')\n",
        "#])\n",
        "#opt = keras.optimizers.Adam(learning_rate=0.001,amsgrad=True)\n",
        "#model.compile(optimizer=opt,\n",
        "              loss='mse')"
      ],
      "metadata": {
        "id": "kxMdxgabsM4D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "025ef7a5-f9e6-4bc6-f237-901f33d5c3dd"
      },
      "id": "kxMdxgabsM4D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#history = model.fit(train_X, train_y, epochs=200, batch_size=32, validation_data=(test_X, test_y), verbose=1)"
      ],
      "metadata": {
        "id": "FS6bbukysZPh"
      },
      "id": "FS6bbukysZPh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plt.plot(history.history['loss'], label='train')\n",
        "#plt.plot(history.history['val_loss'], label='test')\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "-E58tJUj62kv",
        "outputId": "c46cedfe-c5c5-44da-eab0-1f810007bb3a"
      },
      "id": "-E58tJUj62kv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVhUZRsG8Htm2ARkC1dAMSWFsiLFrdTU1JQUyxYrzd02c8vSNL/ULEvLtR2zzFTclyltMe37WkRJQTEFwcAAt5RFQDaZ5/sDnUAQUJh5Z8b7d1335cycd855Zmqec+acMwcNAAEREVk9reoCiIiodrChExHZCDZ0IiIbwYZORGQj2NCJiGyEnaoFnz17FidOnFC1eCIiq9S0aVPUr1+/wmnKGvqJEycQEhKiavFERFYpKirqmtO4y4WIyEawoRMR2Qg2dCIiG6FsH3pFPD09MWHCBPj7+0Oj0aguxyREBMnJyVi0aBEyMjJUl0NENsSiGvqECRPwxx9/YPbs2SguLlZdjknodDqEhoZiwoQJeOONN1SXQ0Q2xKJ2ufj7+2P79u0228wBoLi4GN9++y38/f1Vl0JENsaiGrpGo7HpZn5FcXGxze5SIiJ1LGqXC9U+L6+6GDmyJ06cOIujR1Nx7FgaCgqKVJdFRCbAhl6Ku7s7nnrqKXz88cfX9bxvv/0WTz31FLKyskxU2Y1buGg0hgzpZrxfXFyMpKQzOHo0FXFHU3D0aAqOHk3F0aMpuHDhosJKiaim2NBL8fDwwAsvvFCuoet0ukp3BYWGhpq6tBvSoUNLDBnSDfPnbcSqVT8jMNAPgYF+aBXoh8BAX/TqFQxHR3vj+JMnzxsbfVxcqrHZnzqVrvBVEFF1saGX8s4776B58+aIjo5GUVER8vPzkZGRgVatWqFly5bYvHkz/Pz84OTkhMWLFyM8PBwAkJSUhLZt28LV1RU7duzAr7/+ik6dOiEtLQ1hYWHIz883+2vRaDRYvORZpKWdx+zZEcjNzcehQ8llxuh0WjRr1qBcox/yTHe4uTkbx2Vm5lxu8GW36pOSzsBgMJj5lRHRtVhsQw97dQIatwqo1XmejEvA1nmLrjl96tSpuOOOOxAcHIyuXbvi22+/xR133IHk5GQAwIgRI5CRkQEnJydERUVh48aNSE8vu/UaEBCAJ598EmPGjMHatWsxcOBArFq1qlZfR3UMG9YDISEBGPz0e8jNrXiFUlxsQGLiKSQmnoJev6/MtMaNvUqafCvfy83eFw8+eA+GD3/AOCY/vxDHjqWVa/TcT0+khsU2dEuwb98+YzMHgHHjxuHhhx8GAPj5+SEgIAB79+4t85ykpCQcPHgQALB//34lpye6uTnj7bnP4Pffj2L16v/e0DxOnkzHyZPp+Omng2Ue9/BwMTb5K1v1bdu2wGOP3QuttuSkKe6nJ1LDYht6ZVvS5pKbm2u83bVrVzzwwAPo2LEj8vLysHv3bjg5OZV7TkFBgfF2cXEx6tSpY5ZaS5sxYxDq1XPHQ6Gza33emZm5iIyMR2RkfJnHnZwccNttja9rP33pRn/6NH81S1RTFtvQVcjOzkbdunUrnObu7o6MjAzk5eWhZcuW6NChg5mrq57bbvPBuPH98OUXO7F/f6LZlpufX4hDh5K5n55IITb0UtLT0/Hbb78hNjYWeXl5OHPmjHHad999h+eeew5HjhxBfHw8IiMjFVZ6bQsWjsLFiwWYNm2l6lIAVG8//b/N3hd9+rThfnqiG8SGfpWnn366wscLCwvRt2/fCqc1a9YMAHD+/Hm0bt3a+Pj7779f+wVWom/ftujbty0mv/w5zp7NNOuyb0Rl++lLN/qWrXy5n56oGtjQbYS9vR0WLByFuLhULF36jepyaiQzMxd79sRhz564Mo9zPz1R5djQbcT48f1w220+6NtnJoqKLqkuxyRqYz/91Y0+Lo776cl2sKHbgAYNPPD6jEH45psofPfdftXlmN1176fv2xbDR/Q0jrl6P31MTBK2bdsLETH3SyGqETZ0G/D23KFwcrLHpInhqkuxONXdT98q0BchIQHG/fQbN/6OIYPfR35+oaLKia4fG7qVCwkJwPDhD2D+vI1ITDyluhyrUdl++hde6It584dj509zMCBsDs6du6CoSqLrY1HXQ6frU3K9ljE4fToDc+asVV2OTcjPL8SCBVvw+GPvIjj4Vvz2+3w0b95IdVlE1VKtht67d2/ExcUhISEBU6ZMKTd96NChOHv2LKKjoxEdHY2RI0fWeqHm4O7ujueff/6Gnjt+/Hiz/yp08OD70aFDK7w2dQWys/PMumxbt2nT7+jRfTo8PV2xJ/I9dOjQUnVJRNUilUWr1UpiYqI0a9ZM7O3tJSYmRgIDA8uMGTp0qCxdurTS+VydqKioco999dVX1zWP2k7Tpk0lNjb2hp6blJQkt9xyS7XH1/S1urrWkdS0L2VP5Hui0WiUvm+2nBYtGkn8sU8l9+IGeeSRTsrrYZiKeueVVLmF3q5dOyQmJiIpKQlFRUWIiIhAWFhYVU+zSqUvnztv3jxMnjwZ+/btw8GDBzFz5kwAgLOzM7755hvExMQgNjYWjz/+OF566SU0btwYu3fvxq5du8xS67Rpj6Fx41swftxnPBvDhBITT+HeTq8gJiYJ69ZPwfjx/VWXRHRNVR4U9fHxQUpKivF+amoq2rdvX27cwIED0aVLFxw7dgwTJ05EampquTGjR4/GmDFjAADe3t6VLnfhwlG46+5bq3wB1+NgzF+YOHHZNaeXvnxuz5498eijj6Jdu3bQaDTYtm0bOnfujHr16uHkyZN46KGHAABubm64cOECJk2ahG7duuH8+fO1WnNFmjdvhImTBmDFip+wb98xky/vZnfu3AX06D4dX696GQsXjYa/f328/PJynrtOFqdWDorq9Xr4+/vjrrvuwo8//ogVK1ZUOC48PBwhISEICQnBuXPnamPRJtOrVy/06tUL0dHROHDgAFq1aoWAgADExsaiZ8+eeOedd3DffffhwgXznwHx3vsjUFhYhNemVvw+U+3Lzy/E44+9i0ULt2L8hDCsWz8Fdeo4qi6LqIwqt9DT0tLg5+dnvO/r64u0tLQyY0r/kYdly5Zh3rx5NS6ssi1pc9BoNJg7dy4+++yzctPuuece9O3bF3PmzMFPP/2EN99802x19ewZjLCwDpg65Uv+lN3MDAYDJk1ahuTkM1iwcBR+2jUH/fu9ydMayaJUugNep9PJ8ePHxd/f33hQNCgoqMyYhg0bGm8PGDBA9uzZc0M79lUfFPXy8pLk5GQBID179pTIyEhxcXERANK4cWOpV6+eNGrUSBwdHQWAhIaGyubNmwWAHDp0SPz9/au9rBt5rXZ2Ojn854cSf+xTcXCwU35w5mbOgAEdJPfiBjmW8KkEBDRWXg9z86Syg6JVbqEXFxdj7Nix+P7776HT6bB8+XIcOXIEs2bNwh9//AG9Xo9x48ahf//+uHTpEtLT0zFs2LCqZmuRSl8+d8eOHVi9ejX27NkDAMjJycHgwYPRokULzJ8/HwaDAUVFRcbTHD/77DN89913OHnyJLp3726S+l58MRRBQU3Qv99sFBba5vVarMWWLZHo3m06tuln4Lff5yOs/5vlfqREpILFrGVUb6GbM9f7Wr293SQ9Y41s3zFTee3Mv2nevJHExX8iF/M2ysCBPK2RqToNGnjU6Pk1Om2RLMNbbw2Bi4sTJk5Qe2yByjp+/BTu7fQq9u9PxNp1UzBxom2e0ku148knuyIu/hOMGtXLJPNnQ7cCwcHNMXJUL3yw9BvEx5c/HZTUOn/+Ano+MAObNu3B+wtGYcmSMcY/xEEElPzh9q9WTsKq1ZNx+PAJ7Nx5sOon3SAlXzsq+tqwYsUK0el0yr8SmTo6nU5WrFhR7fH/++VdOX1mpbi7uyivnbl2NBqNzJ8/Qgyil02bp0udOo7Ka2LUp1OnQDn+1zIpLNoiM2YMEp1OW6P5Wc0ul+TkZISGhkKn06kuxWR0Oh1CQ0ORnJxcrfGDBnXBffcFYfq0lcjKyjVtcVQjIoJXXlmOcS99iv7922HX7rdQr5676rJIEZ1Oi5kzn8J//zcXIoIunafgzTcjUFxsuh+kaVDS2c0uKioKISEhZR7z9PTEhAkT4O/vD41Go6IskxMRJCcnY9GiRcjIqPw8cmdnR8TFf4IzZzLRvt3L/GWiFenfvz1Wr3kFp09noG+fmTh2LK3qJ5HNuPXWhlj59cvo2LEVvvzyJ4wf92mtXUCvot5ZmpKvIZV9bWBKMnv202IQvXTqFKi8Fub6ExISIKfPrJR/zq2Se+8NUl4PY54MGdJNsi6slfSMNfL44/fV+vyr6J1qXjQbeuXx928gF/M2ysqvX1ZeC3PjadasgRyN+1gu5m2Uxx6r/Q83Yznx8HCR1WteEYPoZffPc8XPr55JlsOGboVZv+E1yc5ZLz4+1b8kL2OZ8fKqK//93ztiEL1Mnvyw8nqY2k+XLndI8onlUlC4WaZOfVS02pod+KwsbOhWlm7d7hSD6GX69MeV18LUThwd7WVNxKtiEL188MFzNT7TgbGM2NnpZM6cIXKpeKvEH/tU2rYNMPky2dCtKDqdVg4eWirH/1omTk4Oyuthai8ajUbefXeYGEQvW7fNEGdnntZozWnRopHs3bdADKKX8PCXxMXFySzLZUO3orz4YqgYRC8PP9xReS2MafL8832l6NIW2btvgdSvX7OfgTNqMmJET7mQvU7OnV9t9r9kxYZuJfHyqivnzq+WH3fOUV4LY9o89FCIZOesl+N/LZOWLX2V18NUL15edWX9htfEIHrZ+dMcJce42NCtJB9++LwUFm2R229vorwWxvRp06aFnDr9lZw7v1o6d75deT1M5enW7U75O+ULyS/YJJMnP6zsb/myoVtBWrf2l6JLW2TJkjHKa2HMF3//BvLnkY8kL3+TPPFEZ+X1MOVjb28n7747TC4Vb5UjRz+W4ODmSuthQ7eC/LTrLTn7zyrx9HRVXgtj3nh6usrun+eKQfTy6qsDldfD/JuWLX3lj/2LxCB6+fjjFyziQDYbuoVn4MBOYhC9PPdcH+W1MGri4GAnq1ZPFoPo5aOPnudpjRaQMWMelJzcDXL2n1XSv3975fVcCRu6BcfJyUH+Slom0TFLTPpjBMbyo9FoZO7coWIQvWzT/8dsp8ExZePt7Sabt0wXg+jlu+9nS8OGnsprKh02dAvOjBmDxCB66dLlDuW1MJaRZ599UIoubZGoPxbW+K/bMNeXnj2DJe3kCsnL3yQTJoQpO/BZWdjQLTR+fvUkJ3eDRKydorwWxrISGlpyWuNfScskMNBPeT22HkdHe3n//ZFiEL3EHv5Q7rzTX3lN1wobuoVm9ZpXJPfiBmnSxDQX8WGsO23atJCTp76S8+lr+A3OhAkKaiLRMUvEIHpZsmSMxf9Cmw3dAtO58+1iEL288caTymthLDdNm9aXw39+KPkFm+TJJ7sqr8fW8uKLoZJ7cYOcOv2V9O3bVnk91QkbuoVFq9XKgejFknxiOf9MGVNlPDxcZNfut8Ugepk69VHl9dhC6tf3EP03/xGD6OWbb9+wqkswsKFbWMaMeVAMouf1sZlqx8HBTlZ+/bIYRC+ffvoiT2usQfr0aSOnTn8lF/M2yosvhiqv53rDhm5B8fBwkbP/rJJdu99WXgtjXdFoNDJnzhDjVqWrax3lNVlTnJwcZPHiMWIQvcQcXGK1l9hgQ7egLFo0WooubbHoo+iMZWf06N5SWLRF/ti/SBo18lJejzWkdWt/ORT7gRhELwsWjBJHR3vlNd1o2NAtJEFBTaSwaIt89NHzymthrDsPPthGLmSvk+QTyyUoyDq3NM0RjUYj48f3l7z8TZJ2coX06hWsvKaahg3dQvL9D7PlfPoaueUWN+W1MNaf4ODmknZyhaRnrJFu3e5UXo+lpWFDT9m+Y6YYRC9btr4u3t628bljQ7eAhIV1EIPoZezYh5TXwthOmjSpJ7GHS05rfPrp+5XXYynp16+dnDn7teTkbpBnn31QeT21GTZ0xXF0tJfE4+FyKPYDnp3A1Hrc3V1k509zxCB6mTbt5v47tHXqOMpHHz0vBtHL/gOLpFUr2/vjITVu6L1795a4uDhJSEiQKVOu/TP1Rx55RERE2rRpU9OibCqvvfaYGEQv3bvzazFjmtjb28mKryaJQfTy2Wdjxc5Op7wmc+fuu2+VP498JAbRy7x5w8XBwU55TaZIjRq6VquVxMREadasmdjb20tMTIwEBgaWG+fq6ir//e9/Zc+ePWzopdK4sZdcyF4nGza+prwWxvYze/bTYhC9bN8x86Y5rVGj0cjLLz8s+QWbJCX1S+nR4y7lNZkyNWroHTp0kO+++854f+rUqTJ16tRy4xYuXCh9+/aV3bt3s6GXylcrJ8nFvI3i799AeS3MzZERI3pKYdEW2X/A9k9rbNzYS3748U0xiF42bHxNvLzqKq/J1Kmsd2pRBR8fH6SkpBjvp6amwsfHp8yY4OBg+Pn5Yfv27ZXOa/To0YiKikJUVBS8vb2rWrTV69ixFQYP7ob339uM5OQzqsuhm8Ty5T/iodBZaNGiEfZEvofbb2+iuiSTePjhjjh4aCk6dmyF0aOW4tGBc5Genq26LKWqbOhV0Wg0WLBgAV5++eUqx4aHhyMkJAQhISE4d+5cTRdt0TQaDRYvGYPU1HOYO3e96nLoJvPDD9Ho0nkqdDotfv1tHrp3v1N1SbXGxcUJ4eEvYeOmaUhKOoN7gsfj889/UF2Wxah0876qXS5ubm7yzz//SFJSkiQlJUleXp6kpaVVudvF1ne5DB/+gBhEL089xSvkMeri6+stBw8tlYLCzTJkSDfl9dQ0bdsGSFz8J3KpeKu8/fYzYm9vmwc+K0uN9qHrdDo5fvy4+Pv7Gw+KBgUFXXM896FD3Nyc5dTpr+SXX99VXgvDuLk5G/czz5gxSHk9NxKtVitTpz4qBYWb5cTfy6Vr15v3+vA1Pm2xT58+Eh8fL4mJiTJt2jQBILNmzZJ+/fqVG8uGDpk/f4RcKt4q99zTXHktDAOUnNa4/IsJYhC9fP75OKs6rdHX19t4+eCItVPEw8NFeU0qwx8WmTG33eYjBYWbJTz8JeW1MMzVeeONJ41//LhuXcs/rfGxx+6T8+lrJOvCWnnmme7K67GEsKGbMd98+4ZkZEZIvXruymthmIoybFgPKSjcLNExS8TH5xbl9VQUV9c6xm8Uv++ZL82bN1Jek6WEDd1M6du3rRhELxMnhimvhWEqS8+ewZKZtVb+TvlCWrf2V15P6bRv31ISEj+ToktbZNasp61q95A5woZuhtjb20lc/Cdy5OjHN+WRd8b6cued/vJ3yheSmbVWHnjgbuX16HRaef31J6SwaIv8lbRM7r332idf3MxhQzdDJk9+WAyil96971FeC8NUNz4+t0jMwSVSULhZhg3roayOpk3ry/9+eVcMopeVX78sbm7Oyt8bSw0buonToIGHZGatla3bZiivhWGuN25uzvL9D7PFIHp5440nzb78J5/sKhmZEZKZtZa/26hG2NBNnM+Xj5f8gk3SogUP3DDWGTs7nXy+fLwYRC/Lv5hglt2Gbm7O8tXKkitE/vLru7zeUTXDhm7ChIQEiEH08s47Q5XXwjA1zYwZg8Qgevn+h9km3e3RqVOgHP9rmRQWbZHXX3+CfyfgOsKGbqJoNBr5fc98STu54qa5VClj+3nmme5SULhZYg7W/mmNOp1WZs58SooubZHE4+HSoUNL5a/X2sKGbqIMGdJNDKLnDx4Ym0uPHndJRmaEpKR+KXfe6V8r87z11oby2+/zjbt1rOGHTZYYNnQTxNW1jqSdXCF7It8TjUajvB6Gqe3ccUdTOfH3csnMWis9ewbXaF5DhnSTrAtrJT1jjTz++H3KX5s1hw3dBJk7d6gYRC/t2t2mvBaGMVUaN/aSA9GLpbBoiwwf/sB1P9/Dw0VWr3lFDKKX3T/PFT+/espfk7WHDb2W07x5I8nL3yTLv5igvBaGMXXq1q0j23fMFIPoZdasp6v9vC5d7pDkE8uloHCzTJ36qGi1PPBZG2FDr+Vs2fq6ZF1YKw0beiqvhWHMETs7nYSHvyQG0cuXKyZWelqjnZ1O5swZIpeKt0r8sU+lbdsA5fXbUtjQazG9egWLQfTyyiuPKK+FYcyd6dMfF4Po5cedc8TdvfxlbFu0aCR79y0Qg+glPPwlcXFxUl6zrYUNvZZiZ6eTP498JPHHPhUHB16vhbk5M3hwN8kv2CSHYj8os098xIieciF7nZw7v1oeeaST8jptNWzotZQJE8LEIHoJDQ1RXgvDqEy3bndKesYaSU37Urp1u1PWrZ9q3HK31Evy2krY0Gsh9eq5S0ZmhHy7fabyWhjGEhIU1ESSTywXg+glv2CTTJ78ME/hNUPY0Gshn376ohQUbpaWLX2V18IwlpJGjbxkyZIxEhzMP7dorlTWO+1AVQoObo6Ro3ph0cKtiI9PVV0OkcU4dSod48Z9proMukyrugBrsHjJGPzzTxZmz45QXQoR0TVxC70KgwZ1wX33BWHUyCW4cOGi6nKIiK6JW+iVcHZ2xLz5w/HHHwn44oudqsshIqoUt9ArMXXqo/D19cagJ+ZBRFSXQ0RUKW6hX4O/fwNMfuURrFr1M37//ajqcoiIqsSGfg3z3xuB4mIDpk75UnUpRETVwoZegW7d7sTAgZ0w9+31SEs7r7ocIqJqYUO/ik6nxaLFo/HXX6fx/vubVZdDRFRtPCh6leee64PWrf3xyMNvoaCgSHU5RETVxi30Ury86mLW7Kexc2cMtmyJVF0OEdF1qVZD7927N+Li4pCQkIApU6aUm/7ss8/i0KFDiI6Oxi+//ILAwMBaL9Qc3nxzMNzcnDFhfLjqUoiIbkilF4LRarWSmJgozZo1E3t7e4mJiZHAwMAyY+rWrWu83a9fP9mxY0eNLjCjInfe6S9Fl7bI4sVjlNfCMAxzrVTWO6vcQm/Xrh0SExORlJSEoqIiREREICwsrMyY7Oxs420XFxer/BHOosVjkJGRizfeWKW6FCKiG1LlQVEfHx+kpKQY76empqJ9+/blxr3wwguYNGkSHBwc0L179wrnNXr0aIwZMwYA4O3tfaM117pHH70X99/fGs8/9yEyM3NVl0NEdENq7aDoRx99hBYtWmDKlCl4/fXXKxwTHh6OkJAQhISE4Ny5c7W16BqpU8cR898bgZiYvxAe/oPqcoiIbliVDT0tLQ1+fn7G+76+vkhLS7vm+IiICAwYMKB2qjODV155GE2b1sf4cZ/BYDCoLoeI6IZV2dCjoqIQEBAAf39/2NvbY9CgQdi2bVuZMS1atDDeDg0NRUJCQu1XagJ+fvXw6pRHsXbtL/jllz9Vl0NEVCNV7kMvLi7G2LFj8f3330On02H58uU4cuQIZs2ahT/++AN6vR5jx47FAw88gKKiImRkZGDo0KHmqL3G5s0fDgB49ZUvFFdCRFQ7LO7UG3OkS5c7xCB6+c9/Bik/DYlhGKa6qdFpi7ZIqy25XsuJE2cxfz6v10JEtuGmvJbLqFG9cPfdt+Lxx95BXl6B6nKIiGrFTbeF7uHhgjlvDcHPP8diw4bfVJdDRFRrbrqGPnPmU/D0dMH4cZ+pLoWIqFbdVA09KKgJXngxFJ99+j1iY5NVl0NEVKtuqoa+cNEoXLhwETNmfK26FCKiWnfTHBQNC+uAnj2D8dLYT5Cenl31E4iIrMxNsYXu6GiP9xeMRGxsMj75ZIfqcoiITOKm2EKfNGkAbr21IXp0n47iYl6vhYhsk81voTdu7IXXpj2GjRt/x+7dh1SXQ0RkMjbf0N95dxjs7HR4ZfJy1aUQEZmUTTf0jh1bYfDgbnj/vc1ITj6juhwiIpOy2Yau0WiweMkYpKaew9y561WXQ0RkcjZ7UHT48AfQtm0Ann7qPVy8yOu1EJHts8ktdDc3Z7z19hD8+usRrFnzX9XlEBGZhU1uof/nP4NQr547+vaZpboUIiKzsbkt9JYtffHSuH5Y/vmPiI4+rrocIiKzsbmGvmDhKOTm5mP69JWqSyEiMiub2uUSGhqCPn3aYNLEZfjnnyzV5RARmZXNbKE7ONhhwcJROHo0BR988I3qcoiIzM5mttDHj++PgIDGeLD3f3DpUrHqcoiIzM4mttAbNPDA6zOewLZte/HDD9GqyyEiUsImGvrbc4fCwcEeL0/6XHUpRETKWH1DDwkJwPDhD2DRwq04fvyU6nKIiJSx6oZ+5XotJ0+ex1tvrVNdDhGRUlZ9UHTw4PvRoUMrDH1mAXJy8lSXQ0SklNVuobu61sE77w5DZGQcvv76Z9XlEBEpZ7Vb6NOnP45GjbwwIGwORER1OUREylnlFnrz5o0wYWIYvvhiJ6KiElSXQ0RkEayyob+/YCQKCoow7bUVqkshIrIY1WrovXv3RlxcHBISEjBlypRy0ydOnIg///wTBw8exM6dO9GkSZNaL/SKXr2C0b9/e8x5MwJnzmSabDlERNZIKotWq5XExERp1qyZ2NvbS0xMjAQGBpYZc//990udOnUEgDz33HMSERFR6TwBSFRUVJVjKsoTT3SW3/fMF3t7uxt6PsMwjDWnst5Z5RZ6u3btkJiYiKSkJBQVFSEiIgJhYWFlxvz888/Iyys5bTAyMhK+vr5VzfaGrV37Czp1fAVFRZdMtgwiImtUZUP38fFBSkqK8X5qaip8fHyuOX7kyJHYsWNHhdNGjx6NqKgoREVFwdvb+wbKJSKia6nV0xaffvpptG3bFl27dq1wenh4OMLDwwEAUVFRtbloIqKbXpUNPS0tDX5+fsb7vr6+SEtLKzeuR48emD59Orp27YrCwsLarZKIiKql0h3wOp1Ojh8/Lv7+/saDokFBQWXG3H333ZKYmCgtWrSolR37DMMwTMWp0UHR4uJijB07Ft9//z2OHj2KdevW4ciRI5g1axb69esHAJg/fz5cXV2xfv16REdHY+vWrVXNloiIapkGJZ3d7KKiohASEqJi0UREVquy3mmVvxQlIqLy2NCJiGwEGzoRkY1gQycishFs6LtAp6sAAAw5SURBVERENoINnYjIRrChExHZCDZ0IiIbwYZORGQj2NCJiGwEGzoRkY1gQycishFs6ERENoINnYjIRrChExHZCDZ0IiIbwYZORGQj2NCJiGwEGzoRkY1gQycishFs6ERENoINnYjIRrChExHZCDZ0IiIbwYZORGQj2NCJiGwEGzoRkY1gQycishFs6ERENqJaDb13796Ii4tDQkICpkyZUm56586dsX//fhQVFWHgwIG1XiQREVWtyoau1Wrx4Ycfok+fPggKCsKTTz6JwMDAMmP+/vtvDBs2DKtXrzZZoUREVDm7qga0a9cOiYmJSEpKAgBEREQgLCwMR48eNY45ceIEAMBgMJioTCIiqkqVW+g+Pj5ISUkx3k9NTYWPj88NLWz06NGIiopCVFQUvL29b2geRERUMbMeFA0PD0dISAhCQkJw7tw5cy6aiMjmVdnQ09LS4OfnZ7zv6+uLtLQ0kxZFRETXr8qGHhUVhYCAAPj7+8Pe3h6DBg3Ctm3bzFEbERFdJ6kqffr0kfj4eElMTJRp06YJAJk1a5b069dPAEjbtm0lJSVFcnJy5Ny5c3L48OEq5xkVFVXlGIZhGKZsKuudmss3zC4qKgohISEqFk1EZLUq6538pSgRkY1gQycishFs6ERENoINnYjIRrChExHZCDZ0IiIbwYZORGQj2NCJiGwEGzoRkY2o8nroluaeh3rjvkGPlntcKvrB61UPiVQ0pvxjFY0z9fwrnn25BVRzXlLurogBYhCIwQARAwxXbl+JCAyGy2PEACk2wFDmOVfGl0w3FBuumue/8yszn8u3/51Xccn9G52nodSYy/8aiovLzKtM3VeeW2peV24bDIYyy7/yHENxMS5eyIbw+v5kZayuoV8qLEJedk6ZxzSaCgZe9aCmokEVPKZBReMqeqrm6gcqHlNuXPnZVbTMas+/WrVqodFooNFqSm5rNdDqdJcf0/777+XpWp227OMaDbTay4+VHlfq9pV52gqDwYC8rAvIychETkYGctMzkZORidyMTOSkZ5T8e/l2TnoGcjMzYbhUrLpsuslZXUM/9MMuHPphl+oy6BqqvZIwrghKrRi02n9XDlfGXl7xXL3y+Pd5pef5723tVSsf43OuzNO4Uio7XaPVQmdnBxcPd7h4esDVyxMunh6o36wpbm1zN5w93KHVVryn8uKFC6UafwZySq8EjCuFf1cOlwoLzfxfh2yd1TV0smwiAim23S1VjVYLZ3c3uHp6wMXLs+Tfy43feNvTE16+PmjS+na4eHhAZ1/xxyw/N/fyFv/VTb9khZCdXmolkJGJwrx8M79asjZs6ETXQQwG5F7e6sZfydV6Th23usZG7+r1b9N38fKA6+WVgXv9emjcKgCunh6wc3CocD6FefnG5p6TkVmm2ZesFDIu7wYquZ2fk1uLr5ysARs6kYnlXchG3oVsnDuRUvVgAI4uzqUa/uUt/8u3S74NlKwUGjTzh4unBxyd61Q4n0uFhcjNyCrV9K/sBqp4JZCfnV3xQXayGmzoRBamIPciCnIv4nxq9f7Uo72T41XNvmQl4HqL57+Pe3rAy7cxXD094eTqUuF8ii9dQm5mVrkDv7npl5t+qdu5GZnIzczimUAWhg2dyMoV5Rcg49RpZJw6Xa3xdg4OcPF0r3AlUPpbQeOWAXD18oSzu1uF8zEYDLiYmYXczCzkZ+cgPze3ZGV08SLyc0puX3nMOC338rRSYwrz8mrz7bipsaET3WQuFRYi68w/yDrzT7XGa+10cHF3Nx4EdvXyLLsS8PSAk4szHF1c4FbPG44uznBydYGjszN0dlW3GENxMQou5pVp8gW5uci//G/JCqHU7atWCFevHG7m3UZs6ERUKcOlYmSfT0f2+fTrfq69kyMcXZzh6OwCJ9eSpu/k4lLS9C//W/r2lRWBk6sL6nrfYrzt6FLNlYPBgMKLef9+M7jc/CtcERhXGmVXIPm5F1GQk2uVKwc2dCIymaL8AhTlFyDnfEaN52Xn6Gj8JuB0eUVgvO3qAifnkn8dXZzL3nZxgauXZ5kVybVOJb1afm4uCnIq3o1U4a6lMiuNf28XXjTPyoENnYiswqWCAuQUFCAnvRZWDg4OFX4zuPpbQkXfJG7xbFxmRWJnb1+tZZY+jvD9R8sQ893OGr+Ocq+r1udIRGThLhUWXj6tM7PG89LZ2xube2UrgtL/XszKqoVXUR4bOhFRDRQXFZWc7plpmiZ9PXj5XCIiG8GGTkRkI9jQiYhsBBs6EZGNYEMnIrIRbOhERDaCDZ2IyEawoRMR2QgNKvx786Z39uxZnDhx4oae6+3tjXPnztVyRTXHuq4P67p+llob67o+NamradOmqF+//jWni7UlKipKeQ2si3WxNtZlaXVxlwsRkY1gQycishE6ADNVF3EjDhw4oLqECrGu68O6rp+l1sa6ro8p6lJ2UJSIiGoXd7kQEdkINnQiIhth0Q29d+/eiIuLQ0JCAqZMmVJuuoODAyIiIpCQkIDIyEg0bdrUIuoaOnQozp49i+joaERHR2PkyJFmqevzzz/HmTNnEBsbe80xixcvRkJCAg4ePIjg4GCLqKtr167IzMw0vl8zZswweU2+vr7YtWsX/vzzTxw+fBjjxo2rcJy536/q1KXi/XJ0dMTevXsRExODw4cPY+bMmeXGqPg8VqcuVZ9HANBqtThw4AD0en25aaZ6v5Sfk1lRtFqtJCYmSrNmzcTe3l5iYmIkMDCwzJjnn39ePv74YwEgTzzxhERERFhEXUOHDpWlS5ea/T3r3LmzBAcHS2xsbIXT+/TpI9u3bxcA0r59e4mMjLSIurp27Sp6vd6s71XDhg0lODhYAIirq6vEx8eX+++o4v2qTl0q3i8A4uLiIgDEzs5OIiMjpX379mWmq/g8VqcuVZ9HADJx4kRZtWpVhf+9TPF+WewWert27ZCYmIikpCQUFRUhIiICYWFhZcaEhYVhxYoVAIANGzagR48eFlGXKr/88gvS09OvOT0sLAxfffUVAGDv3r3w8PBAw4YNldelwunTpxEdHQ0AyMnJwdGjR+Hj41NmjIr3qzp1qZKbmwsAsLe3h729fbm/Yq/i81idulTx8fFBaGgoli1bVuF0U7xfFtvQfXx8kJKSYryfmppa7n/s0mOKi4uRlZWFW265RXldADBw4EAcPHgQ69evh6+vr0lrqq7q1q5Cx44dERMTg+3btyMoKMisy27atCmCg4Oxd+/eMo+rfr+uVReg5v3SarWIjo7G2bNn8eOPP2Lfvn1lpqv4PFanLkDN53HRokV49dVXYTAYKpxuivfLYhu6NdPr9fD398ddd92FH3/80bgWpoodOHAATZs2xd13342lS5diy5YtZlu2i4sLNm7ciAkTJiA7O9tsy61KZXWper8MBgOCg4Ph6+uLdu3a4fbbbzfLcqtSVV0qPo+hoaE4e/as2c+Bt9iGnpaWBj8/P+N9X19fpKWlXXOMTqeDu7s7zp8/r7yu9PR0FBYWAgCWLVuGNm3amLSm6qpO7SpkZ2cbvzbv2LED9vb2Ztmys7Ozw8aNG7Fq1Sps3ry53HRV71dVdal6v67IysrC7t278eCDD5Z5XMXnsTp1qfg83nvvvejfvz+SkpIQERGB7t27Y+XKlWXGmOr9UnKwoKrodDo5fvy4+Pv7Gw8+BgUFlRnzwgsvlDmosHbtWouoq2HDhsbbAwYMkD179pjtfWvatOk1Dz727du3zEG+vXv3WkRdDRo0MN4OCQmREydOmKWmFStWyMKFC685XdX7VVVdKt4vb29vcXd3FwDi5OQk//vf/yQ0NLTMGBWfx+rUpfLzCFz7ILaJ3i/zvbDrTZ8+fSQ+Pl4SExNl2rRpAkBmzZol/fr1EwDi6Ogo69atk4SEBNm7d680a9bMIup6++235fDhwxITEyO7du2Sli1bmqWu1atXy8mTJ6WwsFBSUlJkxIgR8uyzz8qzzz5rHPPBBx9IYmKiHDp0SNq0aWMRdb344ovG92vPnj3SsWNHk9d07733iojIwYMHJTo6WqKjo6VPnz7K36/q1KXi/WrdurUcOHBADh48KLGxsTJjxgwB1H8eq1OXqs/jlZRu6KZ+v/jTfyIiG2Gx+9CJiOj6sKETEdkINnQiIhvBhk5EZCPY0ImIbAQbOhGRjWBDJyKyEf8H3W0jQYJyCkEAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#yhat = model.predict(train_X)\n",
        "#test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzIiUGAMsxBi",
        "outputId": "4ba14082-a132-4071-cfca-02f2815713f8"
      },
      "id": "nzIiUGAMsxBi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(744, 8653)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#yh     = yhat.reshape((yhat.shape[0], yhat.shape[2]))\n",
        "#inv_yhat = np.concatenate((scaled[:n_train_hours, :-1],yh), axis=1)"
      ],
      "metadata": {
        "id": "SKaz31_EFS7B"
      },
      "id": "SKaz31_EFS7B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pred = scaler.inverse_transform(inv_yhat)"
      ],
      "metadata": {
        "id": "iFlHcHA7tOnb"
      },
      "id": "iFlHcHA7tOnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#yhat2 = model.predict(test_X)\n",
        "#yh2     = yhat2.reshape((yhat2.shape[0], yhat2.shape[2]))\n",
        "#inv_yhat2 = np.concatenate((scaled[n_train_hours:(n_train_hours+n_test_hours), :-1],yh2), axis=1)"
      ],
      "metadata": {
        "id": "QVN9xyAgdHdv"
      },
      "id": "QVN9xyAgdHdv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pred2 = scaler.inverse_transform(inv_yhat2)"
      ],
      "metadata": {
        "id": "qTx8SyyghfNi"
      },
      "id": "qTx8SyyghfNi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.concat([data.drop(['day','hour','month','year'],1),pd.get_dummies(data[['day','hour','month','year']])],1)\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = pd.DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = pd.concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg"
      ],
      "metadata": {
        "id": "1kLCQj6CL5Tq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f73f64aa-5354-42c0-e95d-95f2c71c1177"
      },
      "id": "1kLCQj6CL5Tq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "#    keras.layers.Normalization(),\n",
        "    keras.layers.Dense(128,'relu'),\n",
        "    keras.layers.Dense(1024,'relu'),\n",
        "    keras.layers.Dense(1024,'relu'),\n",
        "    keras.layers.Dense(128,'relu'),\n",
        "    keras.layers.Dense(1,'linear')\n",
        "])\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001,amsgrad=True)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='mse')\n",
        "#X = data2.drop('p',1)\n",
        "#y = data2['p']\n",
        "X = data2.drop('Price,Germany',1)\n",
        "y = data2['Price,Germany']\n",
        "X = np.asarray(X).astype(np.float32)\n",
        "y = np.asarray(y).astype(np.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnz8t7nAEU5J",
        "outputId": "85caa145-6882-4508-c6b7-1c9f55c4ca9d"
      },
      "id": "wnz8t7nAEU5J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X,train_y=X[:mid,:],y[:mid]\n",
        "test_X,test_y=X[mid:time,:],y[mid:time]\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,mode='auto')"
      ],
      "metadata": {
        "id": "27Cj_u2KWFvp"
      },
      "id": "27Cj_u2KWFvp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_X, train_y, epochs=200, batch_size=32, verbose=1,validation_data=(test_X,test_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "OFv-DryPEjqL",
        "outputId": "71fda491-88f5-4655-938c-03902f6fb3f3"
      },
      "id": "OFv-DryPEjqL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "275/275 [==============================] - 6s 20ms/step - loss: 142681.5156 - val_loss: 82.3452\n",
            "Epoch 2/5\n",
            " 62/275 [=====>........................] - ETA: 3s - loss: 124.9149"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-9f8f1c0332dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history['val_loss'][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xc_2dn9_aLU",
        "outputId": "fe9f67d3-9b24-4038-f278-5edf1d951cb9"
      },
      "id": "0Xc_2dn9_aLU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93.6063232421875"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model.predict(train_X)\n",
        "yhat2 = model.predict(test_X)"
      ],
      "metadata": {
        "id": "XNOtCWPxJGR1"
      },
      "id": "XNOtCWPxJGR1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Variable selection"
      ],
      "metadata": {
        "id": "lqTlMs7j1oKt"
      },
      "id": "lqTlMs7j1oKt"
    },
    {
      "cell_type": "code",
      "source": [
        "def combinations(l):\n",
        "    res = [[]]\n",
        "    for i in l:\n",
        "      res = res + [c + [i] for c in res]\n",
        "    return res"
      ],
      "metadata": {
        "id": "b0XEM8vN7w3y"
      },
      "id": "b0XEM8vN7w3y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variables = data.drop(['day','hour','month','year','Price,Germany'],1).columns\n",
        "conventional = list(variables[6:])\n",
        "var_sel   = combinations(conventional)\n",
        "len(var_sel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7sNLsw19PO1",
        "outputId": "9ad3ba6c-5f44-450b-c579-816e6e521cd5"
      },
      "id": "f7sNLsw19PO1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4096"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss = {}\n",
        "p = data2['Price,Germany']\n",
        "data2 = data2.drop('Price,Germany',1)\n",
        "for i in np.arange(len(var_sel)):\n",
        "  model = keras.Sequential([\n",
        "                            keras.layers.Dense(128,'relu'),\n",
        "                            keras.layers.Dense(1024,'relu'),\n",
        "                            keras.layers.Dense(1024,'relu'),\n",
        "                            keras.layers.Dense(128,'relu'),\n",
        "                            keras.layers.Dense(1,'linear')\n",
        "                            ])\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.001,amsgrad=True)\n",
        "  model.compile(optimizer=opt,loss='mse')\n",
        "  X = data2.drop(var_sel[i],1)\n",
        "  y = p\n",
        "  X = np.asarray(X).astype(np.float32)\n",
        "  y = np.asarray(y).astype(np.float32)\n",
        "  train_X,train_y=X[:mid,:],y[:mid]\n",
        "  test_X,test_y=X[mid:time,:],y[mid:time]\n",
        "  history = model.fit(train_X, train_y, epochs=200, batch_size=32, verbose=1,validation_data=(test_X,test_y))\n",
        "  val_loss[i] = np.mean(history.history['val_loss'][-10:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSQ3HJM_-SkS",
        "outputId": "1a877def-cacb-44ca-e770-b36f75a80890"
      },
      "id": "YSQ3HJM_-SkS",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.7931 - val_loss: 180.7630\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.2909 - val_loss: 255.4652\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 65.2791 - val_loss: 230.8767\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.5611 - val_loss: 207.8014\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 95.2156 - val_loss: 174.6569\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.5026 - val_loss: 225.7670\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 122.3269 - val_loss: 175.1073\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.8422 - val_loss: 184.5586\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 135.8804 - val_loss: 203.8215\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.0913 - val_loss: 206.5880\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.9207 - val_loss: 182.1994\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 82.0593 - val_loss: 181.2763\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 72.7841 - val_loss: 183.9704\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 66.5014 - val_loss: 184.5036\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.0877 - val_loss: 155.6618\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 73.0219 - val_loss: 189.4118\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 99.5581 - val_loss: 167.0367\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 92.4974 - val_loss: 240.1062\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.0654 - val_loss: 288.5696\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.6728 - val_loss: 177.4090\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 48.0605 - val_loss: 159.9822\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.9359 - val_loss: 150.3660\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.9733 - val_loss: 275.9565\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 77.6181 - val_loss: 381.5598\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 89.1626 - val_loss: 220.5923\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.6105 - val_loss: 190.9831\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 64.1000 - val_loss: 176.4917\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.2304 - val_loss: 226.8425\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.1957 - val_loss: 170.8057\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 84.2964 - val_loss: 229.6077\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 127.9769 - val_loss: 263.4427\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.4392 - val_loss: 229.9645\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 53.3419 - val_loss: 167.8574\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 82.1322 - val_loss: 223.4131\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 77.4578 - val_loss: 177.7472\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.1304 - val_loss: 163.6336\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.0187 - val_loss: 232.8097\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 60.0199 - val_loss: 178.1430\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.1132 - val_loss: 169.3025\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 58.3667 - val_loss: 178.6171\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.2483 - val_loss: 189.5635\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 58.2741 - val_loss: 184.5859\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.6413 - val_loss: 181.2685\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.4995 - val_loss: 197.2563\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.3269 - val_loss: 254.1052\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.6599 - val_loss: 195.6429\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.1143 - val_loss: 214.4185\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 94.6194 - val_loss: 208.3778\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 53.0255 - val_loss: 182.7901\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 51.6670 - val_loss: 171.1345\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 60.0624 - val_loss: 262.1129\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 83.6210 - val_loss: 373.3657\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 67.2249 - val_loss: 170.5175\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.1140 - val_loss: 215.9265\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.8600 - val_loss: 147.8700\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 87.1960 - val_loss: 126.1752\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 59.1100 - val_loss: 160.7767\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 78.8736 - val_loss: 299.5767\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 76.9458 - val_loss: 262.8079\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.0369 - val_loss: 162.9018\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.8411 - val_loss: 147.7712\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.5396 - val_loss: 216.6545\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 50.5026 - val_loss: 170.5438\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.1830 - val_loss: 239.0150\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.1120 - val_loss: 180.9458\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.3072 - val_loss: 168.3181\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 42.3845 - val_loss: 143.9114\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.2181 - val_loss: 150.8145\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.0598 - val_loss: 165.1330\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 46.5614 - val_loss: 144.5272\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 43.7661 - val_loss: 133.5459\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.8708 - val_loss: 227.6765\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 76.5342 - val_loss: 166.8422\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 56.5755 - val_loss: 147.5025\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 73.1495 - val_loss: 173.4191\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 49.8910 - val_loss: 147.9432\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 42.8261 - val_loss: 145.3008\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 39.7667 - val_loss: 138.4262\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 40.4530 - val_loss: 127.7203\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 50.2941 - val_loss: 159.3676\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 39.9842 - val_loss: 120.4561\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.9719 - val_loss: 199.9270\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.2624 - val_loss: 113.1672\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 121.0557 - val_loss: 165.9441\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.9229 - val_loss: 136.0660\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.0359 - val_loss: 154.3694\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 39.6754 - val_loss: 151.7694\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 49.1637 - val_loss: 113.3591\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.1821 - val_loss: 118.6813\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.4945 - val_loss: 165.7872\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.4730 - val_loss: 172.3628\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.2108 - val_loss: 199.1356\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.9338 - val_loss: 165.8229\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.9509 - val_loss: 181.5703\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 39.1635 - val_loss: 115.7683\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.9310 - val_loss: 153.1538\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.7820 - val_loss: 126.8296\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 38.5980 - val_loss: 135.5928\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.2736 - val_loss: 228.0068\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 46.5068 - val_loss: 106.3590\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 1s 8ms/step - loss: 895746.4375 - val_loss: 562.5767\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 952.6620 - val_loss: 167.0381\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 130.2037 - val_loss: 72.6236\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.1210 - val_loss: 67.8400\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 86.2463 - val_loss: 68.7820\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 83.8402 - val_loss: 69.0394\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 82.7504 - val_loss: 70.9099\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 94.8017 - val_loss: 82.7525\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 98.3498 - val_loss: 119.7180\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 74.3634 - val_loss: 125.3344\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.2004 - val_loss: 121.1337\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.2984 - val_loss: 97.4332\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 102.0155 - val_loss: 165.8049\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.2737 - val_loss: 66.2030\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 67.2990 - val_loss: 88.5528\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 79.9493 - val_loss: 108.0403\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.4869 - val_loss: 104.0070\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.8535 - val_loss: 72.9553\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.0109 - val_loss: 182.6724\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.7775 - val_loss: 84.7934\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 70.3093 - val_loss: 75.0405\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 65.1392 - val_loss: 80.4925\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.6173 - val_loss: 85.8207\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.2447 - val_loss: 65.4632\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.7333 - val_loss: 91.6391\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 52.4139 - val_loss: 79.6284\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.1297 - val_loss: 75.5056\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.1360 - val_loss: 73.7043\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.3038 - val_loss: 86.1404\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.8886 - val_loss: 125.6538\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.1017 - val_loss: 67.4906\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.9367 - val_loss: 87.2447\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 60.7785 - val_loss: 90.5757\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 92.0698 - val_loss: 281.3680\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 144.9507 - val_loss: 78.4481\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.6682 - val_loss: 87.4456\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.4880 - val_loss: 72.9458\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 62.0436 - val_loss: 108.6698\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 60.8275 - val_loss: 107.5688\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 86.7430 - val_loss: 86.1276\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 156.8890 - val_loss: 160.5286\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 87.5541 - val_loss: 118.4127\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.9525 - val_loss: 69.3114\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 67.1827 - val_loss: 147.7112\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 73.3499 - val_loss: 64.3316\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 76.3342 - val_loss: 119.9719\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.7547 - val_loss: 92.5375\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 52.7921 - val_loss: 69.9126\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.1811 - val_loss: 177.5777\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.0492 - val_loss: 65.6844\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.4317 - val_loss: 67.1373\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.8066 - val_loss: 73.9529\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.1550 - val_loss: 65.3974\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 43.9758 - val_loss: 77.7241\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.4907 - val_loss: 108.6741\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.1111 - val_loss: 65.9737\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.5298 - val_loss: 143.1724\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 49.5815 - val_loss: 97.4346\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.1125 - val_loss: 70.2167\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.0320 - val_loss: 91.6352\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 49.8299 - val_loss: 70.0183\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.3473 - val_loss: 75.7050\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.0971 - val_loss: 119.5908\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.9565 - val_loss: 76.7519\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 43.0047 - val_loss: 75.0620\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.7095 - val_loss: 73.1995\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 87.2135 - val_loss: 95.8017\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 82.5613 - val_loss: 95.3819\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.0370 - val_loss: 101.3362\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.0613 - val_loss: 118.3999\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.4386 - val_loss: 111.8468\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 53.5301 - val_loss: 80.0562\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.2131 - val_loss: 137.1383\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 53.3002 - val_loss: 75.8378\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.9717 - val_loss: 84.3554\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 41.9588 - val_loss: 73.2106\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.7104 - val_loss: 120.2707\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.5763 - val_loss: 79.8348\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 55.9746 - val_loss: 87.3169\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 79.4826 - val_loss: 114.0107\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.0807 - val_loss: 88.6220\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.5261 - val_loss: 80.6154\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.4986 - val_loss: 77.5932\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.5654 - val_loss: 79.6287\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.1545 - val_loss: 86.3270\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.9343 - val_loss: 81.1257\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 72.1147 - val_loss: 72.9031\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 62.2959 - val_loss: 91.1453\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.4275 - val_loss: 79.2120\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.6859 - val_loss: 72.6258\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 50.5992 - val_loss: 74.1535\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.8307 - val_loss: 112.7320\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.9765 - val_loss: 148.3253\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 66.4576 - val_loss: 110.1210\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 88.1291 - val_loss: 123.3954\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 44.0154 - val_loss: 117.4447\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.8268 - val_loss: 114.0141\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.6677 - val_loss: 77.2700\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.5208 - val_loss: 82.9918\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 62.5254 - val_loss: 94.5412\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 76.4363 - val_loss: 75.4993\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.3829 - val_loss: 122.2475\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.7095 - val_loss: 76.2860\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 49.8055 - val_loss: 69.9374\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 75.6340 - val_loss: 77.9407\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 39.0618 - val_loss: 167.7702\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 56.9884 - val_loss: 83.2838\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.1243 - val_loss: 72.0542\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.5138 - val_loss: 96.9589\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.8845 - val_loss: 72.9960\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.1775 - val_loss: 79.3244\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.1640 - val_loss: 78.4437\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.9192 - val_loss: 73.4164\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 42.1359 - val_loss: 71.2255\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.0437 - val_loss: 81.5728\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 52.6723 - val_loss: 81.4979\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.6007 - val_loss: 76.2423\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 50.9592 - val_loss: 148.3322\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.5121 - val_loss: 107.7614\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 46.2890 - val_loss: 114.3424\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 37.4351 - val_loss: 65.6371\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 36.9414 - val_loss: 105.0250\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 52.7384 - val_loss: 72.2296\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.3706 - val_loss: 68.5830\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.6767 - val_loss: 200.5532\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.2543 - val_loss: 80.7771\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 35.7881 - val_loss: 115.0973\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.3741 - val_loss: 68.5687\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 38.1833 - val_loss: 94.6526\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.7886 - val_loss: 77.0375\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 86.0481 - val_loss: 99.9767\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 34.6600 - val_loss: 70.9656\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.9135 - val_loss: 65.6640\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 36.7871 - val_loss: 76.3058\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.2672 - val_loss: 75.4184\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.7423 - val_loss: 116.2827\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.3998 - val_loss: 71.4955\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 41.4589 - val_loss: 79.7515\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 39.6916 - val_loss: 109.2917\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.0469 - val_loss: 89.3168\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.4078 - val_loss: 103.6179\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 42.2281 - val_loss: 68.8147\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.2166 - val_loss: 118.1952\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.4378 - val_loss: 89.5768\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 29.0054 - val_loss: 78.6328\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 36.4744 - val_loss: 67.3787\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.4062 - val_loss: 92.1392\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 35.6474 - val_loss: 115.5335\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.9695 - val_loss: 69.4532\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 53.7842 - val_loss: 101.6401\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 28.0160 - val_loss: 81.3192\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 38.7559 - val_loss: 67.7978\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.3460 - val_loss: 85.8239\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.5980 - val_loss: 73.3561\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 29.2097 - val_loss: 66.4645\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 31.4751 - val_loss: 71.0455\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 31.1027 - val_loss: 80.5026\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.3863 - val_loss: 74.2485\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 56.6594 - val_loss: 192.5857\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.8211 - val_loss: 65.9993\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 32.2140 - val_loss: 80.4578\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.3263 - val_loss: 171.7826\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.5906 - val_loss: 99.2283\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 35.3073 - val_loss: 73.2714\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 37.9884 - val_loss: 72.5018\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 35.1689 - val_loss: 68.0906\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 30.3323 - val_loss: 62.4120\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 29.2349 - val_loss: 131.1285\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 34.5626 - val_loss: 71.3487\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 27.2731 - val_loss: 69.6044\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 27.9622 - val_loss: 68.0256\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.8377 - val_loss: 68.9426\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.0855 - val_loss: 92.4911\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 30.9234 - val_loss: 81.3314\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 48.3329 - val_loss: 102.2327\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 36.5964 - val_loss: 68.0993\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 37.7542 - val_loss: 76.2701\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 27.8869 - val_loss: 108.3480\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 26.9902 - val_loss: 84.1380\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 32.9034 - val_loss: 70.9109\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.3484 - val_loss: 261.1347\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 67.9606 - val_loss: 89.7060\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 37.5681 - val_loss: 82.3991\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 30.3482 - val_loss: 67.9392\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 33.5173 - val_loss: 73.5885\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.6039 - val_loss: 77.8165\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 26.6653 - val_loss: 90.8102\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 30.4131 - val_loss: 70.8472\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 33.1539 - val_loss: 89.7146\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.9081 - val_loss: 71.6147\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 41.6796 - val_loss: 158.9700\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.4024 - val_loss: 63.4384\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.9879 - val_loss: 176.7845\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.9159 - val_loss: 66.4726\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 31.3248 - val_loss: 74.7311\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 29.4475 - val_loss: 87.3082\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 28.3168 - val_loss: 132.9614\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 30.7882 - val_loss: 99.8337\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 34.7203 - val_loss: 68.7380\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 24.6660 - val_loss: 73.4695\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 1s 8ms/step - loss: 627528.0625 - val_loss: 223.8171\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 489.8097 - val_loss: 241.9393\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 122.1859 - val_loss: 150.5887\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 111.2709 - val_loss: 161.4489\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 106.5152 - val_loss: 109.1295\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 89.2422 - val_loss: 74.6530\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 92.5032 - val_loss: 71.0746\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 83.3165 - val_loss: 74.0586\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 129.3524 - val_loss: 223.2735\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 89.4378 - val_loss: 72.4835\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.1530 - val_loss: 80.7461\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 86.1088 - val_loss: 95.1464\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.8298 - val_loss: 106.3852\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 93.3215 - val_loss: 80.1696\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 110.7600 - val_loss: 110.3718\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 117.0799 - val_loss: 108.1536\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 109.7763 - val_loss: 423.8361\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 97.1073 - val_loss: 81.6594\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 83.6977 - val_loss: 72.5677\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 76.5868 - val_loss: 114.8743\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 89.9636 - val_loss: 92.8597\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 113.2147 - val_loss: 91.5205\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 83.6583 - val_loss: 85.6415\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.5171 - val_loss: 168.3854\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 70.0184 - val_loss: 73.2982\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 93.4904 - val_loss: 67.2579\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.7841 - val_loss: 96.7790\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.0805 - val_loss: 190.8806\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 82.5325 - val_loss: 73.7410\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 92.7686 - val_loss: 75.8519\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 82.2600 - val_loss: 170.6033\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.9980 - val_loss: 66.5152\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 66.5828 - val_loss: 102.7042\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 69.4923 - val_loss: 180.3976\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 74.6309 - val_loss: 73.2290\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 78.9422 - val_loss: 62.2973\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 100.3339 - val_loss: 248.5319\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 106.1499 - val_loss: 66.1153\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 65.6206 - val_loss: 146.1049\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 62.9790 - val_loss: 68.7511\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 64.7375 - val_loss: 67.1363\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 69.7227 - val_loss: 59.9266\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 97.8720 - val_loss: 63.7465\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 82.5347 - val_loss: 116.8179\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 88.8676 - val_loss: 115.6562\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 73.1003 - val_loss: 169.8968\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 140.6204 - val_loss: 64.4667\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 343.5297 - val_loss: 418.6659\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 149.1062 - val_loss: 87.0657\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 79.7288 - val_loss: 59.5011\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 71.7861 - val_loss: 119.3229\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.1050 - val_loss: 70.9156\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 71.1501 - val_loss: 57.9931\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.9772 - val_loss: 64.4824\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 81.4478 - val_loss: 59.4465\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 62.3128 - val_loss: 90.4262\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 56.4921 - val_loss: 57.3545\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 73.3170 - val_loss: 73.0174\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 72.5488 - val_loss: 90.0980\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.9499 - val_loss: 206.3089\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 77.9456 - val_loss: 147.8663\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 72.5065 - val_loss: 77.8227\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 60.3934 - val_loss: 134.1172\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 62.5708 - val_loss: 72.1233\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.9605 - val_loss: 59.6265\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 70.2679 - val_loss: 115.4556\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.6175 - val_loss: 55.9862\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 66.7383 - val_loss: 81.4058\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.5770 - val_loss: 59.2136\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 65.4330 - val_loss: 103.3444\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 72.2032 - val_loss: 255.6334\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 143.8977 - val_loss: 148.5639\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 66.7515 - val_loss: 153.4888\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.8409 - val_loss: 171.7425\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 67.0817 - val_loss: 112.0487\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.4668 - val_loss: 69.6167\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.9787 - val_loss: 102.3995\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 65.0777 - val_loss: 66.7568\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 56.5303 - val_loss: 63.2156\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 75.7102 - val_loss: 96.8054\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 62.9070 - val_loss: 72.3057\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 86.3715 - val_loss: 57.5883\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.6235 - val_loss: 62.6744\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.5569 - val_loss: 65.1142\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 62.2548 - val_loss: 148.9391\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.1036 - val_loss: 59.9695\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 59.7078 - val_loss: 98.1370\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 67.0958 - val_loss: 117.1929\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 68.7339 - val_loss: 59.5449\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 70.9223 - val_loss: 57.7901\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 62.0885 - val_loss: 90.9165\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 58.5250 - val_loss: 85.6933\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.2044 - val_loss: 77.9114\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.8756 - val_loss: 115.7104\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.7951 - val_loss: 108.8477\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 80.0123 - val_loss: 206.0906\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 89.5223 - val_loss: 57.0629\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.1741 - val_loss: 54.3086\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 70.9102 - val_loss: 67.1502\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 67.0501 - val_loss: 81.3897\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 86.4653 - val_loss: 187.2924\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 73.6273 - val_loss: 94.4732\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.7193 - val_loss: 64.0317\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 71.7580 - val_loss: 57.6906\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 67.3682 - val_loss: 91.2881\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 65.8125 - val_loss: 102.9315\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.6238 - val_loss: 94.8596\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 58.0241 - val_loss: 76.3461\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 53.3498 - val_loss: 58.3578\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.8395 - val_loss: 65.4847\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.6822 - val_loss: 57.3884\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.3712 - val_loss: 76.0883\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 69.6088 - val_loss: 74.9612\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.1087 - val_loss: 74.5450\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 49.5603 - val_loss: 67.6411\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.3670 - val_loss: 60.1312\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.5679 - val_loss: 91.0780\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 59.5964 - val_loss: 149.6803\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 68.4286 - val_loss: 240.0685\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.9395 - val_loss: 65.6534\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 49.6923 - val_loss: 128.5004\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 69.3400 - val_loss: 69.1390\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 75.1264 - val_loss: 56.5817\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 75.0759 - val_loss: 84.2511\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.8463 - val_loss: 82.3085\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.9700 - val_loss: 67.9670\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.7280 - val_loss: 69.3285\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.7949 - val_loss: 63.8801\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 105.6932 - val_loss: 67.4279\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.4085 - val_loss: 63.5744\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 62.1891 - val_loss: 122.4342\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.4684 - val_loss: 57.5947\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.0498 - val_loss: 84.5435\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.6415 - val_loss: 60.5701\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.7334 - val_loss: 84.8459\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.3057 - val_loss: 58.9316\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.7930 - val_loss: 78.9540\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.7322 - val_loss: 63.3040\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 61.0937 - val_loss: 88.4411\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.7114 - val_loss: 67.0008\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.6695 - val_loss: 129.6995\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 59.7332 - val_loss: 75.4228\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.8253 - val_loss: 73.9657\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.3694 - val_loss: 107.9153\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.6230 - val_loss: 81.6566\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.2285 - val_loss: 74.4373\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.0972 - val_loss: 61.7230\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 49.1026 - val_loss: 139.5131\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 54.8868 - val_loss: 62.2996\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.1402 - val_loss: 61.5516\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.8463 - val_loss: 58.7394\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 59.0394 - val_loss: 54.2209\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 56.2773 - val_loss: 62.9291\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 100.5234 - val_loss: 122.8507\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 69.6541 - val_loss: 169.4099\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.1684 - val_loss: 69.6996\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 51.3297 - val_loss: 69.9372\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.5518 - val_loss: 60.3812\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 60.6823 - val_loss: 84.9814\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.0470 - val_loss: 77.3339\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 42.7726 - val_loss: 63.0090\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.4625 - val_loss: 71.4627\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 70.8811 - val_loss: 55.4763\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.2364 - val_loss: 86.8352\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 52.7716 - val_loss: 77.4557\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.3855 - val_loss: 68.6551\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.9745 - val_loss: 264.3884\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.9121 - val_loss: 68.2767\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 72.2976 - val_loss: 104.1520\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.9463 - val_loss: 149.6190\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 53.3162 - val_loss: 68.2465\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 44.7288 - val_loss: 77.5513\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 52.0837 - val_loss: 64.1070\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.1231 - val_loss: 60.2280\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.4793 - val_loss: 68.9453\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.8564 - val_loss: 103.6178\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.6497 - val_loss: 165.0911\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.9035 - val_loss: 65.9622\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 41.8770 - val_loss: 94.7675\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 58.0927 - val_loss: 151.0150\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.5646 - val_loss: 60.5371\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 60.7288 - val_loss: 67.4871\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.9439 - val_loss: 123.4616\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 75.7396 - val_loss: 127.7588\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.6021 - val_loss: 65.7876\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.8046 - val_loss: 86.4525\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.0245 - val_loss: 60.8652\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.0981 - val_loss: 111.5518\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.3559 - val_loss: 58.2138\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.2439 - val_loss: 119.1954\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 42.5303 - val_loss: 86.6347\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 42.9225 - val_loss: 59.3058\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.4209 - val_loss: 77.5949\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 43.0202 - val_loss: 122.6691\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 45.6816 - val_loss: 150.3894\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.1919 - val_loss: 115.1338\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 40.5693 - val_loss: 104.3188\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.7986 - val_loss: 72.8376\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.7515 - val_loss: 94.6389\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.8286 - val_loss: 61.8998\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 1s 9ms/step - loss: 613281.1250 - val_loss: 696.2855\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 552.1946 - val_loss: 340.6549\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 112.2174 - val_loss: 166.1385\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 103.8094 - val_loss: 70.6660\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 87.0654 - val_loss: 166.8102\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 86.7834 - val_loss: 96.1160\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 75.7738 - val_loss: 106.3184\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 75.9570 - val_loss: 75.3442\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 82.4058 - val_loss: 107.9896\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 140.3246 - val_loss: 93.5745\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 80.1431 - val_loss: 59.3381\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 92.8154 - val_loss: 84.7404\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.2304 - val_loss: 278.5287\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.5660 - val_loss: 84.2096\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.7202 - val_loss: 93.3373\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 72.2332 - val_loss: 118.4283\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 71.7621 - val_loss: 107.9950\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.2576 - val_loss: 88.3248\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 70.6540 - val_loss: 73.5347\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 129.4712 - val_loss: 229.8744\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 187.9019 - val_loss: 81.7061\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 97.4663 - val_loss: 63.3649\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 124.9961 - val_loss: 101.7487\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.7421 - val_loss: 119.9764\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 124.8026 - val_loss: 60.6745\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 79.4819 - val_loss: 77.6830\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.7300 - val_loss: 70.6283\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.7274 - val_loss: 148.5716\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 93.9748 - val_loss: 72.2521\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.7970 - val_loss: 79.4203\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 91.8655 - val_loss: 150.1525\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 85.5431 - val_loss: 114.3338\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 77.4573 - val_loss: 124.2701\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.9799 - val_loss: 125.2416\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.2923 - val_loss: 82.2200\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 76.8227 - val_loss: 73.6001\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.3456 - val_loss: 71.4248\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 66.0628 - val_loss: 108.3529\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 69.8477 - val_loss: 93.1893\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 67.6816 - val_loss: 214.2946\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 101.0485 - val_loss: 157.8022\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 81.2244 - val_loss: 128.3359\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 75.8001 - val_loss: 140.9554\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 135.9741 - val_loss: 239.7949\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 102.4963 - val_loss: 68.4227\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 66.7104 - val_loss: 82.6226\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 174.9679 - val_loss: 135.9970\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 88.6235 - val_loss: 75.8036\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.2639 - val_loss: 110.9076\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 108.7780 - val_loss: 87.1917\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 79.2096 - val_loss: 126.7149\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 62.3223 - val_loss: 115.2976\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.9717 - val_loss: 191.4161\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.7570 - val_loss: 67.6668\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 125.7378 - val_loss: 188.4970\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 117.6420 - val_loss: 75.0610\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 129.5784 - val_loss: 88.1814\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.0054 - val_loss: 86.3346\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 72.9202 - val_loss: 128.2060\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 107.9663 - val_loss: 102.4017\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 125.8978 - val_loss: 125.1339\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.1492 - val_loss: 73.7173\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.0405 - val_loss: 70.5151\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.0772 - val_loss: 74.3380\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 91.0175 - val_loss: 115.5106\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.1088 - val_loss: 122.7438\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 78.2198 - val_loss: 101.8929\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 160.3356 - val_loss: 97.9878\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 80.7690 - val_loss: 123.1712\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 80.0389 - val_loss: 89.4806\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 69.7279 - val_loss: 79.5977\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 58.0591 - val_loss: 124.8745\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 82.9314 - val_loss: 105.5912\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 114.9966 - val_loss: 125.6012\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 68.1454 - val_loss: 135.6310\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 77.6470 - val_loss: 75.6836\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.2409 - val_loss: 86.0425\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 52.7252 - val_loss: 76.4355\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.3424 - val_loss: 132.2228\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 84.9503 - val_loss: 86.4859\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 102.0543 - val_loss: 85.3027\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 133.6431 - val_loss: 68.6956\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 89.4723 - val_loss: 84.6294\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 69.7241 - val_loss: 137.0316\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.3224 - val_loss: 124.9297\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 70.8637 - val_loss: 76.5254\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 78.9696 - val_loss: 110.3684\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.6680 - val_loss: 124.4700\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.4699 - val_loss: 144.5913\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.7486 - val_loss: 269.6326\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 70.8469 - val_loss: 81.7671\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 78.2075 - val_loss: 169.0926\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 67.0677 - val_loss: 82.6208\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 76.7783 - val_loss: 244.6155\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 102.4855 - val_loss: 238.8218\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 135.4389 - val_loss: 85.5635\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.4867 - val_loss: 100.1180\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.6538 - val_loss: 89.9195\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 90.0260 - val_loss: 135.8740\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 58.3991 - val_loss: 87.3546\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.1306 - val_loss: 217.1213\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.3546 - val_loss: 118.5268\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 88.9418 - val_loss: 198.8709\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 79.4649 - val_loss: 204.0041\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 62.6031 - val_loss: 72.9409\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.7893 - val_loss: 72.8775\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.4212 - val_loss: 107.0807\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 85.0674 - val_loss: 260.4012\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.9685 - val_loss: 81.5802\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 46.7830 - val_loss: 85.7494\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 75.0115 - val_loss: 99.3674\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 109.7728 - val_loss: 121.9522\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.1538 - val_loss: 83.7077\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.2985 - val_loss: 75.9783\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.4954 - val_loss: 186.8569\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.4880 - val_loss: 69.4227\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 51.1014 - val_loss: 90.1976\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 85.8667 - val_loss: 143.2074\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.0946 - val_loss: 175.7082\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 77.2354 - val_loss: 77.8065\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 127.5528 - val_loss: 155.6046\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 90.6522 - val_loss: 82.0428\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 66.1222 - val_loss: 76.9563\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.1475 - val_loss: 83.8847\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.3215 - val_loss: 134.2305\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 57.5660 - val_loss: 86.1464\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.3780 - val_loss: 77.2767\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 66.8748 - val_loss: 91.8228\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 103.3614 - val_loss: 246.4345\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 66.9807 - val_loss: 77.4378\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 139.5885 - val_loss: 144.3717\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 127.7555 - val_loss: 77.6505\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 60.8527 - val_loss: 131.9872\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 93.3037 - val_loss: 219.5374\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 58.4630 - val_loss: 83.0686\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 60.8183 - val_loss: 84.1829\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.1245 - val_loss: 136.3077\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.0586 - val_loss: 75.7706\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 52.0070 - val_loss: 109.1296\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.7273 - val_loss: 112.0355\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 56.7404 - val_loss: 73.3820\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.5186 - val_loss: 221.3952\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 78.5260 - val_loss: 180.4601\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.9489 - val_loss: 104.0005\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 100.8565 - val_loss: 105.2095\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.5247 - val_loss: 71.8144\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 47.4181 - val_loss: 184.3472\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.4011 - val_loss: 72.5365\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 46.2028 - val_loss: 97.6079\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.3419 - val_loss: 78.0959\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.4287 - val_loss: 136.7714\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 59.1529 - val_loss: 82.0006\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.1365 - val_loss: 91.4717\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 65.7004 - val_loss: 122.8434\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 65.7933 - val_loss: 76.6305\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 77.4744 - val_loss: 79.9711\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.3248 - val_loss: 144.0146\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.1414 - val_loss: 81.4194\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.4347 - val_loss: 75.4093\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 88.7638 - val_loss: 74.7352\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 112.1170 - val_loss: 97.7194\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 66.5732 - val_loss: 172.6002\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 53.8879 - val_loss: 120.1954\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 59.7268 - val_loss: 84.3121\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.3972 - val_loss: 93.9011\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.9095 - val_loss: 88.2591\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 80.0764 - val_loss: 196.2583\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 73.9579 - val_loss: 80.2678\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 54.7977 - val_loss: 116.8975\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 40.5485 - val_loss: 79.5562\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.5263 - val_loss: 91.6667\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.0180 - val_loss: 117.7672\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.3227 - val_loss: 75.0768\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 87.0338 - val_loss: 111.1904\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 73.4820 - val_loss: 150.4557\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 66.3347 - val_loss: 75.2082\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.1597 - val_loss: 117.4456\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.9722 - val_loss: 100.2418\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 49.4563 - val_loss: 81.6227\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.4803 - val_loss: 141.6877\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 50.9755 - val_loss: 222.3989\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 65.2701 - val_loss: 204.6385\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.0757 - val_loss: 77.4756\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.8978 - val_loss: 76.9035\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.9389 - val_loss: 94.8632\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.9293 - val_loss: 129.4441\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 50.4085 - val_loss: 183.7057\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 44.1865 - val_loss: 93.1906\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 109.1167 - val_loss: 200.0404\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 72.9408 - val_loss: 82.9444\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.8502 - val_loss: 96.4319\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 37.9949 - val_loss: 110.0261\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 53.3186 - val_loss: 105.6222\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 61.0907 - val_loss: 82.7717\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 58.9985 - val_loss: 82.0863\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 49.2078 - val_loss: 82.2001\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.3460 - val_loss: 130.2250\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.0696 - val_loss: 83.8560\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.4352 - val_loss: 237.0040\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 72.4299 - val_loss: 87.3805\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 1s 8ms/step - loss: 453624.8438 - val_loss: 2166.1907\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 727.4993 - val_loss: 103.1796\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 142.5974 - val_loss: 101.2081\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 122.6118 - val_loss: 87.1273\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 121.2412 - val_loss: 101.7818\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.9114 - val_loss: 63.7105\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 94.8846 - val_loss: 71.0566\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 105.4318 - val_loss: 69.8673\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 133.0845 - val_loss: 66.2153\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 109.0437 - val_loss: 85.8576\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 86.9367 - val_loss: 80.3581\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 83.3726 - val_loss: 84.0857\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 82.4335 - val_loss: 71.1694\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 154.9354 - val_loss: 165.3065\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 97.2567 - val_loss: 68.9043\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 109.2371 - val_loss: 63.3847\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.1674 - val_loss: 65.1310\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.9640 - val_loss: 88.6074\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 87.1364 - val_loss: 60.7441\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.1625 - val_loss: 94.2176\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 103.2977 - val_loss: 131.7078\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 94.5019 - val_loss: 151.4434\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 92.2830 - val_loss: 61.9046\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 120.3419 - val_loss: 261.5533\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 122.7500 - val_loss: 62.7871\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.2949 - val_loss: 57.3928\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 95.7705 - val_loss: 56.7797\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 107.4967 - val_loss: 113.8394\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 105.5842 - val_loss: 127.0598\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.1567 - val_loss: 65.1878\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 73.3044 - val_loss: 66.5869\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.3467 - val_loss: 59.1154\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 92.3258 - val_loss: 60.2525\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 88.7388 - val_loss: 73.9614\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 153.1545 - val_loss: 141.8813\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 143.4554 - val_loss: 82.7515\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 120.9358 - val_loss: 91.2688\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 100.5786 - val_loss: 79.7081\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.4646 - val_loss: 73.2501\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 111.9000 - val_loss: 79.3060\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 90.0184 - val_loss: 68.7819\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.2269 - val_loss: 65.4067\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.1569 - val_loss: 67.1639\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.6621 - val_loss: 60.6933\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 71.7312 - val_loss: 69.3396\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 125.5338 - val_loss: 232.4291\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 115.0367 - val_loss: 69.5055\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 127.1894 - val_loss: 175.6055\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 99.8468 - val_loss: 66.4101\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 71.5073 - val_loss: 117.4107\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 125.8389 - val_loss: 80.2448\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.5275 - val_loss: 121.2542\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 72.6907 - val_loss: 92.9046\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 82.6176 - val_loss: 237.7912\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 235.4171 - val_loss: 80.0659\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 101.9191 - val_loss: 104.4676\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 79.9119 - val_loss: 81.6672\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.5016 - val_loss: 71.9957\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.9142 - val_loss: 75.4984\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 66.6472 - val_loss: 66.7415\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.6686 - val_loss: 64.0597\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 116.1531 - val_loss: 102.7852\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 76.2316 - val_loss: 60.8550\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 79.9957 - val_loss: 139.1449\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 81.6502 - val_loss: 63.5137\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.6834 - val_loss: 78.2659\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.1511 - val_loss: 129.5878\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 71.9067 - val_loss: 103.1420\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 78.7739 - val_loss: 92.3065\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 70.2103 - val_loss: 82.5968\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.4021 - val_loss: 63.7477\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 64.5145 - val_loss: 66.7716\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 96.6453 - val_loss: 61.0790\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 59.7974 - val_loss: 61.5007\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.5609 - val_loss: 155.4758\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 75.8249 - val_loss: 82.5421\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 66.4547 - val_loss: 66.2635\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.7832 - val_loss: 91.7778\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 82.7785 - val_loss: 87.8897\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 92.7477 - val_loss: 103.5194\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 58.8152 - val_loss: 68.8412\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.5503 - val_loss: 150.3766\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 80.5305 - val_loss: 62.3931\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 72.3566 - val_loss: 71.0313\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 74.4615 - val_loss: 85.2999\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 74.7847 - val_loss: 102.8284\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.0206 - val_loss: 65.0778\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.9723 - val_loss: 78.4859\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.9800 - val_loss: 73.4551\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 111.1072 - val_loss: 124.9237\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 79.7077 - val_loss: 64.3037\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 137.9875 - val_loss: 137.9348\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 111.1942 - val_loss: 94.4848\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.3474 - val_loss: 72.9503\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.6968 - val_loss: 69.7349\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 78.9566 - val_loss: 178.5522\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 62.9644 - val_loss: 66.4884\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 64.7485 - val_loss: 80.8218\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.8548 - val_loss: 77.6357\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.5633 - val_loss: 62.3138\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 59.6022 - val_loss: 80.3007\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 85.3130 - val_loss: 70.8693\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.6796 - val_loss: 73.8847\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 49.6885 - val_loss: 64.2977\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 75.3523 - val_loss: 60.2109\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.7798 - val_loss: 68.1909\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.1030 - val_loss: 67.2487\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.9582 - val_loss: 76.8177\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.5387 - val_loss: 148.0917\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 50.6591 - val_loss: 62.4805\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 76.4576 - val_loss: 123.0937\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.0826 - val_loss: 89.3888\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 49.4596 - val_loss: 71.5032\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 58.5734 - val_loss: 65.9096\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.3823 - val_loss: 75.9301\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.2668 - val_loss: 76.2659\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 67.8939 - val_loss: 104.6348\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.6030 - val_loss: 89.5165\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 70.9934 - val_loss: 81.0484\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.0484 - val_loss: 71.0249\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.6560 - val_loss: 67.0407\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.0310 - val_loss: 70.6821\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.3016 - val_loss: 70.9963\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.3706 - val_loss: 67.1407\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 71.9530 - val_loss: 92.6314\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.1781 - val_loss: 70.4574\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.4702 - val_loss: 86.6204\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 46.8301 - val_loss: 68.3689\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.9190 - val_loss: 72.5172\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 53.1441 - val_loss: 72.6173\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.7069 - val_loss: 92.1040\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 72.5536 - val_loss: 171.0864\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 67.0285 - val_loss: 74.0179\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.9327 - val_loss: 68.7217\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.5849 - val_loss: 69.1051\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 91.9783 - val_loss: 164.8436\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 58.4979 - val_loss: 67.2261\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 435.0251 - val_loss: 348.5914\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 219.7472 - val_loss: 85.9094\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 73.9587 - val_loss: 77.9914\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.6361 - val_loss: 64.0172\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.4411 - val_loss: 64.5286\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 49.6803 - val_loss: 101.1347\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.4882 - val_loss: 95.0847\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 62.2999 - val_loss: 82.2196\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.3568 - val_loss: 82.2512\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.3605 - val_loss: 73.9786\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 44.9991 - val_loss: 67.2288\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 49.7824 - val_loss: 168.9326\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 56.5514 - val_loss: 88.1870\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.2325 - val_loss: 66.9225\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 42.6061 - val_loss: 80.4211\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.1340 - val_loss: 100.9956\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.1735 - val_loss: 68.6421\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 46.2928 - val_loss: 67.3885\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.8659 - val_loss: 66.4263\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.4589 - val_loss: 71.4640\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.3194 - val_loss: 121.1823\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 46.2318 - val_loss: 87.3393\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.2967 - val_loss: 80.5669\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.3360 - val_loss: 70.2787\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 46.0769 - val_loss: 77.7949\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 49.4126 - val_loss: 67.7028\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 43.4708 - val_loss: 108.2590\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.1506 - val_loss: 78.7459\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.8674 - val_loss: 71.1484\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.7542 - val_loss: 87.5943\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.2960 - val_loss: 82.0448\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.0230 - val_loss: 82.6024\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.2568 - val_loss: 89.1204\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 66.3119 - val_loss: 76.8080\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 39.2834 - val_loss: 83.8677\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.6508 - val_loss: 102.3799\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.8186 - val_loss: 77.9308\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 58.2500 - val_loss: 84.4864\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 68.4199 - val_loss: 114.8695\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.3959 - val_loss: 109.8380\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.3747 - val_loss: 86.8576\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.5908 - val_loss: 124.5464\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.0984 - val_loss: 85.8480\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 58.3417 - val_loss: 133.1114\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.3399 - val_loss: 102.6275\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.2510 - val_loss: 82.0173\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 39.7533 - val_loss: 85.5153\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.2178 - val_loss: 80.1151\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.5433 - val_loss: 93.2739\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 40.8738 - val_loss: 94.8866\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.0809 - val_loss: 116.0453\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 90.7174 - val_loss: 82.2821\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.6544 - val_loss: 83.3704\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.8334 - val_loss: 104.4237\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 67.5522 - val_loss: 90.2124\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 46.9484 - val_loss: 88.3980\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.3879 - val_loss: 87.3006\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.0986 - val_loss: 97.6842\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 86.7681 - val_loss: 76.8713\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.8280 - val_loss: 83.2114\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 39.3952 - val_loss: 91.5622\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 42.7673 - val_loss: 106.4634\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 58.9522 - val_loss: 79.5968\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 1s 9ms/step - loss: 1044608.6875 - val_loss: 4837.0771\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 1787.8330 - val_loss: 300.5150\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 167.8987 - val_loss: 213.3299\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 128.7784 - val_loss: 253.4113\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 108.7800 - val_loss: 190.0298\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 93.5987 - val_loss: 192.9695\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 94.0865 - val_loss: 171.8252\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 96.6783 - val_loss: 175.3822\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 86.2048 - val_loss: 192.1504\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 76.8417 - val_loss: 173.9995\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 68.6012 - val_loss: 142.3266\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 66.6178 - val_loss: 147.9064\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.8330 - val_loss: 140.0156\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.6484 - val_loss: 155.0658\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.9429 - val_loss: 171.0303\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 67.1093 - val_loss: 133.3236\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 62.7865 - val_loss: 151.8408\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 69.8402 - val_loss: 206.7709\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 72.3108 - val_loss: 138.4055\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.6144 - val_loss: 128.1846\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.8153 - val_loss: 129.5211\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.0627 - val_loss: 116.0601\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.4716 - val_loss: 171.7730\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.5650 - val_loss: 134.3765\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 62.9101 - val_loss: 123.5527\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 69.2220 - val_loss: 124.2202\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.9079 - val_loss: 126.1343\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 58.9305 - val_loss: 150.2086\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.0658 - val_loss: 156.5325\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.8746 - val_loss: 181.7305\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.4800 - val_loss: 117.4296\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.2553 - val_loss: 92.5527\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 67.1478 - val_loss: 120.8976\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.8889 - val_loss: 126.6874\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.6603 - val_loss: 117.0049\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.6490 - val_loss: 107.3502\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 49.6800 - val_loss: 118.6594\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 60.7205 - val_loss: 94.5142\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 59.0778 - val_loss: 129.2843\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.7696 - val_loss: 136.8946\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.6741 - val_loss: 89.4432\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.2259 - val_loss: 120.6676\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.2111 - val_loss: 97.3283\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 52.0725 - val_loss: 106.3536\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.4361 - val_loss: 109.4463\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.6891 - val_loss: 101.8899\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.2996 - val_loss: 109.3196\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 52.4196 - val_loss: 106.2931\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.7427 - val_loss: 115.3045\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.5272 - val_loss: 105.4978\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.9452 - val_loss: 121.6796\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 50.7935 - val_loss: 108.1101\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 50.9542 - val_loss: 130.2898\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.3107 - val_loss: 106.2127\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 40.0966 - val_loss: 104.3064\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 40.9801 - val_loss: 107.6247\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 53.3894 - val_loss: 104.3201\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.2246 - val_loss: 105.1092\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.4778 - val_loss: 91.0643\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.0206 - val_loss: 103.5972\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.7357 - val_loss: 100.9587\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.1974 - val_loss: 112.3677\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 54.7853 - val_loss: 126.9127\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.0600 - val_loss: 105.0528\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 43.9438 - val_loss: 105.9033\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.5513 - val_loss: 124.9072\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.3565 - val_loss: 91.1290\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.6027 - val_loss: 99.0080\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 42.2927 - val_loss: 98.7243\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.4999 - val_loss: 121.3950\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.2657 - val_loss: 126.9856\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.7789 - val_loss: 125.0403\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.6156 - val_loss: 103.6110\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.6300 - val_loss: 128.1388\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.4247 - val_loss: 95.2365\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 38.6475 - val_loss: 100.8863\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.9566 - val_loss: 105.1753\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.7427 - val_loss: 98.2608\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 44.9874 - val_loss: 126.1421\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.4369 - val_loss: 105.0904\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 44.2579 - val_loss: 92.9117\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.1604 - val_loss: 100.6373\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.7397 - val_loss: 96.1352\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 40.0224 - val_loss: 97.5888\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.0881 - val_loss: 85.4073\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.4295 - val_loss: 117.9051\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 42.6681 - val_loss: 98.3375\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 42.2253 - val_loss: 89.5286\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 43.1813 - val_loss: 132.9218\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 59.6538 - val_loss: 93.3960\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 42.7233 - val_loss: 101.4304\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.8621 - val_loss: 104.3300\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 44.1110 - val_loss: 107.9456\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 40.3817 - val_loss: 98.7969\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 42.1994 - val_loss: 107.9247\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 36.9416 - val_loss: 98.3283\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 39.1723 - val_loss: 105.4882\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.4450 - val_loss: 101.3337\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.6537 - val_loss: 89.9409\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.7479 - val_loss: 132.5134\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.9953 - val_loss: 97.1967\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 43.8407 - val_loss: 120.4242\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 41.9528 - val_loss: 99.4522\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 36.3386 - val_loss: 91.4819\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 36.7167 - val_loss: 133.1009\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.1631 - val_loss: 97.3157\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 33.2316 - val_loss: 93.5683\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.6905 - val_loss: 113.9817\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 38.6971 - val_loss: 96.6973\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 34.2121 - val_loss: 104.5514\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 41.4077 - val_loss: 106.1552\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 39.2789 - val_loss: 96.2244\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 37.7189 - val_loss: 85.4377\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 40.6925 - val_loss: 88.0588\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 35.2144 - val_loss: 100.7306\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.6401 - val_loss: 106.3556\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 44.7701 - val_loss: 109.9520\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 43.8636 - val_loss: 123.5590\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 38.0450 - val_loss: 93.6901\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 42.2432 - val_loss: 124.6959\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 40.8208 - val_loss: 100.9077\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 35.8142 - val_loss: 119.3463\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 41.3337 - val_loss: 114.3632\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 46.1547 - val_loss: 106.9379\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 37.5841 - val_loss: 109.3061\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.4058 - val_loss: 151.6355\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 75.1541 - val_loss: 146.7297\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 37.4179 - val_loss: 111.7006\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 42.1192 - val_loss: 111.0615\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 42.9834 - val_loss: 111.7544\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.5150 - val_loss: 159.8896\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.5095 - val_loss: 116.7651\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 52.7987 - val_loss: 135.6678\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.0436 - val_loss: 125.9314\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.0252 - val_loss: 122.7926\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.4170 - val_loss: 114.9227\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 34.7941 - val_loss: 125.4317\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 35.1268 - val_loss: 105.5073\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.6423 - val_loss: 115.1449\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 35.5872 - val_loss: 117.8448\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 32.0230 - val_loss: 117.0936\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 35.1753 - val_loss: 125.0884\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 40.8440 - val_loss: 137.7352\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.9873 - val_loss: 161.8876\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 37.1132 - val_loss: 108.6280\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 33.3812 - val_loss: 98.9345\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 31.2382 - val_loss: 119.1012\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 33.8392 - val_loss: 125.5138\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.7611 - val_loss: 141.1923\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 40.8670 - val_loss: 108.7401\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 36.0692 - val_loss: 124.3770\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 37.8178 - val_loss: 128.9199\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 36.7589 - val_loss: 104.5189\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.8461 - val_loss: 102.1574\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 36.2901 - val_loss: 136.2066\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 44.9393 - val_loss: 110.6725\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 32.4824 - val_loss: 114.9219\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 35.4680 - val_loss: 127.3420\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 37.4243 - val_loss: 104.4634\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 41.0254 - val_loss: 115.0399\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.0558 - val_loss: 107.5177\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.7349 - val_loss: 123.8488\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 32.6584 - val_loss: 112.4931\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 38.1928 - val_loss: 103.6620\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 46.8184 - val_loss: 110.1019\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 36.6970 - val_loss: 119.5436\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 34.0852 - val_loss: 120.3695\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 31.6616 - val_loss: 102.0061\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 36.6360 - val_loss: 121.2660\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 30.2881 - val_loss: 115.9499\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 33.8989 - val_loss: 119.5816\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 31.4852 - val_loss: 110.5755\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 33.3037 - val_loss: 106.6326\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 33.7420 - val_loss: 97.6172\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 37.0398 - val_loss: 110.8489\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 35.1257 - val_loss: 112.9474\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 36.9269 - val_loss: 115.0446\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.9073 - val_loss: 145.4747\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.1272 - val_loss: 114.8862\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 33.0442 - val_loss: 105.2409\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.8850 - val_loss: 107.4246\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 46.7346 - val_loss: 100.9524\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.2883 - val_loss: 140.8822\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.3566 - val_loss: 142.2431\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.3671 - val_loss: 117.2417\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 35.1434 - val_loss: 117.6828\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 43.1789 - val_loss: 105.6844\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.3052 - val_loss: 117.1905\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 40.7290 - val_loss: 112.3414\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 42.0651 - val_loss: 123.0725\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 35.9043 - val_loss: 111.7009\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 39.5176 - val_loss: 99.7153\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 40.7198 - val_loss: 102.0693\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 38.0551 - val_loss: 107.5512\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 38.2817 - val_loss: 111.2257\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 33.9275 - val_loss: 94.4624\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 40.5573 - val_loss: 104.0749\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 40.5120 - val_loss: 103.9940\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.5070 - val_loss: 102.0452\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 33.4167 - val_loss: 109.9890\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 1s 8ms/step - loss: 1619204.5000 - val_loss: 2921.4287\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 677.6989 - val_loss: 104.5183\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 130.0356 - val_loss: 82.2980\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 116.6783 - val_loss: 64.3025\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 96.1073 - val_loss: 64.7965\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.9339 - val_loss: 123.6475\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 86.7167 - val_loss: 127.3676\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 89.1404 - val_loss: 73.8797\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 85.4525 - val_loss: 74.0747\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 91.7304 - val_loss: 67.3659\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.5915 - val_loss: 126.7984\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 101.4527 - val_loss: 97.5076\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.4886 - val_loss: 70.8991\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.2975 - val_loss: 63.2967\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.1924 - val_loss: 73.9397\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 86.7854 - val_loss: 85.4823\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 93.8420 - val_loss: 104.0194\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 88.2906 - val_loss: 100.6329\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 112.9275 - val_loss: 73.2077\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.0728 - val_loss: 87.2093\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 82.4592 - val_loss: 81.2608\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 68.1680 - val_loss: 70.5635\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 70.3701 - val_loss: 85.9347\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.9935 - val_loss: 84.1409\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 72.8653 - val_loss: 83.2811\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 70.0439 - val_loss: 98.2522\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 74.7356 - val_loss: 142.0343\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 151.3802 - val_loss: 71.4649\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 140.9023 - val_loss: 82.7284\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 69.0708 - val_loss: 70.5693\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.0306 - val_loss: 83.6632\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 96.7113 - val_loss: 119.1560\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 92.5211 - val_loss: 126.9747\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 111.3727 - val_loss: 97.7674\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.4019 - val_loss: 152.3157\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 99.7916 - val_loss: 79.6944\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 82.6602 - val_loss: 77.2010\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 60.0143 - val_loss: 87.6282\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 69.5079 - val_loss: 67.0991\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 92.4036 - val_loss: 81.1609\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 75.8404 - val_loss: 70.5932\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 67.8718 - val_loss: 109.0607\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 67.0478 - val_loss: 150.4529\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.9650 - val_loss: 97.6680\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 121.1680 - val_loss: 120.9616\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 210.1954 - val_loss: 96.4303\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 80.9051 - val_loss: 111.0214\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 112.9267 - val_loss: 92.4201\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 76.2464 - val_loss: 87.5591\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 93.3299 - val_loss: 88.3627\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.2327 - val_loss: 134.7365\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 84.2184 - val_loss: 61.2225\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 58.6167 - val_loss: 71.2125\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 65.6036 - val_loss: 122.8718\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 79.5065 - val_loss: 81.8653\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 78.1982 - val_loss: 72.3267\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.9776 - val_loss: 101.5077\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.7467 - val_loss: 106.3788\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 94.2724 - val_loss: 128.1059\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.6204 - val_loss: 66.9603\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 84.3245 - val_loss: 70.0091\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 76.9101 - val_loss: 63.6938\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 162.1441 - val_loss: 101.9232\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 114.3132 - val_loss: 74.3156\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 103.4439 - val_loss: 129.5481\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.4449 - val_loss: 158.3489\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 104.4137 - val_loss: 132.5607\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 74.0552 - val_loss: 132.8376\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 70.3725 - val_loss: 58.6989\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 61.3266 - val_loss: 66.0919\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 66.1143 - val_loss: 158.4528\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 65.6009 - val_loss: 70.7264\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.2841 - val_loss: 65.7421\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 150.3638 - val_loss: 138.1780\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 91.5399 - val_loss: 112.1811\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.5329 - val_loss: 111.1378\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 69.1513 - val_loss: 62.9973\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.8115 - val_loss: 235.9230\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 114.9490 - val_loss: 82.9737\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 62.1396 - val_loss: 78.0733\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 60.8905 - val_loss: 150.6319\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.3900 - val_loss: 105.5628\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 70.4462 - val_loss: 77.5073\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.5041 - val_loss: 59.7616\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 67.8632 - val_loss: 75.3439\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 62.3653 - val_loss: 125.3183\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 79.3520 - val_loss: 93.7647\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 90.4199 - val_loss: 126.7412\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 107.6070 - val_loss: 64.4418\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 91.9078 - val_loss: 71.9519\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 78.4267 - val_loss: 246.9265\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.3346 - val_loss: 79.8116\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 81.0224 - val_loss: 74.7030\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.9448 - val_loss: 91.7546\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 54.3359 - val_loss: 78.8180\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.2923 - val_loss: 84.1517\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.4872 - val_loss: 91.6090\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.6959 - val_loss: 64.3878\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 75.9137 - val_loss: 64.8189\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 104.2500 - val_loss: 112.7075\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 91.6191 - val_loss: 96.5767\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.3251 - val_loss: 69.4594\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 58.9004 - val_loss: 81.3854\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 57.5459 - val_loss: 67.9223\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.1126 - val_loss: 74.8322\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.1370 - val_loss: 80.7643\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 58.1578 - val_loss: 68.5463\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 58.1141 - val_loss: 64.4667\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.4327 - val_loss: 94.2993\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 71.8707 - val_loss: 155.9888\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.2949 - val_loss: 73.9380\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.6769 - val_loss: 89.6979\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 68.4754 - val_loss: 92.0823\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 119.7123 - val_loss: 93.8636\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 89.9458 - val_loss: 68.9787\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 60.6913 - val_loss: 68.9116\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.2291 - val_loss: 147.0473\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.1802 - val_loss: 85.0399\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 53.6334 - val_loss: 99.1643\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 59.3719 - val_loss: 70.6709\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 61.7741 - val_loss: 63.5847\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 52.4236 - val_loss: 72.5408\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 62.1600 - val_loss: 64.0267\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 70.6145 - val_loss: 100.1929\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 68.6597 - val_loss: 70.2772\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.6756 - val_loss: 82.7538\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 81.0881 - val_loss: 143.7974\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 92.8527 - val_loss: 138.8186\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.5124 - val_loss: 59.7559\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.3650 - val_loss: 80.1919\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.7499 - val_loss: 108.2810\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.3105 - val_loss: 100.1012\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.6289 - val_loss: 66.9074\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 58.0857 - val_loss: 76.1424\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.1761 - val_loss: 79.1689\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.2949 - val_loss: 63.3114\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 56.5932 - val_loss: 77.4268\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 49.8305 - val_loss: 81.6109\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.2264 - val_loss: 124.3008\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 77.0900 - val_loss: 76.2440\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.3401 - val_loss: 75.4324\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.5190 - val_loss: 79.1172\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.9009 - val_loss: 65.0029\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.6240 - val_loss: 63.7270\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.3014 - val_loss: 106.8484\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 74.7736 - val_loss: 128.6428\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.9546 - val_loss: 65.2556\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.8420 - val_loss: 62.6027\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 60.0452 - val_loss: 61.2369\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.2401 - val_loss: 82.9071\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.0703 - val_loss: 68.7521\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 50.1681 - val_loss: 90.4458\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 66.7382 - val_loss: 60.8527\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 59.4059 - val_loss: 99.1974\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 74.0783 - val_loss: 112.7546\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.8838 - val_loss: 103.0503\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.4260 - val_loss: 73.5572\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 50.6353 - val_loss: 63.0041\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.9956 - val_loss: 117.0700\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 82.9360 - val_loss: 66.8006\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 50.4646 - val_loss: 68.8613\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 53.0825 - val_loss: 89.7039\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 44.0833 - val_loss: 82.7697\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 60.1321 - val_loss: 68.8839\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.5219 - val_loss: 100.3517\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.8381 - val_loss: 60.5999\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 74.4662 - val_loss: 123.9792\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.4832 - val_loss: 83.4125\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 43.7034 - val_loss: 63.8821\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.2939 - val_loss: 109.9181\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.4336 - val_loss: 83.1708\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 59.2786 - val_loss: 86.2076\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 58.5271 - val_loss: 79.9834\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 46.0123 - val_loss: 62.8290\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.3945 - val_loss: 63.0297\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.5610 - val_loss: 62.7205\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 41.3393 - val_loss: 64.9382\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 80.7396 - val_loss: 85.5938\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 54.5191 - val_loss: 126.3322\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 114.7044 - val_loss: 72.9783\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.0696 - val_loss: 81.2509\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.2612 - val_loss: 60.4043\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.8179 - val_loss: 66.9676\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 57.6846 - val_loss: 81.8912\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.7888 - val_loss: 97.8267\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.9058 - val_loss: 87.8414\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.9849 - val_loss: 70.7566\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.2960 - val_loss: 67.4976\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 49.7878 - val_loss: 82.0451\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 69.7865 - val_loss: 72.4498\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 41.1687 - val_loss: 62.0360\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.6153 - val_loss: 146.4385\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.2781 - val_loss: 59.4863\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 46.4349 - val_loss: 96.6749\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 56.5344 - val_loss: 94.7631\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.3559 - val_loss: 71.6324\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 43.3944 - val_loss: 61.3366\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.6872 - val_loss: 61.0204\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.1562 - val_loss: 65.4300\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.1358 - val_loss: 69.2602\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 1s 8ms/step - loss: 630799.0625 - val_loss: 333.7323\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 406.5376 - val_loss: 97.9477\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.5893 - val_loss: 89.1642\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 108.3448 - val_loss: 163.8937\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 164.4154 - val_loss: 96.9286\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.3222 - val_loss: 192.1148\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 154.4618 - val_loss: 128.8740\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 188.6932 - val_loss: 83.8219\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 195.9185 - val_loss: 352.6940\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 200.7022 - val_loss: 183.0471\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 146.3270 - val_loss: 165.0551\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 86.9882 - val_loss: 165.8236\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 112.1283 - val_loss: 157.5854\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 110.2250 - val_loss: 155.1336\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 117.2782 - val_loss: 262.5568\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 389.6172 - val_loss: 281.6216\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 144.3879 - val_loss: 199.9468\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 108.3753 - val_loss: 73.9705\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 107.5358 - val_loss: 151.4304\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 88.5892 - val_loss: 143.0630\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 122.1308 - val_loss: 78.6014\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 88.3717 - val_loss: 105.1628\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 85.8816 - val_loss: 97.6563\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 120.2473 - val_loss: 142.7174\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 326.1769 - val_loss: 149.8778\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 194.1057 - val_loss: 170.6373\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 230.6574 - val_loss: 372.5731\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.3779 - val_loss: 165.8734\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 142.0934 - val_loss: 87.5037\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 78.6189 - val_loss: 131.3799\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.0623 - val_loss: 127.6876\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 78.3036 - val_loss: 89.0282\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 282.8223 - val_loss: 300.9093\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 156.6359 - val_loss: 90.3223\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.5942 - val_loss: 94.2091\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 78.2013 - val_loss: 114.2179\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 59.5351 - val_loss: 128.6799\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 101.8991 - val_loss: 156.1749\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 104.9272 - val_loss: 138.4888\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 72.9087 - val_loss: 182.7582\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 106.1481 - val_loss: 97.1832\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 75.9021 - val_loss: 133.1199\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.4056 - val_loss: 80.1936\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 62.9296 - val_loss: 129.0237\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 92.1985 - val_loss: 197.0769\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 109.2439 - val_loss: 91.7988\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 60.7515 - val_loss: 82.3378\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 134.5167 - val_loss: 128.8642\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 101.5625 - val_loss: 180.9409\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.0596 - val_loss: 106.9720\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 80.6533 - val_loss: 84.8411\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.1602 - val_loss: 91.7203\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 88.2480 - val_loss: 150.9994\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 77.5541 - val_loss: 98.5920\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 72.9200 - val_loss: 86.6582\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 81.6464 - val_loss: 82.6586\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.5687 - val_loss: 102.3549\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 97.8906 - val_loss: 315.6385\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 91.1349 - val_loss: 134.6393\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 71.9702 - val_loss: 90.2918\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.8625 - val_loss: 110.9486\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 79.2435 - val_loss: 164.7777\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 134.9996 - val_loss: 126.2404\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 60.2991 - val_loss: 86.2106\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.7569 - val_loss: 78.2555\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 80.0677 - val_loss: 84.8643\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 67.3305 - val_loss: 90.6663\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 98.3068 - val_loss: 101.5597\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 75.2425 - val_loss: 81.3775\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 62.2325 - val_loss: 81.0293\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.6899 - val_loss: 147.2095\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.4902 - val_loss: 98.3223\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.2512 - val_loss: 82.4782\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 81.6604 - val_loss: 85.7116\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.1368 - val_loss: 82.4368\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 65.4224 - val_loss: 119.9874\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 164.3972 - val_loss: 200.1077\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 69.6183 - val_loss: 182.4955\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.9889 - val_loss: 95.9327\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.3358 - val_loss: 107.8455\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.9691 - val_loss: 129.3485\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.0747 - val_loss: 85.9513\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 84.5353 - val_loss: 123.7460\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 65.8995 - val_loss: 91.5878\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 59.9484 - val_loss: 127.0703\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.9616 - val_loss: 136.0911\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 79.8991 - val_loss: 86.6857\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 64.0960 - val_loss: 175.5236\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 115.1492 - val_loss: 286.6900\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 80.3799 - val_loss: 92.9612\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.8226 - val_loss: 105.4101\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 69.4559 - val_loss: 111.3261\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 95.1146 - val_loss: 78.6128\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 49.5268 - val_loss: 89.7067\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 70.2943 - val_loss: 94.6589\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 65.7299 - val_loss: 94.2511\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 52.9477 - val_loss: 79.7265\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.4720 - val_loss: 101.8921\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 44.2925 - val_loss: 125.0457\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.3960 - val_loss: 75.8842\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 94.5373 - val_loss: 328.4014\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 91.2472 - val_loss: 110.9933\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.4963 - val_loss: 114.3049\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.7703 - val_loss: 97.3568\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.3054 - val_loss: 103.4970\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 56.5639 - val_loss: 112.3153\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 49.4716 - val_loss: 88.1896\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.9177 - val_loss: 87.3504\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.2444 - val_loss: 93.2987\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 67.3924 - val_loss: 92.1947\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.1793 - val_loss: 98.9701\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.7448 - val_loss: 95.9066\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.5862 - val_loss: 81.7113\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 120.4933 - val_loss: 215.7214\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 117.1891 - val_loss: 74.4871\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 50.1830 - val_loss: 90.8837\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 60.3562 - val_loss: 80.4999\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.6028 - val_loss: 83.5121\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.1391 - val_loss: 89.4741\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 49.3201 - val_loss: 102.6081\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.9867 - val_loss: 83.4621\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 51.6138 - val_loss: 95.3741\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 102.7444 - val_loss: 164.2882\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 91.9766 - val_loss: 92.5392\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 55.1547 - val_loss: 115.9391\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.0744 - val_loss: 115.3341\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 71.7510 - val_loss: 92.3889\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 52.2273 - val_loss: 84.9795\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.9201 - val_loss: 82.7704\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.1724 - val_loss: 81.3343\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 50.1244 - val_loss: 80.1095\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.9419 - val_loss: 85.5100\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 45.9140 - val_loss: 81.6667\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 72.1418 - val_loss: 170.3655\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 49.2922 - val_loss: 86.5783\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 42.3000 - val_loss: 92.5137\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 57.8898 - val_loss: 192.6804\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 156.6476 - val_loss: 83.6533\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 80.2643 - val_loss: 84.0973\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.6969 - val_loss: 84.1149\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.0644 - val_loss: 110.5148\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.5045 - val_loss: 86.9404\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.5324 - val_loss: 83.7847\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 43.8809 - val_loss: 97.5467\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 46.5512 - val_loss: 87.2401\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 54.0979 - val_loss: 134.5306\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.6660 - val_loss: 157.4802\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 59.3959 - val_loss: 104.9193\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 49.6003 - val_loss: 112.1988\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.1870 - val_loss: 177.1734\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.3583 - val_loss: 84.0412\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 44.0297 - val_loss: 88.5261\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 53.7044 - val_loss: 129.2205\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.3332 - val_loss: 99.8169\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 42.2255 - val_loss: 107.2142\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.0489 - val_loss: 96.7042\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.8288 - val_loss: 150.5240\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 42.3497 - val_loss: 89.0932\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.8812 - val_loss: 83.5317\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 42.1774 - val_loss: 115.3281\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.4400 - val_loss: 85.8831\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 39.8832 - val_loss: 93.2117\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 53.1321 - val_loss: 82.7373\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.5444 - val_loss: 125.6305\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.4152 - val_loss: 87.5429\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.7851 - val_loss: 80.1255\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 41.8458 - val_loss: 93.8779\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.3124 - val_loss: 89.4247\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.8015 - val_loss: 107.2346\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 42.5560 - val_loss: 90.3596\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 44.9566 - val_loss: 79.3039\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 62.3206 - val_loss: 130.6794\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 46.5050 - val_loss: 105.7368\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 48.3108 - val_loss: 153.4159\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 67.2237 - val_loss: 84.5641\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 41.2300 - val_loss: 92.8645\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 40.4215 - val_loss: 108.4948\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 42.8081 - val_loss: 92.9028\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 56.0595 - val_loss: 81.4401\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.9480 - val_loss: 79.7711\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 46.2801 - val_loss: 89.4601\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 41.3520 - val_loss: 151.5749\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 50.3734 - val_loss: 109.3546\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.4097 - val_loss: 87.6566\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 47.1373 - val_loss: 81.1847\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 51.2279 - val_loss: 90.7326\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.3816 - val_loss: 130.3236\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 132.0082 - val_loss: 140.0763\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.3909 - val_loss: 87.9517\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 44.0876 - val_loss: 90.7483\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 49.8621 - val_loss: 88.6786\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.2729 - val_loss: 87.7761\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 45.7791 - val_loss: 87.9528\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 47.3017 - val_loss: 104.7249\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 60.7725 - val_loss: 104.0784\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 68.7956 - val_loss: 107.5106\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 48.8191 - val_loss: 99.8338\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 49.6644 - val_loss: 94.6092\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 57.4140 - val_loss: 91.3775\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 55.5928 - val_loss: 83.0032\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 1s 8ms/step - loss: 638310.1875 - val_loss: 2108.8044\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 423.6748 - val_loss: 244.5239\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 131.8066 - val_loss: 159.0329\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 124.7063 - val_loss: 99.1880\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 119.8754 - val_loss: 151.4933\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 113.0083 - val_loss: 77.5947\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 128.0043 - val_loss: 129.4188\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 129.3123 - val_loss: 93.9659\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 109.3722 - val_loss: 176.2887\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 135.2505 - val_loss: 73.0831\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 147.2744 - val_loss: 130.6442\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 136.4610 - val_loss: 212.2899\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 115.0749 - val_loss: 137.2097\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.9119 - val_loss: 69.1038\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 114.3334 - val_loss: 76.8920\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 138.4736 - val_loss: 176.0746\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 208.1920 - val_loss: 79.7946\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 193.0960 - val_loss: 91.4284\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 125.5819 - val_loss: 473.3552\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 188.5586 - val_loss: 76.5605\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 116.5076 - val_loss: 76.5056\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 83.4489 - val_loss: 97.8988\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 123.8385 - val_loss: 97.3193\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 150.0365 - val_loss: 505.6235\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 254.2026 - val_loss: 69.3046\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 209.4514 - val_loss: 235.1968\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 121.7388 - val_loss: 101.3304\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 139.6160 - val_loss: 116.4716\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 124.5057 - val_loss: 81.0711\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 81.1888 - val_loss: 76.5489\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 96.6546 - val_loss: 219.1053\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 203.1161 - val_loss: 65.2676\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 98.5372 - val_loss: 76.7924\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.5904 - val_loss: 85.6964\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 88.2800 - val_loss: 78.3835\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.5545 - val_loss: 285.3172\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 115.7295 - val_loss: 93.3057\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 172.4793 - val_loss: 88.5905\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 151.5953 - val_loss: 104.9842\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 152.3158 - val_loss: 282.2066\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 309.6700 - val_loss: 561.8401\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 343.2204 - val_loss: 410.6470\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 188.4067 - val_loss: 53.9570\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 99.2601 - val_loss: 66.5564\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.5389 - val_loss: 312.8842\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 136.8672 - val_loss: 77.3526\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.3719 - val_loss: 109.2510\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 151.3806 - val_loss: 59.3532\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 127.9976 - val_loss: 57.2721\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 109.4169 - val_loss: 148.6706\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 147.2469 - val_loss: 95.2496\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 106.7591 - val_loss: 148.4335\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 148.0590 - val_loss: 60.2888\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 102.7300 - val_loss: 153.3259\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.9353 - val_loss: 107.9185\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 98.7488 - val_loss: 70.2420\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.1623 - val_loss: 61.8010\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 117.2813 - val_loss: 90.7279\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 111.7171 - val_loss: 91.0434\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 134.7782 - val_loss: 111.1538\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 78.7840 - val_loss: 70.0069\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 98.2138 - val_loss: 88.8889\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 120.3429 - val_loss: 210.6877\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 77.8968 - val_loss: 62.2658\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 82.9369 - val_loss: 89.4845\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 92.4692 - val_loss: 62.2527\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 82.1222 - val_loss: 70.7207\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 83.5789 - val_loss: 121.7822\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 93.8797 - val_loss: 340.7657\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 102.0956 - val_loss: 59.3908\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 87.9321 - val_loss: 74.1208\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 230.6659 - val_loss: 138.3160\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 152.5641 - val_loss: 72.6569\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 102.0458 - val_loss: 59.9918\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 113.4270 - val_loss: 91.5644\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.3378 - val_loss: 74.5672\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 124.5383 - val_loss: 89.1432\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 103.4607 - val_loss: 92.3603\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 84.6985 - val_loss: 91.0108\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.7845 - val_loss: 62.5863\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 103.0369 - val_loss: 111.6080\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 116.3672 - val_loss: 294.9937\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 193.4349 - val_loss: 73.2685\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 181.6178 - val_loss: 315.0685\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 151.8270 - val_loss: 248.4459\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.7247 - val_loss: 103.0401\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 83.3996 - val_loss: 62.1376\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 103.9397 - val_loss: 63.3604\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 99.5981 - val_loss: 84.8918\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 100.9917 - val_loss: 63.2924\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 104.2532 - val_loss: 60.4495\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.8060 - val_loss: 120.1504\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 107.5543 - val_loss: 119.7120\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.9418 - val_loss: 119.8173\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 113.8194 - val_loss: 289.7005\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 119.9272 - val_loss: 73.5366\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 76.9468 - val_loss: 67.7852\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 71.4211 - val_loss: 68.7610\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 104.3294 - val_loss: 63.1063\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 253.0293 - val_loss: 78.0760\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 127.2987 - val_loss: 122.5246\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 177.1299 - val_loss: 83.1273\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 131.7471 - val_loss: 104.6198\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 116.7732 - val_loss: 79.4028\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 103.2863 - val_loss: 66.0405\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 87.5852 - val_loss: 64.5970\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 82.2774 - val_loss: 84.3162\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.4079 - val_loss: 56.9196\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 263.3040 - val_loss: 215.4281\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 118.0913 - val_loss: 79.2096\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 103.3713 - val_loss: 77.0444\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 82.0115 - val_loss: 66.3626\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.1520 - val_loss: 81.6672\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 78.3866 - val_loss: 62.5231\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 75.3975 - val_loss: 78.6205\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 135.9156 - val_loss: 83.0256\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 150.9535 - val_loss: 77.7168\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 117.8092 - val_loss: 98.6422\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.4127 - val_loss: 58.6488\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 70.2587 - val_loss: 69.1863\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 104.2997 - val_loss: 63.6873\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 80.4071 - val_loss: 72.5144\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 75.3968 - val_loss: 87.0402\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 120.0499 - val_loss: 103.3488\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.7918 - val_loss: 113.4683\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 170.2951 - val_loss: 82.0247\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 117.2280 - val_loss: 378.6826\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 246.1875 - val_loss: 128.1492\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.2583 - val_loss: 72.2213\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 71.0077 - val_loss: 82.1193\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 81.4532 - val_loss: 61.3558\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.8783 - val_loss: 72.0500\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 80.4712 - val_loss: 170.1844\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 168.2398 - val_loss: 95.7674\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 76.1690 - val_loss: 64.1505\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 70.7231 - val_loss: 61.0671\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 83.4284 - val_loss: 111.0439\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 76.4377 - val_loss: 120.9497\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 130.9131 - val_loss: 91.6254\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.4143 - val_loss: 126.2812\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 69.3999 - val_loss: 67.9040\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 69.8696 - val_loss: 98.1614\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.7534 - val_loss: 69.5256\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 73.6382 - val_loss: 57.4983\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 96.4248 - val_loss: 257.6137\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 172.0929 - val_loss: 76.7337\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.4397 - val_loss: 70.3517\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 95.8103 - val_loss: 107.4351\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.6455 - val_loss: 104.3752\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 145.9366 - val_loss: 97.6138\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.6667 - val_loss: 71.7290\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 62.2685 - val_loss: 67.5123\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.4864 - val_loss: 61.8506\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.3045 - val_loss: 73.8807\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 107.6379 - val_loss: 76.9905\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 135.1249 - val_loss: 113.4047\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 80.5069 - val_loss: 86.3612\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 95.1446 - val_loss: 106.6634\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.5639 - val_loss: 86.8693\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 76.1690 - val_loss: 83.1478\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 67.9601 - val_loss: 81.5372\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 65.8903 - val_loss: 192.5475\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.8587 - val_loss: 154.9224\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 97.0497 - val_loss: 113.8551\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.4272 - val_loss: 108.0426\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.1952 - val_loss: 87.4667\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.1271 - val_loss: 89.8480\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 77.4543 - val_loss: 129.9605\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 96.2379 - val_loss: 152.4231\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 99.2583 - val_loss: 107.2159\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 100.0390 - val_loss: 95.3686\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 80.0683 - val_loss: 70.0826\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 80.7547 - val_loss: 191.8482\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 127.3489 - val_loss: 124.2421\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 78.1707 - val_loss: 80.0760\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 173.5715 - val_loss: 118.6744\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 101.5660 - val_loss: 83.5010\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 72.5402 - val_loss: 83.1653\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.7305 - val_loss: 59.0118\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 93.3280 - val_loss: 58.5269\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.1706 - val_loss: 110.3777\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 95.2134 - val_loss: 88.3162\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 111.1228 - val_loss: 132.9838\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 86.3136 - val_loss: 167.5974\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 135.9898 - val_loss: 162.2019\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 85.4221 - val_loss: 82.7507\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 92.0476 - val_loss: 78.0987\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 67.8156 - val_loss: 65.5163\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 78.7827 - val_loss: 81.9757\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 82.1257 - val_loss: 158.5463\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.9581 - val_loss: 71.4085\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 62.6656 - val_loss: 69.4082\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 79.9828 - val_loss: 68.1335\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.4706 - val_loss: 114.2986\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 78.8552 - val_loss: 69.1603\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.5205 - val_loss: 70.2539\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.4454 - val_loss: 77.7145\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 80.4844 - val_loss: 86.3979\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 65.5306 - val_loss: 68.9002\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 72.7740 - val_loss: 68.4778\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 1s 8ms/step - loss: 556237.5625 - val_loss: 337.5344\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 357.7609 - val_loss: 140.4376\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 147.7028 - val_loss: 170.0307\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 132.0572 - val_loss: 182.0347\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 103.3262 - val_loss: 114.9314\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 126.5086 - val_loss: 115.1646\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 99.7937 - val_loss: 120.8792\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 98.1862 - val_loss: 212.0322\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 104.3761 - val_loss: 109.2408\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 111.4648 - val_loss: 188.2975\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 117.1957 - val_loss: 124.8760\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 104.1697 - val_loss: 184.7631\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 140.5633 - val_loss: 101.7979\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.2654 - val_loss: 197.1042\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 112.7373 - val_loss: 107.1405\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.5809 - val_loss: 161.3179\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 108.0043 - val_loss: 217.6777\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 105.6184 - val_loss: 106.7744\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 99.3521 - val_loss: 297.7701\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 104.3403 - val_loss: 118.1611\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 92.7012 - val_loss: 115.0959\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 113.0485 - val_loss: 105.2237\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 219.6105 - val_loss: 183.5159\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 153.9143 - val_loss: 97.4690\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 146.6772 - val_loss: 171.2128\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 233.6681 - val_loss: 155.1540\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 134.7173 - val_loss: 258.3255\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 116.9502 - val_loss: 94.1420\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 96.3443 - val_loss: 154.8925\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 124.3674 - val_loss: 87.0328\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 137.1092 - val_loss: 108.1200\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 195.0054 - val_loss: 359.6166\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 112.7855 - val_loss: 176.6059\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 93.6716 - val_loss: 206.6895\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 178.9956 - val_loss: 92.3095\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 124.2920 - val_loss: 100.5533\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 105.7002 - val_loss: 101.7321\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 110.1543 - val_loss: 170.1925\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 85.8454 - val_loss: 86.8438\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 89.1941 - val_loss: 216.6737\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 117.1872 - val_loss: 127.3678\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 120.6835 - val_loss: 214.1660\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 147.0980 - val_loss: 344.8950\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 303.5168 - val_loss: 117.4385\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 139.5419 - val_loss: 139.6795\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 104.2669 - val_loss: 181.6661\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 108.6553 - val_loss: 101.4545\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 104.8972 - val_loss: 93.9732\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 162.3346 - val_loss: 163.0616\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 189.6815 - val_loss: 86.4940\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 80.5069 - val_loss: 90.4132\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 135.9858 - val_loss: 147.7272\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 119.9766 - val_loss: 101.0217\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.1746 - val_loss: 135.4904\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 112.3671 - val_loss: 229.2855\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 154.4295 - val_loss: 117.9255\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 182.1301 - val_loss: 438.4071\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 153.7546 - val_loss: 137.5789\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 104.2915 - val_loss: 167.3458\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 121.9761 - val_loss: 128.7099\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 103.7347 - val_loss: 178.6347\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 122.3960 - val_loss: 82.5391\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.1653 - val_loss: 106.9507\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.7206 - val_loss: 186.6011\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 82.8477 - val_loss: 134.2231\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 118.6513 - val_loss: 173.8141\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 109.8586 - val_loss: 90.6915\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 108.6927 - val_loss: 84.8569\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 91.5270 - val_loss: 143.8024\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 113.7923 - val_loss: 100.3963\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 91.9279 - val_loss: 100.4382\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 107.3433 - val_loss: 142.7110\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 116.2449 - val_loss: 92.6926\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 86.3444 - val_loss: 88.2712\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 89.0572 - val_loss: 138.0594\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 92.1339 - val_loss: 201.2246\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 120.2829 - val_loss: 105.6759\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.8225 - val_loss: 150.0778\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 79.9476 - val_loss: 118.8247\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 103.4863 - val_loss: 127.1484\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 109.2290 - val_loss: 265.4274\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 89.3180 - val_loss: 145.7878\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 76.5611 - val_loss: 122.4350\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.8557 - val_loss: 98.3892\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 85.1175 - val_loss: 103.0916\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 91.5542 - val_loss: 191.5855\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.3749 - val_loss: 92.4774\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 125.7195 - val_loss: 118.9613\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 102.0834 - val_loss: 92.4156\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 107.8662 - val_loss: 458.3267\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 141.9524 - val_loss: 128.1283\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 122.9760 - val_loss: 229.2551\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 103.9805 - val_loss: 83.4885\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 85.5966 - val_loss: 93.8875\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 83.0266 - val_loss: 139.2075\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 75.6375 - val_loss: 87.4857\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 70.1483 - val_loss: 94.0430\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 79.5909 - val_loss: 100.1880\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.9979 - val_loss: 95.9199\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.0213 - val_loss: 113.0416\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 86.3963 - val_loss: 209.1902\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 81.4079 - val_loss: 109.1805\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 90.3311 - val_loss: 223.7020\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.7880 - val_loss: 133.9825\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 140.2016 - val_loss: 156.7300\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.9276 - val_loss: 130.9452\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 92.4010 - val_loss: 102.8427\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.5837 - val_loss: 269.2519\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 86.4534 - val_loss: 91.0004\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 124.3827 - val_loss: 102.6031\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.7514 - val_loss: 178.3089\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.3013 - val_loss: 87.2492\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 85.1505 - val_loss: 127.8127\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.8570 - val_loss: 96.3291\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 105.8314 - val_loss: 120.1522\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 96.1206 - val_loss: 106.6814\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 73.2583 - val_loss: 84.8731\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 105.8483 - val_loss: 126.1914\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 102.7493 - val_loss: 119.2272\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 91.9391 - val_loss: 88.4208\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 76.6718 - val_loss: 97.9048\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 87.4709 - val_loss: 161.5010\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 85.6088 - val_loss: 94.4759\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.3897 - val_loss: 162.1287\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 65.4572 - val_loss: 90.4396\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 79.4253 - val_loss: 221.1995\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 81.0835 - val_loss: 176.1189\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 72.8020 - val_loss: 97.0851\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 82.0208 - val_loss: 97.6246\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 68.1675 - val_loss: 201.0332\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 75.4929 - val_loss: 167.8245\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 119.1041 - val_loss: 146.0966\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 98.9426 - val_loss: 93.6185\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.0127 - val_loss: 256.8149\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 86.6823 - val_loss: 89.0673\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 78.7231 - val_loss: 92.9878\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 79.9441 - val_loss: 104.7223\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 67.5069 - val_loss: 159.6864\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 66.7800 - val_loss: 88.3827\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.9357 - val_loss: 115.7605\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 65.2881 - val_loss: 128.0649\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 78.1011 - val_loss: 171.3359\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 99.6555 - val_loss: 91.0373\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 127.8195 - val_loss: 460.0022\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 116.4261 - val_loss: 144.8772\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 89.4188 - val_loss: 92.0912\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 157.0008 - val_loss: 482.1059\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 110.4473 - val_loss: 110.9406\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.7994 - val_loss: 98.3135\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 90.3532 - val_loss: 182.5328\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 63.8548 - val_loss: 131.2742\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 69.8494 - val_loss: 142.2206\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 80.3629 - val_loss: 90.4637\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 87.2321 - val_loss: 103.6185\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 84.9066 - val_loss: 109.4568\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 72.6866 - val_loss: 242.1366\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 72.6748 - val_loss: 82.5644\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.1265 - val_loss: 85.7303\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 66.9984 - val_loss: 89.9344\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 84.7297 - val_loss: 156.0410\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.1120 - val_loss: 93.9662\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 77.3615 - val_loss: 297.0805\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 95.8939 - val_loss: 142.9060\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.3413 - val_loss: 97.7310\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 69.6068 - val_loss: 126.2889\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 78.8153 - val_loss: 121.5946\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.5278 - val_loss: 86.3820\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 71.3615 - val_loss: 104.8306\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 62.8679 - val_loss: 88.9899\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.6361 - val_loss: 85.6378\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 102.0728 - val_loss: 156.6474\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 67.5579 - val_loss: 259.7657\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 68.3161 - val_loss: 106.2034\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 64.0623 - val_loss: 94.6521\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 87.3571 - val_loss: 99.1750\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 128.2465 - val_loss: 85.9146\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 79.5011 - val_loss: 84.6031\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 70.7808 - val_loss: 102.0715\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.1311 - val_loss: 116.7221\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 110.6340 - val_loss: 102.9676\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 74.8586 - val_loss: 150.5571\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 63.3696 - val_loss: 87.9921\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 81.5434 - val_loss: 234.3286\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 69.9661 - val_loss: 150.9902\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 70.2229 - val_loss: 93.4692\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 96.8633 - val_loss: 96.8498\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 74.2628 - val_loss: 98.1352\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 64.8281 - val_loss: 121.2047\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 62.9175 - val_loss: 125.6236\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 75.3680 - val_loss: 100.9967\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.8330 - val_loss: 99.2998\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 78.1528 - val_loss: 147.8187\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 78.2490 - val_loss: 126.9199\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 91.8933 - val_loss: 124.4907\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 73.2891 - val_loss: 165.8539\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 88.3857 - val_loss: 170.9626\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 69.6727 - val_loss: 196.9111\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.5296 - val_loss: 92.1561\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.7686 - val_loss: 136.4011\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 61.6722 - val_loss: 89.5176\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 1s 9ms/step - loss: 460280.2188 - val_loss: 3227.3611\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 479.5236 - val_loss: 147.9038\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 128.3705 - val_loss: 114.0100\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 132.8930 - val_loss: 340.0942\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 156.8802 - val_loss: 182.7381\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 131.2299 - val_loss: 116.3819\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 114.8863 - val_loss: 131.9147\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 107.2831 - val_loss: 84.7003\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 112.1335 - val_loss: 185.3945\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 127.0918 - val_loss: 86.2012\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 114.9901 - val_loss: 106.9120\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 140.7077 - val_loss: 94.2879\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 191.5881 - val_loss: 225.3689\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 129.9768 - val_loss: 93.8530\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 106.7265 - val_loss: 98.2662\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 160.1874 - val_loss: 128.5904\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 141.5408 - val_loss: 132.6569\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 161.8988 - val_loss: 112.8175\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.6743 - val_loss: 239.6587\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 123.6978 - val_loss: 111.6042\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 135.6005 - val_loss: 113.0579\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.4375 - val_loss: 97.0275\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 128.8994 - val_loss: 98.7190\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 134.8916 - val_loss: 106.9248\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 140.4451 - val_loss: 128.3093\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 102.7792 - val_loss: 121.1169\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 112.5980 - val_loss: 144.6731\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 201.9251 - val_loss: 117.3708\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 113.6205 - val_loss: 94.2603\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 125.5348 - val_loss: 189.2078\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 110.9865 - val_loss: 167.0602\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 103.4729 - val_loss: 228.5369\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 128.9060 - val_loss: 181.8935\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 161.2200 - val_loss: 99.4653\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 117.1794 - val_loss: 100.4895\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 104.5997 - val_loss: 187.5800\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 120.4380 - val_loss: 144.4291\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 164.4334 - val_loss: 202.7694\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 108.7846 - val_loss: 89.3581\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 254.4203 - val_loss: 141.2219\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 112.6230 - val_loss: 118.4699\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 113.5063 - val_loss: 154.1385\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 134.6224 - val_loss: 95.7636\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 112.7756 - val_loss: 119.3044\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 141.4963 - val_loss: 143.8496\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 114.2891 - val_loss: 99.0232\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 117.6547 - val_loss: 88.6491\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 118.5755 - val_loss: 134.7057\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 101.4137 - val_loss: 96.0060\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 126.3080 - val_loss: 97.7751\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 96.4658 - val_loss: 85.7763\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 102.0913 - val_loss: 189.0199\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 117.7695 - val_loss: 86.8984\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 140.5651 - val_loss: 105.4344\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 138.5459 - val_loss: 100.8396\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.7456 - val_loss: 151.4838\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 128.8465 - val_loss: 124.7797\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 143.8544 - val_loss: 116.7595\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 149.4477 - val_loss: 111.2863\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 129.6230 - val_loss: 128.6701\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 120.3508 - val_loss: 93.7747\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 91.2997 - val_loss: 88.0824\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 107.8623 - val_loss: 91.1374\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 105.9015 - val_loss: 130.9632\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 115.1502 - val_loss: 146.7438\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 87.6184 - val_loss: 92.1216\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 94.0559 - val_loss: 167.5219\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 138.1471 - val_loss: 129.9493\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 121.0763 - val_loss: 89.8631\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.5573 - val_loss: 141.7561\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 173.1679 - val_loss: 518.9289\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 143.6682 - val_loss: 103.5849\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 144.1607 - val_loss: 283.6082\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 133.7071 - val_loss: 105.7577\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 118.5085 - val_loss: 84.1842\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 102.0459 - val_loss: 144.0457\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 129.1337 - val_loss: 270.9176\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 122.4271 - val_loss: 177.5658\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 152.4748 - val_loss: 94.2978\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 153.6499 - val_loss: 119.1474\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 127.9652 - val_loss: 94.3223\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 141.8600 - val_loss: 89.1883\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 116.8277 - val_loss: 189.0548\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 167.5537 - val_loss: 178.0582\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 99.6715 - val_loss: 160.9374\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 132.4887 - val_loss: 274.9818\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 138.0359 - val_loss: 366.5592\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 167.1873 - val_loss: 119.4876\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 94.2708 - val_loss: 95.2643\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 89.2542 - val_loss: 178.0683\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 112.2329 - val_loss: 172.2309\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.6353 - val_loss: 148.9297\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 105.6169 - val_loss: 157.2212\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 106.9629 - val_loss: 105.2614\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 132.6646 - val_loss: 282.9178\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.5850 - val_loss: 304.1633\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 145.4983 - val_loss: 102.6469\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 102.3002 - val_loss: 111.4492\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 156.3870 - val_loss: 409.0252\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 146.8437 - val_loss: 101.1941\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 118.3132 - val_loss: 105.1735\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 108.7293 - val_loss: 93.4289\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.1669 - val_loss: 92.5575\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.0517 - val_loss: 95.9328\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 82.8833 - val_loss: 144.0147\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 121.2311 - val_loss: 243.6100\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 117.6845 - val_loss: 102.8501\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 96.2505 - val_loss: 122.5923\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.7708 - val_loss: 98.2336\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 107.4290 - val_loss: 85.6124\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 103.6111 - val_loss: 92.7912\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.5485 - val_loss: 179.7780\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 115.3586 - val_loss: 147.4026\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 87.8293 - val_loss: 94.0950\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 112.2782 - val_loss: 85.8472\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 113.4909 - val_loss: 91.1003\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 202.5108 - val_loss: 95.8144\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 148.5095 - val_loss: 148.8217\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 102.8188 - val_loss: 118.8366\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 104.2532 - val_loss: 89.1348\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 125.4504 - val_loss: 107.9957\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 225.2048 - val_loss: 103.3841\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 149.8186 - val_loss: 105.6867\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 114.1628 - val_loss: 84.6396\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.7276 - val_loss: 94.2020\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 90.0777 - val_loss: 83.5745\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 102.3148 - val_loss: 212.1729\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 90.7802 - val_loss: 89.4864\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 99.1695 - val_loss: 88.3748\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 113.5585 - val_loss: 85.0439\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 119.9368 - val_loss: 83.3665\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 108.8646 - val_loss: 92.6176\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 103.9906 - val_loss: 93.6723\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 96.8315 - val_loss: 161.2785\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 115.5306 - val_loss: 144.8828\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 83.4832 - val_loss: 85.5272\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 88.1359 - val_loss: 87.2786\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 122.8141 - val_loss: 97.2209\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 107.9398 - val_loss: 80.8775\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 96.2928 - val_loss: 89.5386\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 98.8172 - val_loss: 95.5648\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 151.5330 - val_loss: 86.5056\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 139.1370 - val_loss: 89.8988\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 88.8343 - val_loss: 152.2962\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 98.2648 - val_loss: 115.7840\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 98.1543 - val_loss: 126.6180\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.3733 - val_loss: 189.4565\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 113.2618 - val_loss: 89.5816\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 94.8934 - val_loss: 93.9578\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.0069 - val_loss: 110.1780\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 99.6047 - val_loss: 89.3711\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 93.6436 - val_loss: 98.5144\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.2096 - val_loss: 196.7397\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 98.4903 - val_loss: 157.3508\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.1701 - val_loss: 128.2722\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 111.8356 - val_loss: 89.3373\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.3712 - val_loss: 127.8574\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 85.4330 - val_loss: 82.8817\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 92.9335 - val_loss: 286.7387\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 107.3699 - val_loss: 112.0695\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 89.3675 - val_loss: 86.2743\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 89.4960 - val_loss: 107.0803\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 99.1363 - val_loss: 356.3711\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 100.1954 - val_loss: 104.0873\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 99.2845 - val_loss: 141.2342\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.7855 - val_loss: 155.5665\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 97.2094 - val_loss: 83.5344\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 89.1513 - val_loss: 104.7873\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 99.7724 - val_loss: 96.3982\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 113.9619 - val_loss: 86.3662\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 127.8725 - val_loss: 163.2781\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 91.5072 - val_loss: 139.3312\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 98.8655 - val_loss: 86.9782\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 95.2340 - val_loss: 84.8544\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.3729 - val_loss: 173.3206\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 98.8762 - val_loss: 102.3443\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 92.2708 - val_loss: 205.8568\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 101.4351 - val_loss: 97.5209\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 83.1488 - val_loss: 82.4556\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 95.7286 - val_loss: 116.0649\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 80.1855 - val_loss: 175.4239\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 121.8842 - val_loss: 112.9319\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 149.9544 - val_loss: 92.6282\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 88.7008 - val_loss: 97.2060\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 89.7929 - val_loss: 85.5395\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.0815 - val_loss: 149.1801\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 106.5439 - val_loss: 84.4908\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 110.2785 - val_loss: 91.0232\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 100.9251 - val_loss: 90.3386\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 100.4525 - val_loss: 137.3653\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 98.5358 - val_loss: 88.2358\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 98.3938 - val_loss: 96.3404\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 95.5981 - val_loss: 110.7813\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 98.9506 - val_loss: 201.5546\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 186.7233 - val_loss: 85.5936\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 127.9962 - val_loss: 162.5846\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 115.1900 - val_loss: 120.6669\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 83.2698 - val_loss: 96.3143\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 81.4745 - val_loss: 100.5913\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 82.7260 - val_loss: 96.0079\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 1s 8ms/step - loss: 840055.6250 - val_loss: 3395.5493\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 958.7377 - val_loss: 556.8177\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 210.8631 - val_loss: 314.0897\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 152.4004 - val_loss: 229.4398\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 152.6612 - val_loss: 199.2324\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 144.6938 - val_loss: 171.1470\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 133.5212 - val_loss: 171.6028\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 167.4588 - val_loss: 311.4978\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 125.5362 - val_loss: 165.3968\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 114.8589 - val_loss: 151.0723\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 157.7771 - val_loss: 188.6037\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 113.3292 - val_loss: 374.9428\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 125.0532 - val_loss: 127.7579\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 110.8469 - val_loss: 200.6211\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 129.3955 - val_loss: 136.7386\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 106.6642 - val_loss: 148.5246\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 142.8702 - val_loss: 179.2633\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 117.0381 - val_loss: 192.4548\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 112.8742 - val_loss: 247.4906\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 151.1965 - val_loss: 226.2443\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 140.7056 - val_loss: 229.1652\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 143.3799 - val_loss: 284.4207\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 144.4837 - val_loss: 294.2862\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 108.4943 - val_loss: 218.2178\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 109.4293 - val_loss: 203.0987\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 109.1478 - val_loss: 188.3152\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 125.2306 - val_loss: 225.7435\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 117.6353 - val_loss: 164.1155\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 132.7229 - val_loss: 118.6125\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 112.9022 - val_loss: 201.4963\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 160.3257 - val_loss: 429.0246\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 155.9221 - val_loss: 181.5366\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 109.8234 - val_loss: 211.9820\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 120.4557 - val_loss: 193.9391\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 100.8710 - val_loss: 229.1594\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 105.1050 - val_loss: 168.1138\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 115.4008 - val_loss: 131.0149\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 119.8290 - val_loss: 295.0916\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 121.8913 - val_loss: 233.3112\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 119.0166 - val_loss: 315.2602\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 116.1459 - val_loss: 220.9417\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 124.5729 - val_loss: 238.0852\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 110.3256 - val_loss: 220.2934\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 98.1724 - val_loss: 272.3911\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 118.0259 - val_loss: 335.9796\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 110.3343 - val_loss: 226.8227\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 121.7988 - val_loss: 136.9255\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 147.3537 - val_loss: 235.7485\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 140.8009 - val_loss: 112.5990\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 128.6042 - val_loss: 347.4727\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 118.5965 - val_loss: 140.1290\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 114.0605 - val_loss: 106.7199\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 145.5248 - val_loss: 248.3851\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 103.7688 - val_loss: 136.2394\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 115.5126 - val_loss: 255.2211\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.5569 - val_loss: 200.2903\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 129.7412 - val_loss: 115.0515\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 142.1928 - val_loss: 255.8291\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 138.6318 - val_loss: 172.7135\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 105.8417 - val_loss: 170.5536\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 108.9205 - val_loss: 195.8225\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 139.9122 - val_loss: 170.1322\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 103.9673 - val_loss: 133.2624\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.5757 - val_loss: 163.2338\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 112.8558 - val_loss: 183.9501\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 97.1123 - val_loss: 227.8978\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 110.4839 - val_loss: 163.2820\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 158.8387 - val_loss: 310.9155\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 111.6694 - val_loss: 312.7768\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 113.9674 - val_loss: 149.8471\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.7779 - val_loss: 280.4955\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 116.9942 - val_loss: 418.4760\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 123.1696 - val_loss: 241.3503\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.6664 - val_loss: 272.5734\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 139.8831 - val_loss: 134.8188\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 140.6028 - val_loss: 219.1727\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 109.3651 - val_loss: 198.3961\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 109.9143 - val_loss: 287.8286\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 108.1242 - val_loss: 161.0069\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 98.2206 - val_loss: 251.2104\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 120.4238 - val_loss: 171.2034\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 96.9178 - val_loss: 207.8040\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 117.7948 - val_loss: 181.3305\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 108.0104 - val_loss: 195.0423\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.4241 - val_loss: 176.2892\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 120.6315 - val_loss: 190.9054\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 98.6540 - val_loss: 297.1938\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 109.8065 - val_loss: 242.5314\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 162.6571 - val_loss: 140.9741\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 139.3574 - val_loss: 141.1538\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 111.3514 - val_loss: 264.2645\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 97.6426 - val_loss: 265.7520\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 96.5206 - val_loss: 176.3146\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 108.0863 - val_loss: 383.2386\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 110.7800 - val_loss: 141.3796\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 103.5197 - val_loss: 216.5700\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 95.2733 - val_loss: 235.1416\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 91.7565 - val_loss: 178.2583\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 160.3629 - val_loss: 177.9334\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 141.4890 - val_loss: 113.8281\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 103.7770 - val_loss: 179.7596\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 108.6047 - val_loss: 182.4807\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 105.2474 - val_loss: 172.4042\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 109.3188 - val_loss: 231.7584\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 106.2159 - val_loss: 218.0276\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.7874 - val_loss: 170.8655\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 117.0252 - val_loss: 300.2109\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 92.0584 - val_loss: 153.1006\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.3320 - val_loss: 343.3348\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 117.3229 - val_loss: 175.3523\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 119.7273 - val_loss: 422.3292\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.2976 - val_loss: 162.7674\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 112.1068 - val_loss: 284.8087\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 149.8475 - val_loss: 133.6907\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 107.6772 - val_loss: 187.4725\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 98.8743 - val_loss: 226.5746\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.3742 - val_loss: 142.8390\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 113.5372 - val_loss: 159.0692\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 96.9855 - val_loss: 237.6134\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.9270 - val_loss: 241.9583\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 115.0008 - val_loss: 195.1342\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 105.4574 - val_loss: 233.5094\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 101.9968 - val_loss: 221.3751\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 94.5231 - val_loss: 300.8732\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 111.6595 - val_loss: 129.2944\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 139.0287 - val_loss: 136.0072\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 140.2318 - val_loss: 237.4481\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 91.9308 - val_loss: 295.2646\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 203.9812 - val_loss: 166.4930\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 121.3784 - val_loss: 130.2847\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 102.1206 - val_loss: 249.1267\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 107.6628 - val_loss: 393.2050\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 96.6826 - val_loss: 350.9682\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 108.7430 - val_loss: 241.6615\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 101.7154 - val_loss: 162.9359\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.3700 - val_loss: 156.6343\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 115.4078 - val_loss: 187.8315\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 121.8026 - val_loss: 171.4347\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 115.1593 - val_loss: 267.9167\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 100.9642 - val_loss: 130.9318\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 129.5769 - val_loss: 181.9500\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 121.7024 - val_loss: 339.9870\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.6862 - val_loss: 237.4330\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 116.9921 - val_loss: 146.7061\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 118.2977 - val_loss: 154.5594\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 98.4437 - val_loss: 237.0558\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 94.6751 - val_loss: 225.0933\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 133.7674 - val_loss: 141.1242\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 143.5798 - val_loss: 144.6043\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 103.8650 - val_loss: 177.0640\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 103.9887 - val_loss: 161.5299\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 134.7216 - val_loss: 138.3990\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 122.4251 - val_loss: 379.7197\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 99.4829 - val_loss: 190.2238\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 89.7391 - val_loss: 220.2313\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 89.4109 - val_loss: 186.7496\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 98.7016 - val_loss: 283.3964\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 90.3817 - val_loss: 150.5700\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 86.7731 - val_loss: 360.1728\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 133.5852 - val_loss: 231.3907\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 94.2145 - val_loss: 211.2302\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 97.4641 - val_loss: 285.3725\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 120.5218 - val_loss: 312.1256\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 104.9825 - val_loss: 137.3184\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 99.3193 - val_loss: 219.5800\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 93.4879 - val_loss: 274.1201\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.0267 - val_loss: 270.1443\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 94.6277 - val_loss: 273.1629\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 99.7788 - val_loss: 194.3634\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 94.5761 - val_loss: 215.8495\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 92.5384 - val_loss: 275.3836\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 88.3964 - val_loss: 177.8052\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 87.1761 - val_loss: 161.9480\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 98.7777 - val_loss: 151.3929\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 119.4486 - val_loss: 211.9857\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 90.6973 - val_loss: 155.7546\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 106.7689 - val_loss: 160.8439\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 90.5417 - val_loss: 338.9467\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 96.7928 - val_loss: 240.9747\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 89.5081 - val_loss: 300.3457\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.0926 - val_loss: 161.0134\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 92.4385 - val_loss: 274.2979\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 102.0942 - val_loss: 206.6764\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 87.5669 - val_loss: 267.8215\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.9777 - val_loss: 179.9160\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 86.4790 - val_loss: 196.0532\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 90.2127 - val_loss: 157.7333\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 89.9344 - val_loss: 138.3047\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 105.0623 - val_loss: 177.6522\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 95.8418 - val_loss: 151.7851\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 93.4073 - val_loss: 124.7492\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.0446 - val_loss: 268.6911\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 85.2381 - val_loss: 186.8069\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 95.2832 - val_loss: 228.2327\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 85.0977 - val_loss: 225.1619\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.2818 - val_loss: 297.1246\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 107.5943 - val_loss: 222.0467\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 92.0871 - val_loss: 247.3789\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 84.2292 - val_loss: 226.1801\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 94.1202 - val_loss: 215.4942\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 1s 8ms/step - loss: 781134.3125 - val_loss: 551.9897\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 913.3348 - val_loss: 300.7564\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 214.0025 - val_loss: 194.3327\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 188.4168 - val_loss: 186.7560\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 198.7581 - val_loss: 244.7265\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 227.3417 - val_loss: 146.1702\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 187.7479 - val_loss: 157.4702\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 187.8562 - val_loss: 156.2099\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 178.6951 - val_loss: 212.5399\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 190.7316 - val_loss: 224.0515\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 203.0739 - val_loss: 274.7226\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 222.9360 - val_loss: 296.7717\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 209.4338 - val_loss: 322.1077\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 215.8322 - val_loss: 162.1517\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 191.4031 - val_loss: 211.0039\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 178.9965 - val_loss: 227.7839\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 173.0569 - val_loss: 197.7213\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 188.8048 - val_loss: 378.5411\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 165.7545 - val_loss: 159.8360\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 210.2534 - val_loss: 128.9299\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 214.6264 - val_loss: 351.1307\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 193.6573 - val_loss: 240.5910\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 197.7207 - val_loss: 279.0534\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 207.7691 - val_loss: 116.2240\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 223.9568 - val_loss: 119.1274\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 237.7005 - val_loss: 127.6026\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 264.4019 - val_loss: 136.1758\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 239.3537 - val_loss: 193.0929\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 214.0966 - val_loss: 202.4400\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 212.7783 - val_loss: 216.8655\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 248.0511 - val_loss: 280.3262\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 268.4605 - val_loss: 349.9756\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 173.1288 - val_loss: 174.9684\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 180.2360 - val_loss: 112.4251\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 201.0671 - val_loss: 113.8770\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 175.8109 - val_loss: 142.1042\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 188.6768 - val_loss: 111.4342\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 227.1046 - val_loss: 101.9219\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 239.3319 - val_loss: 122.0700\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 194.2880 - val_loss: 159.5606\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 201.0433 - val_loss: 114.0831\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 200.5943 - val_loss: 493.8745\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 660.3214 - val_loss: 563.4408\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 329.8013 - val_loss: 407.5827\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 268.4987 - val_loss: 137.2988\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 184.7057 - val_loss: 110.6775\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 186.6613 - val_loss: 222.0994\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 168.9627 - val_loss: 163.0060\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 194.3556 - val_loss: 138.6929\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 164.7592 - val_loss: 115.7040\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 199.8527 - val_loss: 127.4419\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 211.1108 - val_loss: 141.7805\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 171.4889 - val_loss: 221.7380\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 280.0367 - val_loss: 306.6478\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 246.9054 - val_loss: 219.5825\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 196.5238 - val_loss: 175.7338\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 238.4479 - val_loss: 160.7069\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 238.0535 - val_loss: 141.0196\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 223.2510 - val_loss: 199.8952\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 173.3118 - val_loss: 175.4587\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 171.3278 - val_loss: 184.6632\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 207.6922 - val_loss: 153.0953\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 212.5989 - val_loss: 129.8592\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 148.2143 - val_loss: 206.6104\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 200.2313 - val_loss: 192.4830\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 186.6790 - val_loss: 214.9025\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 160.1943 - val_loss: 140.9424\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 171.5741 - val_loss: 155.5804\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 191.7158 - val_loss: 189.2695\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 160.1823 - val_loss: 176.1537\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 158.7340 - val_loss: 201.2750\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 185.5839 - val_loss: 160.7758\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 255.2310 - val_loss: 196.4955\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 240.8622 - val_loss: 169.0418\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 174.7177 - val_loss: 183.5330\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 180.3163 - val_loss: 139.5069\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 152.2428 - val_loss: 126.4987\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 151.3663 - val_loss: 271.6339\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 168.0044 - val_loss: 143.4555\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 161.5342 - val_loss: 297.7528\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 173.4538 - val_loss: 144.7302\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 234.7039 - val_loss: 115.9202\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 269.3068 - val_loss: 150.3860\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 151.8553 - val_loss: 225.3084\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 189.0494 - val_loss: 205.7733\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 152.4909 - val_loss: 167.0557\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 176.9218 - val_loss: 125.7399\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 167.3263 - val_loss: 142.0698\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 291.3337 - val_loss: 119.9244\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 171.8506 - val_loss: 444.5293\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 311.6398 - val_loss: 186.7930\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 152.2428 - val_loss: 169.3736\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 158.5470 - val_loss: 158.7597\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 220.0016 - val_loss: 144.9985\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 326.4599 - val_loss: 166.8185\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 176.2766 - val_loss: 147.7645\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 165.9100 - val_loss: 158.9522\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 174.6142 - val_loss: 184.8715\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 173.2363 - val_loss: 127.8570\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 198.0271 - val_loss: 207.9842\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 179.5903 - val_loss: 177.8021\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 160.2605 - val_loss: 204.7219\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 146.3784 - val_loss: 287.9200\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 155.7794 - val_loss: 221.8941\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 157.8899 - val_loss: 175.4068\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 193.5464 - val_loss: 196.1529\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 168.3394 - val_loss: 131.7585\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 260.5873 - val_loss: 456.5770\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 244.3225 - val_loss: 142.5541\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 174.2547 - val_loss: 284.5126\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 152.9433 - val_loss: 192.6090\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 151.2587 - val_loss: 131.5127\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 160.7302 - val_loss: 353.7923\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 177.9741 - val_loss: 143.7532\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 142.8921 - val_loss: 147.1898\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 160.7362 - val_loss: 148.6045\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 147.4285 - val_loss: 130.2364\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 166.1829 - val_loss: 454.3047\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 241.5985 - val_loss: 190.8380\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 195.0750 - val_loss: 189.9583\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 174.6401 - val_loss: 154.4613\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 161.6930 - val_loss: 260.1098\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 232.9072 - val_loss: 144.6772\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 174.1004 - val_loss: 114.1075\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 171.8890 - val_loss: 240.3044\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 194.7242 - val_loss: 139.5768\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 151.2136 - val_loss: 142.7746\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 171.1926 - val_loss: 305.0377\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 157.9582 - val_loss: 125.1897\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 167.2072 - val_loss: 141.9011\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 170.0098 - val_loss: 234.0466\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 142.2217 - val_loss: 145.6989\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 146.9916 - val_loss: 127.5193\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 155.1089 - val_loss: 142.9663\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 143.4326 - val_loss: 150.1935\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 138.8026 - val_loss: 190.7762\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 154.1666 - val_loss: 201.6226\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 150.8316 - val_loss: 117.4636\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 175.9857 - val_loss: 178.1256\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 144.8165 - val_loss: 168.4741\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 160.4046 - val_loss: 167.4063\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 143.7549 - val_loss: 169.9298\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 134.3624 - val_loss: 210.7679\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 181.1579 - val_loss: 187.9721\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 215.2499 - val_loss: 150.1218\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 147.7602 - val_loss: 156.1791\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 154.7514 - val_loss: 156.6666\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 169.7174 - val_loss: 394.1422\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 156.4113 - val_loss: 158.5495\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 140.7992 - val_loss: 157.8093\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 145.2896 - val_loss: 244.8250\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 157.8949 - val_loss: 121.1465\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 154.5972 - val_loss: 144.6715\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 172.1897 - val_loss: 132.0050\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 149.2313 - val_loss: 120.5420\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 184.2880 - val_loss: 248.5803\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 143.0600 - val_loss: 148.8723\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 148.0342 - val_loss: 225.9266\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 131.2303 - val_loss: 136.6879\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 132.7883 - val_loss: 163.3632\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 168.0948 - val_loss: 137.5629\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 138.5852 - val_loss: 178.2950\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 165.5314 - val_loss: 143.2637\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 176.7368 - val_loss: 244.4707\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 167.5492 - val_loss: 192.5628\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 137.9346 - val_loss: 228.1287\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 159.1069 - val_loss: 139.2180\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 158.0553 - val_loss: 304.9320\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 169.1045 - val_loss: 138.6407\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 215.6693 - val_loss: 156.8145\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 144.1237 - val_loss: 220.5295\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 146.3545 - val_loss: 343.9188\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 168.6256 - val_loss: 143.0634\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 151.2576 - val_loss: 186.5556\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 144.6073 - val_loss: 201.8274\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 143.5111 - val_loss: 127.5942\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 169.3414 - val_loss: 137.6590\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 138.7555 - val_loss: 158.0388\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 175.8461 - val_loss: 295.4072\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 164.0650 - val_loss: 218.1732\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 172.8548 - val_loss: 218.0353\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 143.8389 - val_loss: 141.0119\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 144.0971 - val_loss: 201.8698\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 126.7441 - val_loss: 217.3752\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 127.4218 - val_loss: 287.4266\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 171.0420 - val_loss: 124.1017\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 180.1479 - val_loss: 161.6444\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 160.3289 - val_loss: 226.0471\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 136.5354 - val_loss: 174.3991\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 138.6609 - val_loss: 234.5836\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 135.4119 - val_loss: 145.0326\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 164.0488 - val_loss: 144.3874\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 219.1140 - val_loss: 140.1083\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 209.6652 - val_loss: 264.5789\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 148.5779 - val_loss: 300.5740\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 166.9935 - val_loss: 112.7959\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 145.7310 - val_loss: 143.5966\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 165.0507 - val_loss: 111.9603\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 153.7510 - val_loss: 160.4459\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 173.9204 - val_loss: 277.5858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min(val_loss,key=val_loss.get)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl8uBTPlCWGS",
        "outputId": "33000be3-0160-4824-d8ba-e1409b2b63ec"
      },
      "id": "Xl8uBTPlCWGS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var_sel[45]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI4coUTeAuTP",
        "outputId": "0f405264-989f-47b6-eb59-001da198fb0f"
      },
      "id": "qI4coUTeAuTP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Nuclear[MWh]',\n",
              " 'Fossil hard coal[MWh]',\n",
              " 'Fossil gas[MWh]',\n",
              " 'Other conventional[MWh]']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Master Thesis I-updated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}